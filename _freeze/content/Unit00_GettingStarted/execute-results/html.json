{
  "hash": "7cd79c7d8cf9d7167e8c7f14f34e1c97",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Getting Started\"\nsubtitle: \"Inference with Predicted Data\"\ndescription:  |\n  Build intuition for inference with predicted data by simulating \n  labeled/unlabeled sets and comparing naive, classical, and IPD estimators.\nimage: images/ipd_cover.png\nunit: 0\ncore: true\nformat:\n  html:\n    toc: true\n    number-sections: false\n    code-fold: true\n    code-tools: false\nexecute:\n  warning: false\n  message: false\n---\n\n\n\n\n# Overview\n\nWelcome to this workshop on *Inference with Predicted Data (IPD)*! In this \nmodule, we will:\n\n* Introduce the [`ipd`](https://github.com/ipd-tools/ipd) package and its main functions.\n* Demonstrate how to generate synthetic data for different types of outcomes.\n* Fit and compare various IPD methods.\n* Inspect results using built-in `tidy`, `glance`, and `augment` methods.\n\nThroughout the workshop, exercises are provided with solutions for practice.\n\n::: {.callout-tip}\n## How to Use This Module\n\n- Try each **Exercise** on your own first.\n- Expand the **Answer** callout to view one possible solution.\n- Use the **Notes** callouts to interpret results and connect them to IPD concepts.\n:::\n\n::: {.callout-note}\n## Why This Unit Uses Synthetic Data\n\nUnit 00 intentionally uses synthetic examples. The goal is to make the\nIPD workflow concrete before moving to real data in later modules.\n:::\n\n::: {.callout-important}\n## Key Takeaway\n\nUsing predicted outcomes for can improve efficiency, but treating them as if they \nare real data can bias downstream inference. This unit shows how IPD methods\ncan use a small set of labeled data to recover valid inference while leveraging \nunlabeled data and predicted outcomes to gain more information.\n:::\n\n## Background and Motivation\n\nWhen an outcome, $Y$, is costly or difficult to measure, it can be tempting to \nreplace missing values with predictions, $f(\\boldsymbol{X})$, from a \nmachine learning model (e.g., a random forest or neural network) built on \neasier-to-measure features, $\\boldsymbol{X}$. However, using $f$ as if \nit were the true outcome in downstream analyses, e.g., in estimating a \nregression coefficient, $\\beta$, for the association between $Y$ and \n$\\boldsymbol{X}$, leads to biased point estimates and underestimated \nuncertainty. Methods for \n[Inference with Predicted Data (IPD)](https://academic.oup.com/bioinformatics/article/41/2/btaf055/7997267) \naddress this by leveraging a small subset of \"labeled\" data with true $Y$ \nvalues to calibrate inference in a larger \"unlabeled\" dataset. \n\n## The IPD Framework\n\nConsider data arising from three sets of observations:\n\n* **Training Set**: $\\{(X_j, Y_j)\\}_{j=1}^{n_t}$, used to fit a predictive model, $f(\\cdot)$.\n* **Labeled Set**: $\\{(X_i, Y_i)\\}_{i=1}^{n_l}$, smaller sample with true outcomes measured.\n* **Unlabeled Set**: $\\{(X_i)\\}_{i=n_l +1}^{n_l + n_u}$, only features available.\n\nAfter fitting $f$ on the training set, we apply it to the labeled and unlabeled sets to obtain predictions $f_i = f(X_i)$:\n\n<br/>\n\n![*Overview of setup for common 'inference with predicted data' problems*](images/ipd_overview.png){width=100%}\n\n<br/>\n\nEspecially for 'good' predictions, it is tempting to treat $f_i$ as surrogate \noutcomes and use them to estimate quantities such as regression parameters, \n$\\beta$.  However, as we will see, this leads to *invalid inference*. By \ncombining the predicted $f_i$ with the observed $Y_i$ in the labeled set, \nwe can calibrate our estimates and standard errors to achieve *valid inference*.\n\n## Key Formulas\n\nConsider a simple linear regression model for the association between $Y$ and \n$X$. We discuss the following potential estimators, which we will later \nimplement using simulated data.\n\n### Naive Estimator\n\nUsing only the *unlabeled* predictions, the **naive** OLS estimator solves\n\n$$\n\\hat\\gamma_{\\text{naive}} = \\arg\\min_\\gamma \\sum_{i\\in U} (f_i - X_i'\\gamma)^2.\n$$\n\nWe are careful to write the coefficients for this model as $\\gamma$, because \nthey bear no necessary correspondence with $\\beta$, except under the extremely \nrestrictive scenario when $f$ perfectly captures the true regression function.\n\n### Classical Estimator\n\nInstead, a valid approach would be to use only the *labeled* data. This \n**classical** estimator solves\n\n$$\n\\hat\\beta_{\\text{classical}} = \\arg\\min_\\beta \\sum_{i\\in L} (Y_i - X_i'\\beta)^2.\n$$\nWhile this approach is valid, it has limited precision because $n_l$ is small \nin practice and we do not utilize any potential information from the (often\nmuch larger) **unlabeled** data.\n\n### IPD Estimators\n\nMany estimators tailored to inference with predicted data\nshare a similar form, as given in \n[Ji et al. (2025)](https://arxiv.org/pdf/2501.09731):\n\n$$\n\\widehat\\beta_\\text{ipd} = \\arg\\min_\\beta \\frac{1}{n_l}\\sum_{i=1}^{n_l} \\ell(X_i, Y_i) - \\left[\\frac{1}{n_l}\\sum_{i=1}^{n_l} g(X_i, f_i) - \\frac{1}{n_l+n_u}\\sum_{i=n_l+1}^{n_l+n_u} g(X_i, f_i)\\right],\n$$\n\nfor some loss function, $\\ell(\\cdot)$, such as the squared error loss for \nlinear regression, and some $g(\\cdot)$, which they call the 'imputed loss'. \nHere, the first term is exactly the **classical** estimator, which anchors \nthese methods on a valid model, and the second term in the square brackets \n'augments' the estimator with additional information from the predictions. \nThis allows us to have an estimator that is provably unbiased and \nasymptotically at least as efficient as the **classical** estimator, which \nonly uses a fraction of the data.\n\nThe \n[Inference with Predicted Data (IPD)](https://academic.oup.com/bioinformatics/article/41/2/btaf055/7997267) \npackage implements several recent methods for IPD, such as **Chen & Chen** \nmethod of\n[Gronsbell et al.](https://arxiv.org/pdf/2411.19908), \nthe Prediction-Powered Inference (**PPI**) and **PPI++** methods of \n[Angelopoulos et al. (a)](https://www.science.org/doi/10.1126/science.adi6000) \nand \n[Angelopoulos et al. (a)](https://arxiv.org/pdf/2311.01453), the \nPost-Prediction Inference (**PostPI**) method of \n[Wang et al.](https://www.pnas.org/doi/10.1073/pnas.2001238117), and the \nPost-Prediction Adaptive Inference (**PSPA**) method of \n[Miao et al.](https://arxiv.org/abs/2311.14220) \nto conduct valid, efficient inference, even when a large proportion of outcomes \nare predicted.  \n\nIn this first tutorial, we demonstrate how to:\n\n1. **Generate** fully synthetic data with `ipd::simdat()`.\n2. **Fit** a simple linear prediction model (e.g., linear regression).\n3. **Apply** `ipd::ipd()` to estimate the association, $\\beta$, between $Y$ and $X$ using *labeled* and *unlabeled* data.\n4. **Compare** the **naive**, **classical**, and **IPD** estimates of $\\beta$.\n5. **Visualize** these results.\n\n## Installation and Setup\n\nFirst, insure you have the `ipd` package and some additional packages installed:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install these packages if you have not already:\n# install.packages(c(\"ipd\", \"broom\", \"tidyverse\", \"patchwork\"))\n\nlibrary(ipd)\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(patchwork)\n```\n:::\n\n\n\n\nThroughout the workshop, we will use reproducible seeds and \n[tidyverse](https://www.tidyverse.org/) conventions.\n\n## Function References\n\nBelow is a high-level summary of the core ``ipd`` functions.\n\n### `simdat()`\n\nGenerates synthetic datasets for various inferential models.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimdat(\n  n,        # Numeric vector of length 3: c(n_train, n_labeled, n_unlabeled)\n  effect,   # Numeric: true effect size for simulation\n  sigma_Y,  # Numeric: residual standard deviation\n  model,    # Character: one of \"mean\", \"quantile\", \"ols\", \"logistic\", \"poisson\"\n  ...       # Additional arguments\n)\n```\n:::\n\n\n\nThis function returns a data.frame with columns:\n\n* `X1, X2, ...`: covariates\n* `Y`: true outcome (for training, labeled, and unlabeled subsets)\n* `f`: predictions from the model (for labeled and unlabeled subsets)\n* `set_label`: character indicating \"training\", \"labeled\", or \"unlabeled\".\n\n### `ipd()`\n\nFits IPD methods for downstream inference on predicted data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nipd(\n  formula,      # A formula: e.g., Y - f ~ X1 + X2 + ...\n  method,       # Character: one of \"chen\", \"postpi_boot\", \"postpi_analytic\", \n                # \"ppi\", \"ppi_all\", \"ppi_plusplus\", \"pspa\"\n  model,        # Character: one of \"mean\", \"quantile\", \"ols\", \"logistic\", \n                # \"poisson\"\n  data,         # Data frame containing columns for formula and label\n  label,        # Character: name of the column with set labels (\"labeled\" and \n                # \"unlabeled\")\n  ...           # Additional arguments\n)\n```\n:::\n\n\n\n\n#### Supported Methods\n\n* **chen**: Chen and Chen estimator ([Gronsbell et al., 2025](https://arxiv.org/pdf/2411.19908))\n* **postpi_analytic**: analytic post-prediction inference ([Wang et al., 2020](https://www.pnas.org/doi/10.1073/pnas.2001238117)).\n* **postpi_boot**: bootstrap post-prediction inference [Wang et al., 2020](https://www.pnas.org/doi/10.1073/pnas.2001238117)).\n* **ppi**: prediction-powered inference ([Angelopoulos et al., 2023](https://www.science.org/doi/10.1126/science.adi6000))\n* **ppi_plusplus**: PPI++ (PPI with data-driven weighting; [Angelopoulos et al., 2024](https://arxiv.org/pdf/2311.01453))\n* **ppi_a**: PPI using all data ([Gronsbell et al., 2025](https://arxiv.org/pdf/2411.19908))\n* **pspa**: assumption-lean and data-adaptive post-prediction inference ([Miao et al., 2024](https://arxiv.org/pdf/2311.14220))\n\n### Tidy Methods\n\n* `print()` and `summary()`: display model summaries.\n* `tidy()`: return a tibble of estimates and standard errors.\n* `glance()`: return a one-row tibble of model-level metrics.\n* `augment()`: return the original data with fitted values and residuals.\n\n## Simulating Data\n\nThe `ipd::simdat()` function makes it easy to generate:\n\n* A **training set** (where you fit your prediction model),\n* A **labeled set** (where you observe the true $Y$),\n* An **unlabeled set** (where $Y$ is *presumed missing* but you compute predictions $f$).\n\nWe supply the sample sizes, `n = c(n_train, n_label, n_unlabel)`, an effect \nsize (`effect`), residual standard deviation (`sigma_Y`; i.e., how much random\nnoise is in the data), and a model type (`\"ols\"`, `\"logistic\"`, etc.). In this \ntutorial, we focus on a continuous outcome generated from a linear regression\nmodel (`\"ols\"`). We can also optionally shift and scale the predictions (via \nthe `shift` and `scale` arguments) to control how the predicted outcomes relate \nto their true underlying counterparts.\n\n### Exercise 1: Data Generation\n\nLet us generate a synthetic dataset for a linear model with:\n\n* 5,000 training observations\n* 500 labeled observations\n* 1,500 unlabeled observations\n* Effect size = 1.5\n* Residual SD = 3\n* Predictions shifted by 1 and scaled by 2\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n# n_t = 5000, n_l = 500, n_u = 1500\nn <- c(5000, 500, 1500)\n\n# Effect size = 1.5, noise sd = 3, model = \"ols\" (ordinary least squares)\n# We also shift the mean of the predictions by 1 and scale their values by 2\ndat <- simdat(\n  n       = n,\n  effect  = 1.5,\n  sigma_Y = 3,\n  model   = \"ols\",\n  shift   = 1,\n  scale   = 2\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# The resulting data.frame `dat` has columns:\n#  - X1, X2, X3, X4: Four simulated covariates (all numeric ~ N(0,1))\n#  - Y             : True outcome (available in unlabeled set for simulation)\n#  - f             : Predicted outcome (Generated internally in simdat)\n#  - set_label     : {\"training\", \"labeled\", \"unlabeled\"}\n\n# Quick look:\ndat |> \n  group_by(set_label) |> \n  summarize(n = n()) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  set_label     n\n  <chr>     <int>\n1 labeled     500\n2 training   5000\n3 unlabeled  1500\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\nLet us also inspect the first few rows of each subset:\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Training set\ndat |>\n  filter(set_label == \"training\") |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 5,000\nColumns: 7\n$ X1        <dbl> -0.56047565, -0.23017749, 1.55870831, 0.07050839, 0.12928774…\n$ X2        <dbl> -1.61803670, 0.37918115, 1.90225048, 0.60187427, 1.73234970,…\n$ X3        <dbl> -0.91006117, 0.28066267, -1.03567040, 0.27304874, 0.53779815…\n$ X4        <dbl> -1.119992047, -1.015819127, 1.258052722, -1.001231731, -0.40…\n$ Y         <dbl> 3.8625325, -1.6575634, 4.1872914, -3.3624963, 6.9978916, 1.5…\n$ f         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ set_label <chr> \"training\", \"training\", \"training\", \"training\", \"training\", …\n```\n\n\n:::\n\n```{.r .cell-code}\n# Labeled set\ndat |>\n  filter(set_label == \"labeled\") |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 500\nColumns: 7\n$ X1        <dbl> -0.4941739, 1.1275935, -1.1469495, 1.4810186, 0.9161912, 0.3…\n$ X2        <dbl> -0.15062996, 0.80094056, -1.18671785, 0.43063636, 0.21674709…\n$ X3        <dbl> 2.0279109, -1.4947497, -1.5729492, -0.3002123, -0.7643735, -…\n$ X4        <dbl> 0.53495620, 0.36182362, -1.89096604, -1.40631763, -0.4019282…\n$ Y         <dbl> 2.71822922, 1.72133689, 0.86081066, 3.77173123, -2.77191549,…\n$ f         <dbl> 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n$ set_label <chr> \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labe…\n```\n\n\n:::\n\n```{.r .cell-code}\n# Unlabeled set\ndat |>\n  filter(set_label == \"unlabeled\") |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,500\nColumns: 7\n$ X1        <dbl> -1.35723063, -1.29269781, -1.51720731, 0.85917603, -1.214617…\n$ X2        <dbl> 0.01014789, 1.56213812, 0.41284605, -1.18886219, 0.71454993,…\n$ X3        <dbl> -1.42521509, 1.73298966, 1.66085181, -0.85343610, 0.26905593…\n$ X4        <dbl> -1.1645365, -0.2522693, -1.3945975, 0.3959429, 0.5980741, 1.…\n$ Y         <dbl> -3.8296223, -3.0350282, -5.3736718, 2.6076634, -4.7813463, -…\n$ f         <dbl> -1.9014253, -0.1679210, -0.3030749, 0.1249168, -0.8998546, -…\n$ set_label <chr> \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabel…\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\n* Rows where `set_label == \"training\"` form an internal training set.  Here, `Y` is observed, but `f` is `NA`, as we learn the prediction rule in this set.\n* Rows where `set_label == \"labeled\"` also have both `Y` and `f`.  In practice, `f` will be generated by your own prediction model; for simulation, `simdat` does so automatically.\n* Rows where `set_label == \"unlabeled\"` have `Y` for posterity (but in a real‐data scenario, you would not know `Y`); `simdat` still generates `Y`, but the IPD routines will not use these. The column `f` always contains 'predicted' values.\n\n:::\n\n## Generating Predictions\n\nIn practice, we would take the **training** portion and fit an AI/ML model to \npredict $Y$ from $(X_1, X_2, X_3, X_4)$. This is done automatically by the\n`simdat` function, but we will do this for demonstration.\n\n### Exercise 2: Fitting a Linear Prediction Model\n\nLet us fit a *linear prediction model* on the training data:\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) Subset training set\ndat_train <- dat |> \n  filter(set_label == \"training\")\n\n# 2) Fit a linear model: Y ~ X1 + X2 + X3 + X4\nlm_pred <- lm(Y ~ X1 + X2 + X3 + X4, data = dat_train)\n\n# 3) Prepare a full-length vector of NA\ndat$f_pred <- NA_real_\n\n# 4) Identify the rows to predict (all non–training rows)\nidx_analytic <- dat$set_label != \"training\"\n\n# 5) Generate predictions just once on that subset (shifted and scaled to match)\npred_vals <- (predict(lm_pred, newdata = dat[idx_analytic, ]) - 1) / 2 \n\n# 6) Insert them back into the full data frame\ndat$f_pred[idx_analytic] <- pred_vals\n\n# 7) Verify: `f_pred` is equal to `f` for the labeled and unlabeled data\ndat |> \n  select(set_label, Y, f, f_pred) |> \n  filter(set_label != \"training\") |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,000\nColumns: 4\n$ set_label <chr> \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labe…\n$ Y         <dbl> 2.71822922, 1.72133689, 0.86081066, 3.77173123, -2.77191549,…\n$ f         <dbl> 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n$ f_pred    <dbl> 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\n* `lm(Y ~ X1 + X2 + X3 + X4, data = dat_train)` fits an ordinary least squares (OLS) regression on the training subset.\n* `predict(lm_pred, newdata = .)` generates a new `f` (stored as `f_pred`) for each row outside of the training set.\n* In real workflows, you might use random forests (`ranger::ranger()`), gradients (`xgboost::xgboost()`), or any other ML algorithm; the IPD methods only require that you supply a vector of predictions, `f`, in your data.\n\n:::\n\n## Creating 'Labeled' and 'Unlabeled' Datasets\n\n### Exercise 3: Splitting the Data\n\nWe now split the data into two subsets:\n\n* **labeled**: those rows where we retain the true `Y` (to be used for final inference alongside their predictions).\n* **unlabeled**: those rows where we **hide** the true `Y` (we pretend we do not observe them; `ipd` will still require the `f` + covariates).\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_ipd <- dat |>\n  filter(set_label != \"training\") |>\n  # Keep only the columns needed for downstream IPD\n  select(set_label, Y, f, X1, X2, X3, X4) \n\n# Show counts:\ndat_ipd |> \n  group_by(set_label) |> \n  summarize(n = n())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  set_label     n\n  <chr>     <int>\n1 labeled     500\n2 unlabeled  1500\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\n* After this step, `dat_ipd` has two groups:\n* `labeled` (500 rows where we observe both `Y` and `f`),\n* `unlabeled` (1500 rows where we only 'observe' `f`).\n\n:::\n\n## Comparison of the True vs Predicted Outcomes\n\nBefore modeling, it is helpful to see graphically how the predicted values, \n$f$, compare to the true outcomes, $Y$. \n\n### Exercise 4: Plotting the Data\n\nWe can visually assess the **bias** and **variance** of our predicted \noutcomes, $f$, versus the true outcomes, $Y$, in our analytic data by \nplotting:\n\n1. **Scatterplot** of $Y$ and $f$ vs. $X_1$  \n2. **Density plots** of $Y$ and $f$  \n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prepare data\ndat_visualize <- dat_ipd |> \n  select(X1, Y, f) |>\n  pivot_longer(Y:f, names_to = \"Measure\", values_to = \"Value\") |>\n  arrange(Measure)\n\n# Scatter + trend lines\nggplot(dat_visualize, aes(x = X1, y = Value, color = Measure)) +\n  theme_minimal() +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_manual(values = c(\"steelblue\", \"gray\")) +\n  labs(\n    x     = \"X1\",\n    y     = \"True Y or Predicted f\",\n    color = \"Measure\"\n  ) +\n  theme(legend.position = \"bottom\") \n```\n\n::: {.cell-output-display}\n![](Unit00_GettingStarted_files/figure-html/unnamed-chunk-8-1.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Density plots\nggplot(dat_visualize, aes(x = Value, fill = Measure)) +\n  theme_minimal() +\n  geom_density(alpha = 0.4) +\n  scale_fill_manual(values = c(\"steelblue\", \"gray\")) +\n  labs(\n    x     = \"Value\",\n    y     = \"Density\",\n    fill  = \"Measure\"\n  ) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](Unit00_GettingStarted_files/figure-html/unnamed-chunk-9-1.png){width=100%}\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\n* In the **scatterplot**, note that the predicted values $f$ (in blue) lie more tightly along the fitted trend line than the true $Y$ (in gray), indicating stronger correlation with $X_1$.\n* In the **density plot**, you can see that the spread of $f$ is narrower than that of $Y$, illustrating that the predictive model has reduced variance (often due to \"regression to the mean\").\n\n:::\n\n## Some Baselines: Naive vs Classical Inference\n\nBefore applying IPD, let's see what happens if we:\n\n1. Regress the *unlabeled* predicted `f` on `X1` (the **naive** approach).\n2. Regress only the *labeled* true `Y` on `X1` (the **classical** approach).\n\nWe will compare these to IPD‐corrected estimates.\n\n### Exercise 5: Naive vs. Classical Model Fitting\n\nUsing the `labeled` and `unlabeled` sets, fit two models:\n\n1. **Naive OLS** on the *unlabeled* set using `lm()` with `f ~ X1`.\n2. **Classical OLS** on the *labeled* set using `lm()` with `Y ~ X1`.\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) Naive: treat f as if it were truth (only on unlabeled)\nnaive_model <- lm(f ~ X1, data = filter(dat_ipd, set_label == \"unlabeled\"))\n\n# 2) Classical: regress true Y on X1, only on the labeled set\nclassical_model <- lm(Y ~ X1, data = filter(dat_ipd, set_label == \"labeled\"))\n```\n:::\n\n\n\n\n:::\n\nLet's also extract the coefficient summaries using the `tidy` method and compare the \nresults of the two approaches:\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnaive_df <- tidy(naive_model) |>\n  mutate(method = \"Naive\") |>\n  filter(term == \"X1\") |>\n  select(method, estimate, std.error) \n\nclassical_df <- tidy(classical_model) |>\n  mutate(method = \"Classical\") |>\n  filter(term == \"X1\") |>\n  select(method, estimate, std.error)\n\nbind_rows(naive_df, classical_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  method    estimate std.error\n  <chr>        <dbl>     <dbl>\n1 Naive        0.730    0.0145\n2 Classical    1.48     0.155 \n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\n* The **naive** coefficient is *attenuated*, or *biased* toward zero, since the predictions are imperfect.\n* The **classical** coefficient is unbiased but has a larger standard error due to the smaller sample size.\n\n:::\n\n## IPD: Corrected Inference with `ipd::ipd()`\n\nThe single wrapper function `ipd()` implements multiple IPD methods (e.g., \n**Chen & Chen**, **PostPI**, **PPI**, **PPI++**, **PSPA**) for various\ninferential tasks (e.g., **mean** and **quantile** estimation, **ols**, \n**logistic**, and **poisson** regression).  \n\n::: {.callout-important}\n\n## Reminder\n\nBasic usage of `ipd()`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nipd(\n  formula = Y - f ~ X1,     # The downstream inferential model\n  method  = \"pspa\"          # The IPD method to run \n  model   = \"ols\"           # The type of inferential model\n  data    = dat_ipd,        # A data.frame with columns:\n                            #   - set_label: {\"labeled\", \"unlabeled\"}\n                            #   - Y: true outcomes (for labeled data)\n                            #   - f: predicted outcomes\n                            #   - X covariates (here X1, X2, X3, X4)\n  label   = \"set_label\",    # Column name indicating \"labeled\"/\"unlabeled\"\n)\n```\n:::\n\n\n\n\n:::\n\n### Exercise 6: IPD Model Fitting via the PSPA Estimator\n\nLet's run one method, `pspa`, proposed by [Miao et al., 2024](https://arxiv.org/pdf/2311.14220). The **PSPA** \nestimator is an IPD method that combines information from:\n\n1. **Labeled data** (where the true outcome, $Y$, and model predictions, $f$, are available), and  \n2. **Unlabeled data** (where only model predictions, $f$, are available).\n\nRather than treating the predicted outcomes with the same importance as the true outcomes, \nthe method estimates a **data-driven weight**, $\\hat{\\omega}$, and applies it to the predicted \noutcome contributions:\n\n$$\n\\hat{\\beta}_\\text{pspa} = \\hat{\\beta}_\\text{classical} - \\hat{\\omega}\\cdot (\\hat{\\gamma}_\\text{naive}^l - \\hat{\\gamma}_\\text{naive}^u),\n$$\n\nwhere $\\hat{\\beta}_{\\rm classical}$ is the estimate from the **classical** \nregression, $\\hat{\\gamma}_{\\rm naive}^l$ is the estimate from the **naive**\nregression in the *labeled* data, $\\hat{\\gamma}_{\\rm naive}^u$ is the estimate\nfrom the **naive** regression in the *unlabeled* data, and \n$\\hat{\\omega}$ reflects the amount of additional information carried by the\npredictions. By adaptively weighting the unlabeled information, the **PSPA**\nestimator achieves greater precision than by using the labeled data alone, \nwithout sacrificing validity, even when the predictions are imperfect.  \n\nLet's call the method using the `ipd()` function and collect the estimate for \nthe slope of `X1` in a linear regression (`model = \"ols\"`):\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nipd_model <- ipd(\n  formula = Y - f ~ X1,\n  data    = dat_ipd,\n  label   = \"set_label\",\n  method  = \"pspa\",\n  model   = \"ols\"\n)\n\nipd_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIPD inference summary\n  Method:   pspa \n  Model:    ols \n  Formula:  Y - f ~ X1 \n\nCoefficients:\n            Estimate Std. Error z value  Pr(>|z|)    \n(Intercept)  0.88014    0.14702  5.9865 2.143e-09 ***\nX1           1.43248    0.14628  9.7929 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\nThe `ipd_model` is an `S4` object with slots for things like the coefficient, se, \nci, coefTable, fit, formula, data_l, data_u, method, model, and intercept. We \ncan extract the coefficient table using `ipd`'s `tidy` helper and compare with\nthe naive and classical methods:\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the coefficient estimates\nipd_df <- tidy(ipd_model) |>\n  mutate(method = \"IPD\") |>\n  filter(term == \"X1\") |>\n  select(method, estimate, std.error)\n\n# Combine with naive & classical:\ncompare_tab <- bind_rows(naive_df, classical_df, ipd_df)\ncompare_tab\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  method    estimate std.error\n  <chr>        <dbl>     <dbl>\n1 Naive        0.730    0.0145\n2 Classical    1.48     0.155 \n3 IPD          1.43     0.146 \n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n### Exercise 7: Visualizing Uncertainty\n\nLet's plot the coefficient estimates and 95% CIs for each of the naive, classical, and IPD methods:\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Forest plot of estimates and 95% confidence intervals\ncompare_tab |>\n  mutate(\n    lower = estimate - 1.96 * std.error,\n    upper = estimate + 1.96 * std.error\n  ) |>\n  ggplot(aes(x = estimate, y = method)) +\n    geom_point(size = 2) +\n    geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2) +\n    geom_vline(xintercept = 1.5, linetype = \"dashed\", color = \"red\") +\n    labs(\n      title = \"Comparison of slope estimates \\u00B1 1.96·SE\",\n      x = expression(hat(beta)[X1]),\n      y = \"\"\n    ) +\n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Unit00_GettingStarted_files/figure-html/unnamed-chunk-15-1.png){width=576}\n:::\n:::\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\n* The dashed red line at **1.5** is the *true* data-generating effect for $X_1$.\n* Compare how far each method's interval is from 1.5, and whether 1.5 lies inside each interval.\n* 'Naive' often severely underestimates (biased); 'Classical' is unbiased but wide; IPD methods cluster around 1.5 with better coverage than 'naive,' often similar to classical but sometimes narrower.\n\n:::\n\n### Exercise 8: Inspecting Results\n\nUse `tidy()`, `glance()`, and `augment()` on `ipd_model`. Compare the \ncoefficient estimate and standard error for `X1` with the naive fit.\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(ipd_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  <chr>          <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)    0.880     0.147    0.592      1.17\n2 X1             1.43      0.146    1.15       1.72\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(ipd_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 6\n  method model intercept nobs_labeled nobs_unlabeled call      \n  <chr>  <chr> <lgl>            <int>          <int> <chr>     \n1 pspa   ols   TRUE               500           1500 Y - f ~ X1\n```\n\n\n:::\n\n```{.r .cell-code}\naugment(ipd_model) |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,500\nColumns: 9\n$ set_label <chr> \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabel…\n$ Y         <dbl> -3.8296223, -3.0350282, -5.3736718, 2.6076634, -4.7813463, -…\n$ f         <dbl> -1.9014253, -0.1679210, -0.3030749, 0.1249168, -0.8998546, -…\n$ X1        <dbl> -1.35723063, -1.29269781, -1.51720731, 0.85917603, -1.214617…\n$ X2        <dbl> 0.01014789, 1.56213812, 0.41284605, -1.18886219, 0.71454993,…\n$ X3        <dbl> -1.42521509, 1.73298966, 1.66085181, -0.85343610, 0.26905593…\n$ X4        <dbl> -1.1645365, -0.2522693, -1.3945975, 0.3959429, 0.5980741, 1.…\n$ .fitted   <dbl> -1.0640638, -0.9716221, -1.2932266, 2.1108854, -0.8597740, 1…\n$ .resid    <dbl> -2.7655584, -2.0634061, -4.0804452, 0.4967781, -3.9215723, -…\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with naive\nbroom::tidy(naive_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -0.118    0.0146     -8.07 1.40e-15\n2 X1             0.730    0.0145     50.3  0       \n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n## Summary and Key Takeaways\n\n1. **Naive** regression on predicted outcomes is biased (point estimates are pulled toward zero and SEs are artificially small).\n2. **Classical** regression on the labeled data alone is unbiased but inefficient when the labeled set is small.\n3. IPD methods (Chen & Chen, PPI, PPI++, PostPI, PSPA) strike a balance: they use predictions to effectively enlarge sample size but adjust for prediction error to avoid bias.\n4. Even with 'simple' linear prediction models, IPD corrections can drastically improve inference on downstream regression coefficients.\n\n## Further Exploration\n\n* Try other methods such as **PPI++** (`\"ppi_plusplus\"`). How do the results compare?\n* Repeat the analysis for a **logistic** model by setting `model = \"logistic\"` in both `simdat()` and `ipd()`.\n\nHappy coding! Feel free to modify and extend these exercises for your own data.\n\n---\n\n*This is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org*\n",
    "supporting": [
      "Unit00_GettingStarted_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}