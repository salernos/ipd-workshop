{
  "hash": "bf5dfe30844a8c2488c08201675b2a4e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Income Inequality\"\nsubtitle: \"Predicting Earnings with Age Effects\"\ndescription:  |\n  Use census-style features to predict income and perform valid OLS inference \n  on age effects with mixed labeled and unlabeled data.\nimage: images/income_cover.png\nunit: 0\ncore: true\nformat:\n  html:\n    toc: true\n    number-sections: false\n    code-fold: true\n    code-tools: false\nexecute:\n  warning: false\n  message: false\n---\n\n\n\n\n\n## Learning Objectives\n\nBy the end of this module you will be able to:\n\n1. Set up labeled vs unlabeled splits when **income is predicted** but only some true incomes are observed.\n2. Estimate the **OLS coefficient for age** while controlling for sex.\n3. Use `ipd()` for:\n   - `postpi_analytic`, `ppi`, `ppi_plusplus`, `pspa`, `chen`\n4. Compare CI widths across labeled sample sizes.\n5. Run a power experiment to find the smallest labeled `n` achieving **80% power** for a one-sided null.\n\n::: {.callout-note collapse=\"true\"}\n## Background + references (foldable)\n\nThe `ipd` package provides a unified `ipd()` interface for inference when outcomes are partially predicted (e.g., ML predictions available broadly, true labels only on a subset). This module mirrors a Folktables/Census income setup where predictions are generated via gradient boosting (e.g., XGBoost), but the inferential target is a downstream **linear regression coefficient**.\n:::\n\n## 1. Setup\n\n### Packages\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"ipd\")\nlibrary(ipd)\n\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n\n### Data: expected columns\n\nWe assume a dataset with:\n\n* `Y`: true yearly income (numeric), observed for labeled subset\n* `Yhat`: predicted income (numeric), observed for everyone\n* covariates: `age` (numeric) and `sex` (binary or factor)\n* (optional) other covariates\n\n::: {.callout-important}\n\n## Plug in your real dataset\n\nNEED REAL DATA\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuse_simulated <- TRUE\n\nif (!use_simulated) {\n  dat_full <- read.csv(\"data/census_income_ca2019.csv\")\n  stopifnot(all(c(\"Y\",\"Yhat\",\"age\",\"sex\") %in% names(dat_full)))\n  dat_full <- as_tibble(dat_full)\n} else {\n  set.seed(1)\n  n <- 15000\n\n  # Simulate: income depends on age and sex\n  sex <- rbinom(n, 1, 0.48)\n  age <- pmin(pmax(round(rnorm(n, mean = 42, sd = 12)), 18), 80)\n\n  # True income model (linear signal + noise)\n  beta0 <- 15000\n  beta_age <- 900          # target coefficient ~ 900 dollars/year per year of age\n  beta_sex <- 6000         # illustrative\n  eps <- rnorm(n, sd = 20000)\n\n  Y <- beta0 + beta_age * age + beta_sex * sex + eps\n\n  # Predicted income: biased/noisy proxy (as if from an ML model trained on prior year)\n  Yhat <- Y + rnorm(n, sd = 12000) + 1500   # add some bias\n\n  dat_full <- tibble(Y = Y, Yhat = Yhat, age = age, sex = sex)\n}\n\n# Build the downstream design variables.\ndat_full <- dat_full %>%\n  mutate(\n    sex = factor(sex, levels = c(0,1), labels = c(\"F\",\"M\"))\n  )\n```\n:::\n\n\n\n\n\n## 2. Labeled vs unlabeled splits\n\nWe create a stacked dataset where the unlabeled rows have `Y = NA` but retain `Yhat` and covariates.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_split <- function(dat, n_labeled, seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n\n  idx <- sample.int(nrow(dat), size = n_labeled, replace = FALSE)\n  labeled   <- dat[idx, ] %>% mutate(set_label = \"labeled\")\n  unlabeled <- dat[-idx,] %>% mutate(Y = NA_real_, set_label = \"unlabeled\")\n\n  bind_rows(labeled, unlabeled)\n}\n```\n:::\n\n\n\n\n\n## 3. Fit many methods with `ipd()`\n\nWe target the coefficient on `age` in:\n\n`Y ~ age + sex`\n\nWith `ipd`, we write:\n\n`Y - Yhat ~ age + sex`  (where `Yhat` is the prediction) ([rdrr.io](https://rdrr.io/cran/ipd/man/ipd.html))\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_ipd_methods <- function(stacked_df, alpha = 0.05, n_t = Inf) {\n\n  methods <- c(\"postpi_analytic\", \"postpi_boot\", \"ppi\", \"ppi_plusplus\", \"pspa\")\n\n  fits <- map(methods, ~ ipd(\n    formula = Y - Yhat ~ age + sex,\n    method  = .x,\n    model   = \"linear\",\n    data    = stacked_df,\n    label   = \"set_label\",\n    alpha   = alpha,\n    n_t     = n_t,\n    alternative = \"two-sided\"\n  ))\n\n  names(fits) <- methods\n  fits\n}\n\nfit_classical_labeled <- function(stacked_df, alpha = 0.05) {\n  labeled <- stacked_df %>% filter(set_label == \"labeled\")\n  m <- lm(Y ~ age + sex, data = labeled)\n\n  est <- coef(m)[[\"age\"]]\n  se  <- sqrt(vcov(m)[[\"age\",\"age\"]])\n  tcrit <- qt(1 - alpha/2, df = df.residual(m))\n  ci_beta <- c(est - tcrit*se, est + tcrit*se)\n\n  list(\n    coefficients = coef(m),\n    se = sqrt(diag(vcov(m))),\n    ci = rbind(age = ci_beta),\n    fit = m\n  )\n}\n\nfit_naive_imputation <- function(stacked_df, alpha = 0.05) {\n  # naive: treat Yhat as if it were Y and run standard OLS\n  m <- lm(Yhat ~ age + sex, data = stacked_df)\n\n  est <- coef(m)[[\"age\"]]\n  se  <- sqrt(vcov(m)[[\"age\",\"age\"]])\n  tcrit <- qt(1 - alpha/2, df = df.residual(m))\n  ci_beta <- c(est - tcrit*se, est + tcrit*se)\n\n  list(\n    coefficients = coef(m),\n    se = sqrt(diag(vcov(m))),\n    ci = rbind(age = ci_beta),\n    fit = m\n  )\n}\n\nextract_ci_for_term <- function(fit_obj, term = \"age\") {\n  # ipd objects have $ci as matrix; baselines mimic that structure\n  ci <- fit_obj$ci[term, ]\n  c(lower = ci[1], upper = ci[2])\n}\n```\n:::\n\n\n\n\n\n## 4. CI experiment: width vs labeled sample size\n\nThis mirrors your Python loop over `ns` and `num_trials`, storing `lower`, `upper`, `width`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.05\nns <- as.integer(seq(100, 2000, length.out = 10))\nnum_trials <- 100\n\n# Truth from the fully-labeled benchmark dataset (only for evaluation)\ntrue_theta <- coef(lm(Y ~ age + sex, data = dat_full))[[\"age\"]]\n\nrun_one <- function(n_labeled, trial) {\n  stacked <- make_split(dat_full, n_labeled = n_labeled, seed = 10000 + 10*n_labeled + trial)\n\n  ipd_fits <- fit_ipd_methods(stacked, alpha = alpha)\n  classical <- fit_classical_labeled(stacked, alpha = alpha)\n  naive_imp <- fit_naive_imputation(stacked, alpha = alpha)\n\n  rows <- list(\n    tibble(method = names(ipd_fits)) %>%\n      mutate(ci = map(ipd_fits, extract_ci_for_term)) %>%\n      unnest_wider(ci),\n\n    tibble(method = \"classical\") %>%\n      mutate(!!!extract_ci_for_term(classical)),\n\n    tibble(method = \"imputation\") %>%\n      mutate(!!!extract_ci_for_term(naive_imp))\n  )\n\n  bind_rows(rows) %>%\n    mutate(n = n_labeled, trial = trial, width = upper - lower)\n}\n\nres_ci <- crossing(n = ns, trial = seq_len(num_trials)) %>%\n  mutate(out = map2(n, trial, run_one)) %>%\n  select(out) %>%\n  unnest(out)\n```\n:::\n\n\n\n\n\n### Plot 1: example intervals (5 random trials)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nexample_trials <- sample(unique(res_ci$trial), size = 5)\n\nex <- res_ci %>%\n  filter(trial %in% example_trials, n == ns[2]) %>%\n  mutate(method = factor(method, levels = c(\"classical\",\"imputation\",\"postpi_analytic\",\"postpi_boot\",\"ppi\",\"ppi_plusplus\",\"pspa\")))\n\nggplot(ex, aes(y = method, x = (lower + upper)/2, xmin = lower, xmax = upper)) +\n  geom_vline(xintercept = true_theta, linetype = 2) +\n  geom_errorbarh(height = 0.2) +\n  geom_point(size = 1.8) +\n  labs(\n    title = paste0(\"Age coefficient CIs (n_labeled = \", ns[2], \", 5 random trials)\"),\n    x = \"OLS coefficient on age (dollars/year per year of age)\",\n    y = NULL,\n    caption = \"Dashed line = benchmark OLS coefficient from the full labeled data.\"\n  ) +\n  theme_bw()\n```\n:::\n\n\n\n\n\n### Plot 2: average CI width vs n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_width <- res_ci %>%\n  group_by(method, n) %>%\n  summarize(mean_width = mean(width, na.rm = TRUE), .groups = \"drop\")\n\nggplot(avg_width, aes(x = n, y = mean_width, group = method)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Average CI width vs labeled sample size\",\n    x = \"Number of labeled observations\",\n    y = \"Mean CI width (coefficient scale)\"\n  ) +\n  theme_bw()\n```\n:::\n\n\n\n\n\n::: {.callout-tip}\n\n## Interpretation checklist\n\n* If `ppi` / `ppi_plusplus` CIs are **narrower** than classical at the same `n`, youâ€™re leveraging unlabeled predictions to reduce variance while staying valid.\n* `imputation` often looks appealing but can be biased and anti-conservative (too narrow).\n  :::\n\n## 5. Power experiment: smallest n for 80% power\n\nWe test the one-sided null:\n\n[\nH_0: \\theta^* < 800\n]\n\nReject if the **lower CI bound > 800**.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha_pval <- alpha\nnum_experiments <- 100\nthreshold <- 800\n\npower_at_n <- function(n_labeled, method = c(\"ppi\",\"classical\")) {\n  method <- match.arg(method)\n\n  rej <- replicate(num_experiments, {\n    stacked <- make_split(dat_full, n_labeled = n_labeled)\n\n    if (method == \"ppi\") {\n      fit <- ipd(\n        Y - Yhat ~ age + sex,\n        method = \"ppi\",\n        model = \"linear\",\n        data = stacked,\n        label = \"set_label\",\n        alpha = alpha_pval,\n        alternative = \"one-sided\"\n      )\n      ci <- extract_ci_for_term(fit, term = \"age\")\n      as.integer(ci[\"lower\"] > threshold)\n    } else {\n      fit <- fit_classical_labeled(stacked, alpha = alpha_pval)\n      ci <- extract_ci_for_term(fit, term = \"age\")\n      as.integer(ci[\"lower\"] > threshold)\n    }\n  })\n\n  mean(rej)\n}\n\nfind_n_for_power <- function(method = c(\"ppi\",\"classical\"),\n                             n_min = 100, n_max = 2000, target = 0.80) {\n  method <- match.arg(method)\n\n  f <- function(n) power_at_n(as.integer(n), method = method) - target\n\n  grid <- unique(round(seq(n_min, n_max, length.out = 20)))\n  vals <- map_dbl(grid, f)\n\n  if (all(vals < 0)) return(NA_integer_)\n  if (all(vals > 0)) return(n_min)\n\n  i <- which(vals >= 0)[1]\n  lo <- grid[max(1, i-1)]\n  hi <- grid[i]\n\n  as.integer(uniroot(f, lower = lo, upper = hi)$root)\n}\n\nn_ppi <- find_n_for_power(\"ppi\", n_min = 100, n_max = 2000)\nn_cls <- find_n_for_power(\"classical\", n_min = 100, n_max = 2000)\n\ntibble(\n  method = c(\"ppi\",\"classical\"),\n  n_for_80_power = c(n_ppi, n_cls)\n)\n```\n:::\n\n\n\n\n\n## 6. Exercises\n\n::: {.callout-warning}\n\n## Exercise 1 (core): change target coefficient\n\nChange the null threshold from 800 to 600 or 1000:\n\n* How does the required labeled `n` change?\n* Does PPI keep a consistent advantage over classical?\n  :::\n\n::: {.callout-warning collapse=\"true\"}\n\n## Exercise 2 (intermediate): add interactions\n\nFit:\n\n`Y - Yhat ~ age * sex`\n\nInterpret the age effect for each sex and compare how much unlabeled data helps in each subgroup.\n:::\n\n::: {.callout-warning collapse=\"true\"}\n\n## Exercise 3 (advanced): heteroskedastic-robust SEs\n\nModify the classical baseline to use robust (HC) SEs (e.g., `sandwich::vcovHC`) and compare with IPD methods.\nDiscuss what changes you expect when prediction errors vary with age.\n:::\n",
    "supporting": [
      "Unit04_Income_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}