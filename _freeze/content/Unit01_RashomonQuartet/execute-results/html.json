{
  "hash": "2115b11de2cdffdfe231130d388a63b5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unit 01: The Rashomon Quartet\"\nformat: html\n---\n\n\n\n\n## Overview\n\nIn \"Performance is not enough: the story of Rashomon's quartet\" \n([Biecek et al., 2023](https://www.tandfonline.com/doi/full/10.1080/10618600.2024.2344616)), \nfour different models (linear regression, decision trees, random forests, and\nneural networks) achieve near-identical predictive performance on the same \ndataset. However, their interpretations vary significantly. In this module, \nwe will: \n\n1. **Fit** each model on a *training* set. \n2. **Evaluate** their *predictive* performance on a *testing* set. \n3. **Demonstrate** how the **naive**, **classical**, and **IPD**-based *inference* results differ.\n\n>\n> **Note:** We will replicate the results of the Rashomon Quartet analysis following:\n>\n> https://github.com/MI2DataLab/rashomon-quartet\n>\n\n## Background on the Rashomon Quartet\n\nThe underlying datasets are synthetically generated to include three covariates, \n$\\boldsymbol{X} = (X_1, X_2, X_3)^\\top$, so that\n\n$$\n\\boldsymbol{X} = (X_1, X_2, X_3)^\\top \\sim \\mathcal{N}_3 \\left(\\boldsymbol{0}, \\Sigma\\right), \\quad \\Sigma_{ij} = \\begin{cases} 1 & i=j,\\\\ 0.9 & i \\neq j\\end{cases}\n$$\nThe outcomes, $Y$, are then generated to depend only on $X_1$ and, to a lesser\nextent, $X_2$, given by\n\n$$\nY = \\sin\\left(\\frac{3X_1 + X_2}{5}\\right) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}\\left(0, \\frac{1}{3}\\right).\n$$\n\n>\n> **Note:** $X_3$ does not appear in the true data generating mechanism, but is highly correlated with $X_1$ and $X_2$.\n>\n\nTwo independent batches of 1,000 observations (a *training* set and a \n*testing* set) are generated under this model. We will fit the four candidate \nmodels on the 1,000-row *training* data, and evaluate them separately on the\n1,000-row *testing* data. As we will show, all four achieve $R^2 \\approx 0.729$ \nand $\\mathrm{RMSE} \\approx 0.354$ on the *testing* set. However, their \ninterpretations diverge:\n\n* **Linear regression** captures a near-linear approximation of $\\sin\\left(\\tfrac{3X_{1} + X_{2}}{5}\\right)$.\n* **Decision tree** approximates the nonlinear relationship via a small set of splits.\n* **Random forest** produces a smooth ensemble derivative of the tree.\n* **Neural network** learns a different nonlinear basis with hidden units.\n\n![*Source: https://github.com/MI2DataLab/rashomon-quartet*](images/rashomon_overview.png){width=100%}\n\nIn addition to comparing interpretability across models, we will demonstrate \nhow to perform valid downstream inference for $Y$ on $\\boldsymbol{X}$ when $Y$ \nis only partially observed. We will:\n\n1. **Train** each predictive model on the full 1,000-row *training* set.\n2. **Predict** on all 1,000 *testing* observations.\n3. **Randomly split** the 1,000 *testing* rows into two groups:\n\n   * *Labeled* subset (with true $Y$ observed).\n   * *Unlabeled* subset (with only predicted $f$).\n4. In the *labeled* subset, regress $Y$ on $\\boldsymbol{X}$ (\"Classical\").\n5. In the *unlabeled* subset, regress $f$ on $\\boldsymbol{X}$ (\"Naive\").\n6. Combine the two subsets and use `ipd::ipd()` to correct bias and variance.\n\nThis mirrors realistic scenarios where the *analytic* dataset (here the \n*testing* set) has only some observations labeled, and we want to use model \npredictions for the rest while still making valid inference.\n\n## Setup and Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install if needed:\n# install.packages(c(\"ipd\",\"DALEX\",\"partykit\",\"randomForest\",\"neuralnet\",\"GGally\",\"tidyverse\"))\n\nlibrary(ipd)           # Inference with Predicted Data\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ipd' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(DALEX)         # Model Explanation\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'DALEX' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWelcome to DALEX (version: 2.5.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(partykit)      # Decision Trees\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'partykit' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: grid\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: libcoin\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'libcoin' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: mvtnorm\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'mvtnorm' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(randomForest)  # Random Forests\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'randomForest' was built under R version 4.4.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrandomForest 4.7-1.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(neuralnet)     # Neural Networks\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'neuralnet' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(GGally)        # Pairwise Plots\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'GGally' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: ggplot2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'ggplot2'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:randomForest':\n\n    margin\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)     # Data Manipulation and Visualization\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidyverse' was built under R version 4.4.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tibble' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidyr' was built under R version 4.4.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'readr' was built under R version 4.4.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'purrr' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'dplyr' was built under R version 4.4.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'stringr' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'forcats' was built under R version 4.4.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'lubridate' was built under R version 4.4.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ lubridate 1.9.4     ✔ tibble    3.3.0\n✔ purrr     1.1.0     ✔ tidyr     1.3.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()  masks randomForest::combine()\n✖ dplyr::compute()  masks neuralnet::compute()\n✖ dplyr::explain()  masks DALEX::explain()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ ggplot2::margin() masks randomForest::margin()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n:::\n\n\n\n\n### Read-In the Training and Testing Data\n\nWe have included two CSV files from the \n[MI2DataLab/rashomon-quartet](https://github.com/MI2DataLab/rashomon-quartet) \nrepository on GitHub. Note that these CSVs are *semicolon-delimited*.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use read_delim to parse semicolons\ntrain <- read_delim(\"data/rq_train.csv\", delim = \";\", col_types = cols(), show_col_types = FALSE)\ntest  <- read_delim(\"data/rq_test.csv\",  delim = \";\", col_types = cols(), show_col_types = FALSE)\n\n# Show the first few rows of each\nglimpse(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,000\nColumns: 4\n$ y  <dbl> -0.47048454, 0.17177895, 0.07295012, -0.34301361, 0.47167402, 0.191…\n$ x1 <dbl> -0.51211395, 0.49038643, -0.89461065, 0.02725393, 0.44364997, -0.26…\n$ x2 <dbl> -0.34174653, 0.25511508, -0.28744899, -0.09277031, 0.52147200, 0.28…\n$ x3 <dbl> 0.255531783, -0.179610715, -0.310620340, -0.109603650, 0.920225141,…\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,000\nColumns: 4\n$ y  <dbl> -0.44547765, -0.67618374, -0.78822993, -0.55024530, -0.49205810, -0…\n$ x1 <dbl> -0.553685719, -0.965436826, -0.962515641, -0.871816734, -1.96565982…\n$ x2 <dbl> -0.74221752, -0.19959118, -1.25801758, -1.08469684, -1.43546177, -0…\n$ x3 <dbl> -1.15266476, -1.00944121, -1.34017902, -0.84322216, -1.29871031, -0…\n```\n\n\n:::\n:::\n\n\n\n\n> **Confirm Dimensions and Column Names:**\n>\n> * Each dataset should have exactly 1,000 rows and four columns: `y`, `x1`, `x2`, `x3`.\n> * We will use all 1,000 rows of `train` to fit each predictive model, and then randomly split the 1,000-row `test` for IPD.\n\n## Predictive Model Training on the Training Set\n\nBelow, we fit four predictive models using the *training* set:\n\n1. **Decision Tree** via `ctree()` (max depth = 3, minsplit = 250).\n2. **Linear Regression** via `lm()`.\n3. **Random Forest** via `randomForest()` (100 trees).\n4. **Neural Network** via `neuralnet()` (two hidden layers: 8 units, 4 units; threshold = 0.05).\n\nWe then evaluate each model's performance in the *testing* set using the `DALEX` \npackage to extract the model's $R^2$ and $RMSE$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1568)\n\n# 1) Decision Tree (ctree)\nmodel_dt <- ctree(\n  formula = y ~ x1 + x2 + x3,\n  data    = train,\n  control = ctree_control(maxdepth = 3, minsplit = 250)\n)\n\nexp_dt <- DALEX::explain(\n  model   = model_dt,\n  data    = test |> select(x1, x2, x3),\n  y       = test$y,\n  verbose = FALSE,\n  label   = \"Decision Tree\"\n)\n\nmp_dt <- model_performance(exp_dt)\n\n# 2) Linear Regression\nmodel_lm <- lm(y ~ x1 + x2 + x3, data = train)\n\nexp_lm <- DALEX::explain(\n  model   = model_lm,\n  data    = test |> select(x1, x2, x3),\n  y       = test$y,\n  verbose = FALSE,\n  label   = \"Linear Regression\"\n)\n\nmp_lm <- model_performance(exp_lm)\n\n# 3) Random Forest\nmodel_rf <- randomForest(\n  formula = y ~ x1 + x2 + x3,\n  data    = train,\n  ntree   = 100\n)\n\nexp_rf <- DALEX::explain(\n  model   = model_rf,\n  data    = test |> select(x1, x2, x3),\n  y       = test$y,\n  verbose = FALSE,\n  label   = \"Random Forest\"\n)\n\nmp_rf <- model_performance(exp_rf)\n\n# 4) Neural Network\nmodel_nn <- neuralnet(\n  formula      = y ~ x1 + x2 + x3,\n  data         = train,\n  hidden       = c(8, 4),\n  threshold    = 0.05,\n  linear.output = TRUE\n)\n\npredict_nn <- function(object, newdata) {\n  as.numeric(predict(object, newdata))\n}\n\nexp_nn <- DALEX::explain(\n  model            = model_nn,\n  data             = test |> select(x1, x2, x3),\n  y                = test$y,\n  predict_function = predict_nn,\n  verbose          = FALSE,\n  label            = \"Neural Network\"\n)\n\nmp_nn <- model_performance(exp_nn)\n\n# Save DALEX explainers for later reuse\nsave(exp_dt, exp_lm, exp_rf, exp_nn, file = \"models.RData\")\n\n# Compile performance metrics into one data frame\nmp_all <- list(\n  \"Linear Regression\" = mp_lm,\n  \"Decision Tree\"     = mp_dt,\n  \"Random Forest\"     = mp_rf,\n  \"Neural Network\"    = mp_nn\n)\n\nR2_values   <- sapply(mp_all, function(x) x$measures$r2)\nRMSE_values <- sapply(mp_all, function(x) x$measures$rmse)\n\nperf_df <- tibble(\n  Model = names(R2_values),\n  R2    = round(as.numeric(R2_values),   4),\n  RMSE  = round(as.numeric(RMSE_values), 4)\n)\n\nperf_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  Model                R2  RMSE\n  <chr>             <dbl> <dbl>\n1 Linear Regression 0.729 0.354\n2 Decision Tree     0.729 0.354\n3 Random Forest     0.728 0.354\n4 Neural Network    0.730 0.352\n```\n\n\n:::\n:::\n\n\n\n\n> **Interpretation:**\n> All four models (Linear Regression, Decision Tree, Random Forest, Neural Network) deliver essentially identical predictive performance on the *testing* set:\n>\n> $$\n> R^2 \\approx 0.729,\\quad \\mathrm{RMSE} \\approx 0.354.\n> $$\n\n## Inspecting the Prediction Models\n\nWe now take a quick look at each model's internal structure or summary.\n\n>\n> **Note:** \n>\n> * Often, in practice, we do not know the operating characteristics of the predictive model or have access to its training data. \n> * IPD methods are distinct in that they are agnostic to the source or form of these 'black-box' predictions\n>\n\n### Decision Tree Visualization\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_dt, main = \"Decision Tree (ctree) Structure\")\n```\n\n::: {.cell-output-display}\n![](Unit01_RashomonQuartet_files/figure-html/unnamed-chunk-4-1.png){width=100%}\n:::\n:::\n\n\n\n\n> The tree has depth 3, with splits on $x_1, x_2, x_3$ that approximate $\\sin\\left(\\tfrac{3X_{1} + X_{2}}{5}\\right)$ piecewise.\n\n### Linear Regression Summary\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.68045 -0.24213  0.01092  0.23409  1.26450 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.01268    0.01114  -1.138    0.255    \nx1           0.48481    0.03001  16.157  < 2e-16 ***\nx2           0.14316    0.02966   4.826 1.61e-06 ***\nx3          -0.03113    0.02980  -1.045    0.296    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.352 on 996 degrees of freedom\nMultiple R-squared:  0.7268,\tAdjusted R-squared:  0.726 \nF-statistic: 883.4 on 3 and 996 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n> The OLS coefficients show how the linear model approximates the underlying sine function. In particular, the slope on $x_1$ will be roughly an average of this over the training distribution.\n\n### Random Forest Summary\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = y ~ x1 + x2 + x3, data = train, ntree = 100) \n               Type of random forest: regression\n                     Number of trees: 100\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1193538\n                    % Var explained: 73.58\n```\n\n\n:::\n:::\n\n\n\n\n> We see OOB MSE and variance explained on the training data. The forest builds many trees of depth >3 and averages them to approximate the sine structure.\n\n### Neural Network Topology\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_nn, rep = \"best\", main = \"Neural Network (8,4) Topology\")\n```\n\n::: {.cell-output-display}\n![](Unit01_RashomonQuartet_files/figure-html/unnamed-chunk-7-1.png){width=100%}\n:::\n:::\n\n\n\n\n> This diagram shows two hidden layers (8 units $\\to$ 4 units) with activation functions that capture nonlinearity. The network approximates $\\sin\\left(\\tfrac{3X_{1} + X_{2}}{5}\\right)$ more flexibly than the decision tree or random forest.\n\n## Variable Importance (`DALEX`)\n\nNext, we compute variable importance for each model via `DALEX::model_parts()`. \nWe request `type=\"difference\"` so that importance is measured in terms of loss \nof performance (RMSE) when that variable is permuted.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_dt <- model_parts(exp_dt, N = NULL, B = 1, type = \"difference\")\nimp_lm <- model_parts(exp_lm, N = NULL, B = 1, type = \"difference\")\nimp_rf <- model_parts(exp_rf, N = NULL, B = 1, type = \"difference\")\nimp_nn <- model_parts(exp_nn, N = NULL, B = 1, type = \"difference\")\n\nplot(imp_dt, imp_nn, imp_rf, imp_lm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the ingredients package.\n  Please report the issue at\n  <https://github.com/ModelOriented/ingredients/issues>.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Unit01_RashomonQuartet_files/figure-html/unnamed-chunk-8-1.png){width=100%}\n:::\n:::\n\n\n\n\n> **Interpretation:**\n>\n> * Each model ranks features $(x_{1},x_{2},x_{3})$ by their impact on predictive accuracy.\n> * Because $\\mathbf{x}$ are correlated, some models (e.g.\\ tree, forest) may split differently than linear regression or neural network.\n\n## Partial-Dependence Plots (`DALEX`)\n\nWe now generate partial-dependence (PD) profiles for each feature under each\nmodel via `DALEX::model_profile()`. This shows how each model's predicted \n$f$ changes when we vary one feature at a time, averaging out the others.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npd_dt <- model_profile(exp_dt, N = NULL)\npd_lm <- model_profile(exp_lm, N = NULL)\npd_rf <- model_profile(exp_rf, N = NULL)\npd_nn <- model_profile(exp_nn, N = NULL)\n\nplot(pd_dt, pd_nn, pd_rf, pd_lm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ingredients package.\n  Please report the issue at\n  <https://github.com/ModelOriented/ingredients/issues>.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Unit01_RashomonQuartet_files/figure-html/unnamed-chunk-9-1.png){width=100%}\n:::\n:::\n\n\n\n\n> **Interpretation:**\n>\n> * For $x_{1}$, the **Linear Regression** PD is a straight line.\n> * The **Decision Tree** PD is piecewise-constant.\n> * The **Random Forest** PD is smoother but still stepwise.\n> * The **Neural Network** PD approximates the sine shape.\n>\n> Because $x_{3}$ does not appear in the generating formula, its PD should be nearly flat—though correlation can induce slight structure.\n\n## Inference with Predicted Data (IPD)\n\nWe now demonstrate how to perform valid inference in the *testing* set when \nonly some test rows have true $Y$. Specifically, we:\n\n1. **Randomly split** the 1,000 *testing* rows into:\n\n   * A *labeled* subset of size $n_l$ (we choose 10%, but this can be adjusted).\n   * An *unlabeled* subset of size $n_u = 1,000 - n_l$.\n2. In the *labeled* subset, regress $Y$ on $X_1$, $X_2$, and $X_3$ (**Classical**).\n5. In the *unlabeled* subset, regress $f$ on $X_1$, $X_2$, and $X_3$ (**Naive**).\n6. Use both subsets in an **IPD** pipeline and run:\n\n   ```r\n   ipd_res <- ipd(\n     formula        = y - f ~ x1 + x2 + x3,\n     data           = test_labeled,\n     unlabeled_data = test_unlabeled,\n     method         = \"pspa\",\n     model          = \"ols\"\n   )\n   ```\n\n   to obtain bias-corrected estimates of $\\beta_1$, $\\beta_2$, and $\\beta_3$.\n \n>  \n> **Note:** Instead of supplying one stacked dataset and the column name for the set labels, we can alternativel supply the *labeled* and *unlabeled* data separately.\n>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\n# Predict outcomes in the testing set\nipd_data <- test |>\n  mutate(\n    \n    preds_lm = c(predict(model_lm, newdata = test)),\n    preds_dt = c(predict(model_dt, newdata = test)),\n    preds_rf = c(predict(model_rf, newdata = test)),\n    preds_nn = c(predict(model_nn, newdata = test)))\n\n# Randomly split the testing set into labeled and unlabeled subsets\nfrac_labeled <- 0.1\n\nn_test    <- nrow(ipd_data)\nn_labeled <- floor(frac_labeled * n_test)\n\nlabeled_idx <- sample(seq_len(n_test), size = n_labeled)\n\ntest_labeled <- ipd_data |>\n  slice(labeled_idx) |>\n  mutate(set_label = \"labeled\")\n  \ntest_unlabeled <- ipd_data |>\n  slice(-labeled_idx) |>\n  mutate(set_label = \"unlabeled\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Classical\nclass_fit <- lm(y ~ x1 + x2 + x3, data = test_labeled)\nclass_df  <- broom::tidy(class_fit) |>\n  mutate(method = \"Classical\") \n  \n# Naive (Linear Model)\nnaive_fit_lm <- lm(preds_lm ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_lm  <- broom::tidy(naive_fit_lm) |>\n  mutate(method = \"Naive (lm)\") \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in summary.lm(x): essentially perfect fit: summary may be unreliable\n```\n\n\n:::\n\n```{.r .cell-code}\n# Naive (Decision Tree)\nnaive_fit_dt <- lm(preds_dt ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_dt  <- broom::tidy(naive_fit_dt) |>\n  mutate(method = \"Naive (dt)\") \n\n# Naive (Random Forest)\nnaive_fit_rf <- lm(preds_rf ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_rf  <- broom::tidy(naive_fit_rf) |>\n  mutate(method = \"Naive (rf)\") \n\n# Naive (Neural Net)\nnaive_fit_nn <- lm(preds_nn ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_nn  <- broom::tidy(naive_fit_nn) |>\n  mutate(method = \"Naive (nn)\") \n\n# IPD (Linear Model)\nipd_fit_lm <- ipd(y - preds_lm ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled, \n  method = \"pspa\", model = \"ols\")\nipd_df_lm  <- tidy(ipd_fit_lm) |>\n  mutate(method = \"IPD (lm)\") \n\n# IPD (Decision Tree)\nipd_fit_dt <- ipd(y - preds_dt ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled,\n  method = \"pspa\", model = \"ols\")\nipd_df_dt  <- tidy(ipd_fit_dt) |>\n  mutate(method = \"IPD (dt)\") \n\n# IPD (Random Forest)\nipd_fit_rf <- ipd(y - preds_rf ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled,\n  method = \"pspa\", model = \"ols\")\nipd_df_rf  <- tidy(ipd_fit_rf) |>\n  mutate(method = \"IPD (rf)\") \n\n# IPD (Neural Net)\nipd_fit_nn <- ipd(y - preds_nn ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled,\n  method = \"pspa\", model = \"ols\")\nipd_df_nn  <- tidy(ipd_fit_nn) |>\n  mutate(method = \"IPD (nn)\") \n\ncombined_df <- bind_rows(class_df,\n  naive_df_lm, naive_df_dt, naive_df_rf, naive_df_nn,\n  ipd_df_lm,   ipd_df_dt,   ipd_df_rf,   ipd_df_nn) |>\n  \n  mutate(\n    conf.low  = estimate - 1.96 * std.error,\n    conf.high = estimate + 1.96 * std.error\n  )\n\ncombined_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 36 × 8\n   term        estimate std.error statistic   p.value method  conf.low conf.high\n   <chr>          <dbl>     <dbl>     <dbl>     <dbl> <chr>      <dbl>     <dbl>\n 1 (Intercept)  0.00872  3.68e- 2  2.37e- 1 8.13e-  1 Classi…  -0.0634   0.0808 \n 2 x1           0.643    1.09e- 1  5.91e+ 0 5.24e-  8 Classi…   0.430    0.856  \n 3 x2           0.0702   8.70e- 2  8.07e- 1 4.22e-  1 Classi…  -0.100    0.241  \n 4 x3          -0.0791   9.94e- 2 -7.96e- 1 4.28e-  1 Classi…  -0.274    0.116  \n 5 (Intercept) -0.0127   4.88e-18 -2.60e+15 0         Naive …  -0.0127  -0.0127 \n 6 x1           0.485    1.29e-17  3.77e+16 0         Naive …   0.485    0.485  \n 7 x2           0.143    1.31e-17  1.09e+16 0         Naive …   0.143    0.143  \n 8 x3          -0.0311   1.31e-17 -2.37e+15 0         Naive …  -0.0311  -0.0311 \n 9 (Intercept) -0.0183   6.76e- 3 -2.71e+ 0 6.89e-  3 Naive …  -0.0315  -0.00506\n10 x1           0.546    1.78e- 2  3.06e+ 1 1.57e-141 Naive …   0.511    0.581  \n# ℹ 26 more rows\n```\n\n\n:::\n:::\n\n\n\n\n### Plotting The Model Estimates\n\nWe now visualize, for each predictive model, the estimated coefficients and\n95% confidence intervals from each inference method:\n\n>\n> **Note:** The vertical dashed line marks the estimated slope for the **Classical** method, which serves as our benchmark for comparison.\n>\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nref_lines <- combined_df %>% \n  filter(method == \"Classical\") %>% \n  select(term, estimate)\n\nggplot(combined_df, aes(x = estimate, y = method, color = method)) +\n  geom_point(size = 2) +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  facet_wrap(~ term, ncol = 4, scales = \"free_x\") +\n  geom_vline(\n    data = ref_lines,\n    aes(xintercept = estimate),\n    linetype = \"dashed\",\n    color    = \"black\"\n  ) +\n  labs(\n    x = expression(hat(beta)),\n    y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`height` was translated to `width`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Unit01_RashomonQuartet_files/figure-html/unnamed-chunk-12-1.png){width=100%}\n:::\n:::\n\n\n\n\n> **Interpretation of Results**\n>\n> 1. The **Naive** estimates are biased due to the prediction error and has artificially narrow confidence intervals.\n> 2. The **Classical** is our benchmark, as it is unbiased, but has wider CI's\n> 3. The **IPD** methods cover the **Classical** estimates and have wider CI's to account for the prediction uncertainty.\n>\n\n## Data Distribution: Train vs Test (`GGally::ggpairs`)\n\nFinally, we visualize pairwise relationships among $(x_{1},x_{2},x_{3},y)$ for train vs test. This confirms that train and test were drawn from the same synthetic distribution.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboth <- bind_rows(\n  train |> mutate(label = \"train\"),\n  test  |> mutate(label = \"test\")\n)\n\nggpairs(\n  both,\n  columns = c(\"x1\", \"x2\", \"x3\", \"y\"),\n  mapping = aes(color = label, alpha = 0.3),\n  lower = list(\n    continuous = wrap(\"points\", size = 0.5, alpha = 0.3),\n    combo      = wrap(\"facethist\", bins = 20)\n  ),\n  diag = list(\n    continuous = wrap(\"densityDiag\", alpha = 0.4, bw = \"SJ\")\n  ),\n  upper = list(\n    continuous = wrap(\"cor\", size = 3, stars = FALSE)\n  )\n) +\n  theme_minimal() +\n  labs(\n    title    = \"Pairwise Distribution: Train vs Test (x1,x2,x3,y)\",\n    subtitle = \"Color = train (blue) vs test (pink)\"\n  )\n```\n\n::: {.cell-output-display}\n![](Unit01_RashomonQuartet_files/figure-html/unnamed-chunk-13-1.png){width=100%}\n:::\n:::\n\n\n\n\n> **Interpretation:**\n>\n> * The diagonals show almost identical marginal distributions of $x_{1},x_{2},x_{3},y$ in train vs test.\n> * The off-diagonals confirm the covariance structure $\\Sigma$ is consistent across both sets.\n\n## Summary and Takeaways\n\n1. **Predictive Performance:**\n\n   * Linear Regression, Decision Tree, Random Forest, and Neural Network each achieve nearly identical test set performance ($R^2 \\approx 0.729, \\mathrm{RMSE} \\approx 0.354$).\n2. **Model Interpretability:**\n\n   * Variable Importance and Partial-Dependence plots highlight how each model \"tells a different story\" about which features matter and how $y$ responds to $x_{1},x_{2},x_{3}$.\n   * The Neural Network's PD best tracks the true sine function; the tree's PD is piecewise-constant; the forest's PD is smoothed piecewise; the linear PD is a straight line.\n3. **Inference on Predicted Data (IPD):**\n\n   * By randomly splitting the test set into \"labeled\" and \"unlabeled\" halves, we simulate a real-world scenario where only some new observations have true $y$.\n   * **Naive** regression of $f \\sim x_{1} + x_{2} + x_{3}$ on the unlabeled set is biased due to prediction error.\n   * **Classical** regression of $y \\sim x_{1} + x_{2} + x_{3}$ on the labeled set is unbiased but less efficient.\n   * **IPD** methods  combine the labeled/unlabeled sets to correct bias and properly estimate variance. \n4. **Key Lesson:**\n\n   * **\"Performance is not enough.\"** Even models with identical $R^2$ can lead to very different bias and variance in downstream inference on covariate effects. IPD provides a principled correction when using model predictions in place of true outcomes for some observations.\n\n## References\n\n* Biecek, Przemysław, et al. \"Performance is not enough: The story told by a Rashomon Quartet.\" *Journal of Computational and Graphical Statistics* 33.3 (2024): 1118-1121. \n* MI2DataLab. \"The Rashomon Quartet.\" [*GitHub repository*](https://github.com/MI2DataLab/rashomon-quartet) (2023).\n\n---\n\n*This is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org*\n",
    "supporting": [
      "Unit01_RashomonQuartet_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}