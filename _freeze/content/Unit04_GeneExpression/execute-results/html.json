{
  "hash": "2a2666b55d665c97e9cfba50ff187ec8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unit 04: Gene Expression with Transformers\"\nformat: html\n---\n\n\n\n## Learning Goals\n\nBy the end of this module, you will be able to:\n\n1. Estimate a population **quantile** (e.g., median) and build a confidence interval using **classical** methods.\n2. Understand why using only **predictions** (“imputation”) can misrepresent uncertainty.\n3. Use **Prediction-Powered Inference (PPI)** and **PPI++** to improve statistical power by leveraging unlabeled predicted outcomes.\n4. Run a simple **power experiment** (find smallest labeled sample size to achieve 80% power at level α).\n5. Place **PostPI**, **PSPA**, **PSPS**, and **Chen & Chen** in the broader IPD landscape.\n\n::: {.callout-note collapse=\"true\"}\n### Background (Vaishnav et al. motivation)\n\nThe goal is to characterize how a population of promoter sequences affects gene expression. A transformer model can predict expression from promoter sequence, enabling large-scale comparisons of promoter populations. But when the *true* expression is only observed for a small subset, we need methods that **propagate prediction error into inference** (instead of treating predictions as truth).\n:::\n\n---\n\n## Setup\n\n### Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\n# IPD methods (PPI/PPI++ quantile CI, plus other IPD methods via wrapper)\nlibrary(ipd)  # CRAN package \"ipd: Inference on Predicted Data\" :contentReference[oaicite:5]{index=5}\n\n# For reading .npz in a robust way\nlibrary(reticulate)\nnp <- import(\"numpy\", delay_load = TRUE)\n\ndir.create(\"data\", showWarnings = FALSE)\ndir.create(\"plots\", showWarnings = FALSE)\n```\n:::\n\n\n\n::: {.callout-tip collapse=\"true\"}\n\n### Reproducibility tip\n\nThis module uses random subsampling to simulate \"labeled\" vs \"unlabeled\" splits. Set a seed whenever you want exact reproducibility.\n:::\n\n---\n\n## Data: gene expression with predictions\n\nIn the original Python notebook, the dataset `gene_expression.npz` is downloaded from Google Drive and contains:\n\n* `Y`: gold-standard expression measurements (labeled outcomes)\n* `Yhat`: transformer-predicted expression values (available for everyone)\n\nWe reproduce that download pattern here.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download if missing (same style as the python example)\nnpz_path <- file.path(\"data\", \"gene_expression.npz\")\nif (!file.exists(npz_path)) {\n  url <- \"https://drive.google.com/uc?id=17PwlvAAKeBYGLXPz9L2LVnNJ66XjuyZd\"\n  download.file(url, destfile = npz_path, mode = \"wb\", quiet = FALSE)\n}\n\ndat <- np$load(npz_path)\n\nY_total    <- as.numeric(dat[[\"Y\"]])\nYhat_total <- as.numeric(dat[[\"Yhat\"]])\n\nn_total <- length(Y_total)\nstopifnot(length(Yhat_total) == n_total)\n\ntibble(\n  n_total = n_total,\n  Y_mean = mean(Y_total),\n  Yhat_mean = mean(Yhat_total),\n  cor = cor(Y_total, Yhat_total)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  n_total Y_mean Yhat_mean   cor\n    <int>  <dbl>     <dbl> <dbl>\n1   61150   6.77      8.27 0.963\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-note collapse=\"true\"}\n\n### What do “labeled” and “unlabeled” mean here?\n\n* **Labeled**: we observe both `(Y, Yhat)` for those units.\n* **Unlabeled**: we only observe `Yhat` (and features if present; here it’s intercept-only).\n  PPI-style methods use labeled data to *calibrate* prediction error and use unlabeled predictions to *increase precision*.\n:::\n\n## Problem setup\n\nWe target the **median** (q = 0.5). We’ll compare:\n\n* **Classical** CI: uses only labeled `Y`\n* **Imputation** CI: treats all `Yhat` as truth\n* **PPI** CI: combines labeled `Y` + predictions `Yhat` + unlabeled predictions\n* **PPI++** CI: an efficiency-tuned version of PPI (power-tuning) ([CRAN][6])\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.05\nq <- 0.50\n\ntrue_q25 <- unname(quantile(Y_total, 0.25))\ntrue_q50 <- unname(quantile(Y_total, 0.50))\ntrue_q75 <- unname(quantile(Y_total, 0.75))\n\nc(true_q25 = true_q25, true_q50 = true_q50, true_q75 = true_q75)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n true_q25  true_q50  true_q75 \n 2.932565  5.650312 10.302128 \n```\n\n\n:::\n:::\n\n\n\n## Construct confidence intervals across labeled sample sizes\n\nWe’ll mimic the Python experiment:\n\n* labeled sizes `ns = seq(100, 2000, length.out = 5)`\n* `num_trials = 100` random splits\n* unlabeled set = everyone not selected as labeled\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\nns <- as.integer(seq(100, 2000, length.out = 5))\nnum_trials <- 100\n\n# helper: compute intervals for a single split\none_split_intervals <- function(idx_labeled) {\n  idx_unlabeled <- setdiff(seq_len(n_total), idx_labeled)\n\n  Y_l    <- Y_total[idx_labeled]\n  f_l    <- Yhat_total[idx_labeled]\n  f_u    <- Yhat_total[idx_unlabeled]\n\n  # ipd provides PPI quantile CI utilities :contentReference[oaicite:7]{index=7}\n  ci_ppi <- ipd::ppi_quantile_ci(Y_l = Y_l, f_l = f_l, f_u = f_u, q = q, alpha = alpha)\n\n  # PPI++ quantile CI (efficiency / power tuning)\n  ci_ppi_pp <- ipd::ppi_plusplus_quantile_ci(Y_l = Y_l, f_l = f_l, f_u = f_u, q = q, alpha = alpha)\n\n  # classical quantile CI on labeled Y only\n  ci_classical <- ipd::classical_quantile_ci(Y = Y_l, q = q, alpha = alpha)\n\n  tibble(\n    method = c(\"PPI\", \"PPI++\", \"Classical\"),\n    lower  = c(ci_ppi[1], ci_ppi_pp[1], ci_classical[1]),\n    upper  = c(ci_ppi[2], ci_ppi_pp[2], ci_classical[2])\n  )\n}\n\n# run experiment\nresults <- purrr::map_dfr(ns, function(n) {\n  purrr::map_dfr(seq_len(num_trials), function(trial) {\n    idx <- sample.int(n_total, n)\n    one_split_intervals(idx) %>%\n      mutate(n = n, trial = trial)\n  })\n})\n\n# Imputation CI (treat Yhat as if it were the true outcome everywhere)\nci_impute <- ipd::classical_quantile_ci(Y = Yhat_total, q = q, alpha = alpha)\n\nresults <- bind_rows(\n  results,\n  tibble(method = \"Imputation\", lower = ci_impute[1], upper = ci_impute[2],\n         n = NA_integer_, trial = 1L)\n) %>%\n  mutate(width = upper - lower)\n\nresults\n```\n:::\n\n\n\n::: {.callout-warning collapse=\"true\"}\n\n### Why “imputation CI” is misleading\n\nA classical CI for the median of `Yhat` reflects uncertainty about the **distribution of predictions**, not uncertainty about the **true outcome distribution**. If predictions are noisy or biased, imputation can give intervals that are too narrow and/or centered incorrectly.\n:::\n\n## Plot results\n\nWe reproduce the spirit of the Python plots:\n\n1. A small set of randomly chosen intervals for each method + the true median.\n2. Average interval width vs labeled `n`, plus scatter of sampled widths.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\n\n# pick 5 trials per n for visualization\nplot_df <- results %>%\n  filter(method != \"Imputation\") %>%\n  group_by(method, n) %>%\n  slice_sample(n = 5) %>%\n  ungroup()\n\np_intervals <- ggplot(plot_df, aes(x = (lower + upper)/2, y = factor(trial), color = method)) +\n  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2) +\n  geom_vline(xintercept = true_q50, linetype = \"dashed\") +\n  facet_wrap(~ n, scales = \"free_y\") +\n  labs(\n    title = \"Randomly selected 95% CIs for the median across labeled sample sizes\",\n    x = \"Median gene expression (CI midpoint + error bar)\",\n    y = \"Trial (sampled)\"\n  )\n\np_width <- results %>%\n  filter(method %in% c(\"PPI\", \"PPI++\", \"Classical\")) %>%\n  ggplot(aes(x = n, y = width, color = method)) +\n  geom_point(alpha = 0.25) +\n  stat_summary(fun = mean, geom = \"line\", linewidth = 1) +\n  labs(\n    title = \"Interval width vs labeled sample size\",\n    x = \"Number of labeled outcomes (n)\",\n    y = \"CI width\"\n  )\n\np_impute <- ggplot() +\n  geom_segment(aes(x = ci_impute[1], xend = ci_impute[2], y = 1, yend = 1)) +\n  geom_vline(xintercept = true_q50, linetype = \"dashed\") +\n  labs(\n    title = \"Imputation CI (classical CI computed on predictions only)\",\n    x = \"Median gene expression\",\n    y = NULL\n  ) +\n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\np_intervals\np_width\np_impute\n\nggsave(\"plots/gene_expression_intervals.pdf\", p_intervals, width = 11, height = 5)\nggsave(\"plots/gene_expression_widths.pdf\", p_width, width = 8, height = 5)\nggsave(\"plots/gene_expression_imputation.pdf\", p_impute, width = 8, height = 2.5)\n```\n:::\n\n\n\n::: {.callout-tip collapse=\"true\"}\n\n### Interpretation checklist\n\n* Do PPI/PPI++ intervals stay approximately centered around the true median?\n* Are PPI/PPI++ widths systematically smaller than Classical at the same `n`?\n* Does Imputation produce unrealistically narrow intervals?\n:::\n\n## Power experiment (find n for 80% power)\n\nWe test the null:\n\n[\nH_0: \\text{median}(Y) \\le 5\n]\n\nUsing a one-sided rejection rule: reject if the **lower CI endpoint > 5**.\n\nWe find the smallest `n` such that the rejection probability is ~0.8 (at level α).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3)\n\nnum_experiments <- 100\nrand_idx_list <- replicate(num_experiments, sample.int(n_total), simplify = FALSE)\n\nreject_rate_ppi <- function(n) {\n  n <- as.integer(n)\n  mean(purrr::map_lgl(rand_idx_list, function(idx) {\n    idx_l <- idx[seq_len(n)]\n    idx_u <- idx[-seq_len(n)]\n\n    Y_l <- Y_total[idx_l]\n    f_l <- Yhat_total[idx_l]\n    f_u <- Yhat_total[idx_u]\n\n    ci <- ipd::ppi_quantile_ci(Y_l, f_l, f_u, q = q, alpha = alpha)\n    ci[1] > 5\n  }))\n}\n\nreject_rate_classical <- function(n) {\n  n <- as.integer(n)\n  mean(purrr::map_lgl(rand_idx_list, function(idx) {\n    idx_l <- idx[seq_len(n)]\n    Y_l <- Y_total[idx_l]\n\n    ci <- ipd::classical_quantile_ci(Y_l, q = q, alpha = alpha)\n    ci[1] > 5\n  }))\n}\n\n# invert to find n s.t. reject_rate(n) = 0.8\nf_ppi <- function(n) reject_rate_ppi(n) - 0.8\nf_cls <- function(n) reject_rate_classical(n) - 0.8\n\nn_ppi <- uniroot(f_ppi, interval = c(100, 2000), tol = 10)$root |> round()\nn_cls <- uniroot(f_cls, interval = c(100, 2000), tol = 10)$root |> round()\n\ncat(\"PPI requires ~ n =\", n_ppi, \"labeled data points for 80% power.\\n\")\ncat(\"Classical requires ~ n =\", n_cls, \"labeled data points for 80% power.\\n\")\n```\n:::\n\n\n\n::: {.callout-note collapse=\"true\"}\n\n### Why this works\n\nFor a one-sided level-α test based on a (1−α) CI:\n\n* Reject (H_0: \\theta \\le \\theta_0) when the **lower CI bound > θ₀**.\n  Then the probability of rejection under an alternative is the test’s **power**.\n:::\n\n## Where do PostPI, PSPA, PSPS, and Chen & Chen fit?\n\nThe `ipd` package is designed as a **unified wrapper** for multiple IPD approaches ([CRAN][1]). In this module, we focused on **quantiles**, where PPI/PPI++ are especially natural.\n\nBut many applied analyses care about **regression coefficients**, **means**, or GLM parameters. That’s where other methods enter.\n\n::: {.callout-note collapse=\"true\"}\n\n### PostPI (Wang et al., 2020)\n\nPostPI is a correction framework for inference after predicting outcomes, originally motivated by downstream regression tasks, and is implemented in the `postpi` R package ([PNAS][2]).\n:::\n\n::: {.callout-note collapse=\"true\"}\n\n### PSPA (Miao et al.)\n\nPSPA (“assumption-lean, data-adaptive post-prediction inference”) targets validity with weaker assumptions and adapts to prediction quality ([Journal of Machine Learning Research][4]).\n:::\n\n::: {.callout-note collapse=\"true\"}\n\n### PSPS (task-agnostic ML-assisted inference)\n\nPSPS is a “plug-in” protocol designed to work with many estimators in a task-agnostic way ([arXiv][3]).\n:::\n\n::: {.callout-note collapse=\"true\"}\n\n### Chen & Chen\n\nChen & Chen-style estimators arise in *double sampling* settings and are revisited in modern post-prediction inference discussions ([arXiv][5]).\n:::\n\n## (Optional) A quick regression mini-demo with `ipd()` wrapper\n\nThis is not part of the original notebook, but it shows how, for regression tasks, you can switch methods via the single `ipd()` wrapper ([RDocumentation][7]).\n\nWe’ll fabricate a feature `X` for illustration (so you can see the workflow end-to-end).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\n\n# toy feature to demonstrate downstream regression\nX <- rnorm(n_total)\n\n# create labeled/unlabeled indicator to mimic partial labels\nset_label <- rep(\"unlabeled\", n_total)\nset_label[sample.int(n_total, 400)] <- \"labeled\"\n\ndf_reg <- tibble(\n  Y = ifelse(set_label == \"labeled\", Y_total, NA_real_),\n  f = Yhat_total,\n  X = X,\n  set_label = set_label\n)\n\n# Labeled data (has Y observed)\ndat_l <- df_reg %>% filter(set_label == \"labeled\")\n# Unlabeled data (Y missing)\ndat_u <- df_reg %>% filter(set_label == \"unlabeled\")\n\n# Example: estimate slope of Y ~ X using different IPD methods (where supported)\n# NOTE: method names depend on ipd's API; see ?ipd::ipd for allowed \"method\"/\"model\" combos. :contentReference[oaicite:14]{index=14}\nfit_ppi <- ipd::ipd(\n  formula = Y - f ~ X,\n  method = \"ppi\",\n  model = \"ols\",\n  data = dat_l,\n  unlabeled_data = dat_u,\n  alpha = 0.05\n)\n\nfit_postpi <- ipd::ipd(\n  formula = Y - f ~ X,\n  method = \"postpi\",\n  model = \"ols\",\n  data = dat_l,\n  unlabeled_data = dat_u,\n  alpha = 0.05\n)\n\nfit_pspa <- ipd::ipd(\n  formula = Y - f ~ X,\n  method = \"pspa\",\n  model = \"ols\",\n  data = dat_l,\n  unlabeled_data = dat_u,\n  alpha = 0.05\n)\n\nlist(\n  ppi = broom::tidy(fit_ppi),\n  postpi = broom::tidy(fit_postpi),\n  pspa = broom::tidy(fit_pspa)\n)\n```\n:::\n\n\n\n::: {.callout-warning collapse=\"true\"}\n\n### About PSPS in this workshop\n\nPSPS has its own implementation and workflow (often via a separate package/repo) ([GitHub][8]). In a full workshop, I usually include PSPS as a **separate module** because it is a different “protocol” layer than PPI/PostPI/PSPA.\n:::\n\n## Exercises\n\n### Exercise 1 (core): quantiles other than the median\n\n1. Set `q <- 0.25` and rerun the CI experiment.\n2. Repeat for `q <- 0.75`.\n3. Compare how interval width behaves for lower vs upper quantiles.\n\n::: {.callout-tip collapse=\"true\"}\n**Hint:** replace `true_q50` with `true_q25` / `true_q75` in the plots.\n:::\n\n### Exercise 2 (core): effect of prediction quality\n\nSimulate a “worse” predictor:\n\n[\n\\tilde{Y} = Y + \\epsilon,\\quad \\epsilon \\sim N(0, \\sigma^2)\n]\n\n1. Replace `Yhat_total` with `Y_total + rnorm(n_total, 0, sigma)` for `sigma ∈ {0.5, 1, 2}`.\n2. Rerun the width plot.\n3. When does PPI still help? When does it become similar to classical?\n\n### Exercise 3 (stretch): bootstrap-based quantile intervals\n\nImplement a bootstrap CI for the median (classical) on labeled `Y` and compare:\n\n* bootstrap CI width vs classical order-stat CI width\n* does bootstrap behave better/worse for small `n`?\n\n### Exercise 4 (stretch): add a covariate and estimate conditional quantiles\n\nIf you create a covariate and want conditional quantiles (quantile regression), discuss:\n\n* which IPD methods transfer directly?\n* what would need new derivations vs “wrapper-able” protocols?\n\n## Wrap-up\n\nIn this module you reproduced a real “IPD quantile” workflow in R:\n\n* Classical: only labeled outcomes\n* Imputation: predictions-only (usually overconfident)\n* PPI / PPI++: calibrated + more power via unlabeled predicted outcomes ([CRAN][6])\n\nNext module ideas:\n\n* regression coefficients (OLS/GLM) with `ipd()` across PostPI/PPI/PPI++/PSPA ([CRAN][1])\n* an explicit PSPS protocol module ([GitHub][8])\n\n```\n\n---\n\n## Notes on “Chen & Chen” and PSPS/PSPA naming\n\n- **PSPA** is the “post-prediction adaptive inference” line :contentReference[oaicite:19]{index=19}.  \n- **PSPS** is “post-prediction summary-statistics-based inference / task-agnostic ML-assisted inference” :contentReference[oaicite:20]{index=20}.  \n- Chen & Chen appears in the double-sampling literature and is discussed in modern “inference after prediction” perspectives :contentReference[oaicite:21]{index=21}.\n\n---\n\nIf you want, I can also generate:\n1) a **second module** that is *purely regression/GLM* (so PostPI/PSPA/Chen&Chen comparisons are more central), and  \n2) a **third module** that implements the PSPS protocol as its own “plug-in layer” with exercises around swapping downstream estimators.\n::contentReference[oaicite:22]{index=22}\n```\n\n[1]: https://cloud.r-project.org/web/packages/ipd/readme/README.html \"README\"\n[2]: https://www.pnas.org/doi/10.1073/pnas.2001238117 \"Methods for correcting inference based on outcomes ...\"\n[3]: https://arxiv.org/abs/2405.20039 \"Task-Agnostic Machine-Learning-Assisted Inference\"\n[4]: https://www.jmlr.org/papers/v26/24-0056.html \"Assumption-lean and data-adaptive post-prediction inference\"\n[5]: https://arxiv.org/pdf/2411.19908 \"Another look at inference after prediction\"\n[6]: https://cran.r-project.org/web/packages/ipd/ipd.pdf \"ipd: Inference on Predicted Data\"\n[7]: https://www.rdocumentation.org/packages/ipd/versions/0.1.4/topics/ipd \"ipd Inference on Predicted Data (ipd)\"\n[8]: https://github.com/qlu-lab/PSPS \"qlu-lab/psps: Task-agnostic machine-learning-assisted ...\"\n\n\n\nKey references/tools this module uses:\n\n* `ipd` package overview + wrapper + quantile functions ([CRAN][1])\n* `postpi` background (Wang et al., PNAS 2020) ([PNAS][2])\n* PSPS (task-agnostic ML-assisted inference) ([arXiv][3])\n* PSPA (assumption-lean, data-adaptive post-prediction inference) ([Journal of Machine Learning Research][4])\n* Chen & Chen context (double sampling; “Another look at inference after prediction”) ([arXiv][5])\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}