{
  "hash": "1e8063cc95cf1e88967f4987a710c4f2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Proteomic Analysis with AlphaFold\"\nsubtitle: \"Relating Protein Disorder and Post-Translational Modifications\"\nformat:\n  html:\n    toc: true\n    number-sections: false\n    code-fold: true\n    code-tools: false\nexecute:\n  warning: false\n  message: false\n---\n\n\n\n\n\n![https://www.nature.com/articles/s41586-021-03819-2](images/alphafold.png)\n\n# Overview\n\n## Background\n\nModern protein biology increasingly relies on machine learning predictions.\nAlphaFold is an AI-powered system developed by Google DeepMind that predicts \na protein's 3D structure from its amino acid sequence with high accuracy. By \nutilizing deep learning and neural networks, it has modeled nearly all known \nproteins, accelerating research in drug discovery, disease understanding, and \nbiotechnology. \n\nAlphaFold now provides structural annotations for essentially the entire \nproteome, enabling large-scale analyses that were previously impossible. \nHowever, many downstream biological questions still depend on \n*statistical inference*, for example, asking whether post-translational \nmodifications (PTMs) preferentially occur in intrinsically disordered \nregions (IDRs).\n\nThis module walks through such an analysis using the `ipd` package. \nYou will learn how to:\n\n- Combine ML predictions with limited ground truth\n- Estimate odds ratios relating PTMs to disorder\n- Compare classical inference to various IPD methods\n- Quantify efficiency gains and power improvements from unlabeled data\n\nThe module is designed so that you can reproduce the analysis on your own \nusing only the provided dataset and instructions.\n\n## Biological Motivation\n\nAlphaFold predicts 3D protein structure from sequence at near-experimental \naccuracy for many proteins. These predictions can be post-processed to \nidentify IDRs, i.e., segments that do not adopt a stable fold.\n\nIDRs play central roles in signaling and regulation. They are flexible, \naccessible, and often enriched for short linear motifs. Many \npost-translational modifications, especially phosphorylation, ubiquitination, \nand acetylation, are hypothesized to concentrate in IDRs, supporting a model \nin which structural disorder enables rapid regulatory control.\n\nIn this workshop, each amino acid residue has:\n\n- `Y`: A binary \"gold-standard\" disorder label\n- `Yhat`: A predicted probability of being disordered (derived from AlphaFold)\n- PTM Indicators:\n  - `phosphorylated`\n  - `ubiquitinated`\n  - `acetylated`\n\nOur scientific question is:\n\n> **Are residues with a given PTM more likely to lie in intrinsically**\n> **disordered regions?**\n\n## Statistical Framing\n\nFor a chosen PTM, `Z`:\n\n- Let `Z = 1` if the residue carries that PTM\n- Let `Y = 1` if the residue is in an IDR\n\nWe estimate:\n\n$$\n  \\text{OR} = \\frac{\\text{Odds}(Y=1\\mid Z=1)}{\\text{Odds}(Y=1\\mid Z=0)}\n$$\n\nusing logistic regression.\n\nHowever, in practice we may have that:\n\n- `Y` is only observed on a *labeled* subset\n- `Yhat` is available everywhere\n\nThis is exactly the setting for *Inference with Predicted Data (IPD)*.\n\n## Learning Objectives                                                           <!-- NEED TO CHECK THESE -->\n\nBy the end of this module, you will be able to:\n\n1. Construct labeled/unlabeled splits from real biological data\n2. Estimate PTM-disorder odds ratios\n3. Apply various IPD methods to draw valid inference\n4. Compare uncertainty across methods\n5. Compute labeled sample sizes required for 80% power\n\n# Part I: Understanding the dataset\n\nPlease first load the `ipd`, `broom`, and `tidyverse` packages:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ipd)\n\nlibrary(broom)\n\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n\nWe have included file `alphafold.RData`, containing a tibble named \n`alphafold` with columns:\n\n- `Y`\n- `Yhat`\n- `phosphorylated`\n- `ubiquitinated`\n- `acetylated`\n\nEach row corresponds to a residue.\n\n### Exercise 1: Inspecting the AlphaFold/PTM Dataset\n\nBefore doing any inference, we will want to understand the data. Try loading \nthe dataset and answering the following:\n\n1. How many residues are there?\n2. What fraction are disordered?\n3. How many residues carry each PTM?\n\nThis gives you basic biological context and sanity-checks the input.\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"data/alphafold.RData\")\n\nglimpse(alphafold)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,802\nColumns: 5\n$ Y              <dbl[1d]> <array[26]>\n$ Yhat           <dbl[1d]> <array[26]>\n$ phosphorylated <dbl[1d]> <array[26]>\n$ ubiquitinated  <dbl[1d]> <array[26]>\n$ acetylated     <dbl[1d]> <array[26]>\n```\n\n\n:::\n\n```{.r .cell-code}\nalphafold |>\n  \n  summarise(\n    \n    n = n(),\n    \n    frac_disordered = mean(Y, na.rm = TRUE),\n    \n    phosphorylated  = sum(phosphorylated == 1),\n    ubiquitinated   = sum(ubiquitinated  == 1),\n    acetylated      = sum(acetylated     == 1)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n      n frac_disordered phosphorylated ubiquitinated acetylated\n  <int>           <dbl>          <int>         <int>      <int>\n1 10802           0.177           6017          3738       1171\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\nWe should observe that our dataset contains:\n\n* 10,802 residues, with about 18% labeled as intrinsically disordered\n* PTMs are unevenly represented, with phosphorylation being the most common\n\n:::\n\n### Discussion Questions\n\n* Why might experimentally validated disorder labels be scarce compared to \n  AlphaFold predictions?\n* How might only observing a fraction of the disorder labels affect\n  downstream inference?\n\n# Part II:Choosing a PTM and Defining Variables\n\nFor illustration, we will analyze one PTM at a time. For that PTM:\n\n* `Z` becomes its indicator\n* `Y` is disorder\n* `Yhat` is the AlphaFold prediction\n\n### Exercise 2: Creating our Analytic Dataset\n\nLet's study *phosphorylation* first. Please create an analytic dataset from the\n`alphafold` data with columns:\n\n* `Y`\n* `Yhat`\n* `Z` (phosphorylation indicator)\n\nThis will be the working dataset for the rest of the workshop.\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nptm <- \"phosphorylated\"\n\ndat <- alphafold |>\n  \n  transmute(\n    \n    Y    = as.integer(Y),\n    Yhat = as.numeric(Yhat),\n    Z    = as.integer(.data[[ptm]])\n  )\n\ndat |>\n  \n  group_by(Z) |>\n  \n  count() |>\n  \n  ungroup() |>\n  \n  mutate(pct = n / sum(n) * 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n      Z     n   pct\n  <int> <int> <dbl>\n1     0  4785  44.3\n2     1  6017  55.7\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\nHere we defined the basic analytic variables:\n\n* `Y`: True Disorder\n* `Yhat`: Predicted Disorder\n* `Z`: PTM Indicator\n\nWe should see that `Z = 1` in approximately 56% of samples.\n\n:::\n\n### Discussion Questions\n\n* Why might some PTMs be easier to detect than others?\n* How could PTM-specific detection pipelines influence our results?\n\n# Part III: Simulating Partial Labeling\n\nIn practice, only a subset of residues may have trusted disorder labels. \nHere, we are fortunate to have ground-truth data for all of our samples. \nSo, to study how IPD methods work in this setting, we will artificially:\n\n1. Select `n_labeled` rows\n2. Keep their `Y`\n3. Replace `Y` with `NA` everywhere else\n\n### Exercise 3: Creating Labeled/Unlabeled Splits\n\nWrite a function that:\n\n* Randomly selects `n_labeled` residues\n* Marks them as `\"labeled\"`\n* Sets `Y = NA` for all others\n* Returns a stacked dataset with a column `set_label`\n\nThis mimics limited experimental labeling.\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmake_split <- function(dat, n_labeled, seed = NULL) {\n  \n  if (!is.null(seed)) set.seed(seed)\n\n  idx <- sample.int(nrow(dat), size = n_labeled, replace = FALSE)\n\n  labeled <- dat[idx, ] |> \n    \n    mutate(set_label = \"labeled\")\n  \n  unlabeled <- dat[-idx, ] |> \n    \n    mutate(Y = NA_integer_, set_label = \"unlabeled\")\n\n  bind_rows(labeled, unlabeled)\n}\n```\n:::\n\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\nBy artificially masking labels, we created a realistic experimental scenario:\n\n* Only a small subset has true disorder labels\n* All residues retain AlphaFold predictions\n\nThis mirrors many modern biological pipelines, where:\n\n* ML provides global predictions\n* Experimental validation is expensive and sparse\n\nFrom this point forward, we will evaluate every method under this setup.\n\n:::\n\n### Discussion Questions\n\n* In real studies, how might labeled residues differ systematically from \n  unlabeled ones?\n* What assumptions are implicitly made when we randomly subsample labels?\n\n# Part IV: Inference Methods\n\nWe will now compare several potential methods for drawing inference:\n\n* Oracle Logistic Regression (`Y` in All Samples, Not Usually Possible)\n* Classical Logistic Regression (`Y` in Labeled Samples Only)\n* Naive Logistic Regression (Imputed `Y` with threshold `Yhat`)\n* IPD Methods:\n\n  * PPI\n  * PPI++\n  * PSPA\n  * Chen & Chen\n\nWe will use these approaches estimate the same OR of `Y`/`Yhat` on `Z`.\n\n### Exercise 4: Running all Methods on One Split\n\nWith `n_labeled = 400`:\n\n1. Create a labeled/unlabeled split\n2. Fit the oracle logistic regression\n3. Fit the classical logistic regression\n4. Fit the naive logistic regression\n5. Fit the above IPD methods\n6. Extract the OR and confidence intervals\n\nand compare how wide the intervals are at a significance level of \n`alpha = 0.05`.\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.05\n\nstacked <- make_split(dat = dat, n_labeled = 400, seed = 12345)\n\nfit_oracle <- glm(Y ~ Z, data = dat, family = binomial())\n\nfit_classical <- glm(Y ~ Z, data = filter(stacked, set_label == \"labeled\"),\n                     \n  family = binomial())\n  \nfit_naive <- glm(I(Yhat > 0.5) ~ Z, data = stacked, family = binomial())\n\nfit_ppi <- ipd(Y - Yhat ~ Z, method = \"ppi\", model = \"logistic\", \n               \n  data = stacked, label = \"set_label\")\n\nfit_plusplus <- ipd(Y - Yhat ~ Z, method = \"ppi_plusplus\", model = \"logistic\", \n               \n  data = stacked, label = \"set_label\")\n\nfit_pspa <- ipd(Y - Yhat ~ Z, method = \"pspa\", model = \"logistic\", \n               \n  data = stacked, label = \"set_label\")\n\n### CHEN AND CHEN HERE                                                           ### NEED CHEN AND CHEN\n\nex4_results <- bind_rows(\n  \n  tidy(fit_oracle,    conf.int = TRUE) |> mutate(Method = \"Oracle\"),\n  \n  tidy(fit_classical, conf.int = TRUE) |> mutate(Method = \"Classical\"),\n  \n  tidy(fit_naive,     conf.int = TRUE) |> mutate(Method = \"Naive\"),\n  \n  tidy(fit_ppi)                        |> mutate(Method = \"PPI\"),\n  \n  tidy(fit_plusplus)                   |> mutate(Method = \"PPI++\"),\n  \n  tidy(fit_pspa)                       |> mutate(Method = \"PSPA\")\n  \n### CHEN AND CHEN HERE                                                           ### NEED CHEN AND CHEN  \n) |>\n  \n  filter(term == \"Z\") |>\n  \n  mutate(\n    \n    Method = factor(Method) |>\n      \n      fct_relevel(\"Oracle\", \"Classical\", \"Naive\", \"PPI\", \"PPI++\", \"PSPA\"),       ### NEED CHEN + CHEN\n    \n    OR  = exp(estimate),\n    \n    LCL = exp(conf.low),\n    \n    UCL = exp(conf.high)) |>\n  \n  select(Method, OR, LCL, UCL)\n\nglimpse(ex4_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 6\nColumns: 4\n$ Method <fct> Oracle, Classical, Naive, PPI, PPI++, PSPA\n$ OR     <dbl> 2.130875, 1.786170, 2.716635, 2.019634, 1.986547, 2.012504\n$ LCL    <dbl> 1.916754, 1.025458, 2.412201, 1.407421, 1.379686, 1.404529\n$ UCL    <dbl> 2.371509, 3.209141, 3.065175, 2.898153, 2.860340, 2.883653\n```\n\n\n:::\n\n```{.r .cell-code}\noracle_est <- ex4_results |> filter(Method == \"Oracle\") |> pull(OR)\n\nex4_results |>\n  \n  mutate(covers = if_else((LCL <= oracle_est) & (UCL >= oracle_est), T, F)) |>\n  \n  ggplot(aes(x = OR, xmin = LCL, xmax = UCL, y = Method, color = covers)) +\n  \n    theme_bw() +\n  \n    geom_vline(xintercept = oracle_est, linetype = 2, linewidth = 1.1) +\n  \n    geom_linerange(linewidth = 5) +\n  \n    geom_point(size = 3, color = \"white\") +\n  \n    scale_y_discrete(limits = rev) +\n  \n    scale_color_manual(values = palette.colors(4)[2:3]) +\n  \n    labs(\n      \n      x = \"Odds Ratio (95% CI)\", \n      \n      y = NULL,\n      \n      color = \"Covers the Oracle Estimate?\") +\n  \n    theme(\n      \n      legend.position = \"top\",\n    \n      axis.text = element_text(face = \"bold\", size = 12),\n    \n      axis.title.x = element_text(face = \"bold\", size = 14, \n                              \n      margin = margin(t = 20, unit = \"pt\")))\n```\n\n::: {.cell-output-display}\n![](Unit05_AlphaFold_files/figure-html/ex4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\nThis single-split comparison highlights several key behaviors:\n\n* **Oracle**: Uses the ground truth (often an unattainable gold standard)\n* **Classical**: Uses only labeled data and typically produces wider intervals\n* **Naive**: Treats the predictions as truth and often produces \n  *anticonservative* intervals\n* **IPD Methods**: Incorporate unlabeled predictions while correcting bias and \n  uncertainty using the labeled data.\n\nWe should observe:\n\n* Classical CIs that are wide\n* Naive CIs that are too narrow\n* IPD CIs that are in between: More efficient than the classical, but\n  reflecting more uncertainty than the naive while covering the oracle estimate\n\n:::\n\n### Discussion Questions\n\n* Why does the naive approach look so \"good\" numerically?\n* Which method would you trust in a scientific paper and why?\n* What would happen if `Yhat` were poorly calibrated?\n\n# Part V: Efficiency Experiment\n\nWe will now study how uncertainty changes as labeled sample size increases.\n\n### Exercise 5: Confidence Interval Width vs Labeled Sample Size\n\nFor each `n = {200, 400, 800, 1500, 3000}`, let's:\n\n* Repeat the experiment 20 times\n* Record the OR confidence intervals\n* Compute the CI widths\n\nWe will store everything in a single results table for now so that we can\ninspect and plot these results in later exercises.\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns <- c(200, 400, 800, 1500, 3000)\n\nnum_trials <- 1                                                                  ### Up to 20\n\nrun_one <- function(n, t) {\n  \n  s <- make_split(dat, n, seed = 1000 + n + t)\n  \n  fit_oracle <- glm(Y ~ Z, data = dat, family = binomial())\n\n  fit_classical <- glm(Y ~ Z, data = filter(s, set_label == \"labeled\"),\n                     \n    family = binomial())\n  \n  fit_naive <- glm(I(Yhat > 0.5) ~ Z, data = s, family = binomial())\n\n  fit_ppi <- ipd(Y - Yhat ~ Z, method = \"ppi\", \n                 \n    model = \"logistic\", data = s, label = \"set_label\")\n\n  fit_plusplus <- ipd(Y - Yhat ~ Z, method = \"ppi_plusplus\", \n                      \n    model = \"logistic\", data = s, label = \"set_label\")\n\n  fit_pspa <- ipd(Y - Yhat ~ Z, method = \"pspa\", \n                  \n    model = \"logistic\", data = s, label = \"set_label\")\n\n  ### CHEN AND CHEN HERE                                                         ### NEED CHEN AND CHEN\n  \n  results <- bind_rows(\n    \n    tidy(fit_oracle,    conf.int = TRUE) |> mutate(Method = \"Oracle\"),\n    \n    tidy(fit_classical, conf.int = TRUE) |> mutate(Method = \"Classical\"),\n    \n    tidy(fit_naive,     conf.int = TRUE) |> mutate(Method = \"Naive\"),\n    \n    tidy(fit_ppi)                        |> mutate(Method = \"PPI\"),\n    \n    tidy(fit_plusplus)                   |> mutate(Method = \"PPI++\"),\n    \n    tidy(fit_pspa)                       |> mutate(Method = \"PSPA\")\n    \n  ### CHEN AND CHEN HERE                                                         ### NEED CHEN AND CHEN  \n  ) |>\n  \n  filter(term == \"Z\") |>\n  \n  mutate(\n    \n    Method = factor(Method) |>\n      \n      fct_relevel(\"Oracle\", \"Classical\", \"Naive\", \"PPI\", \"PPI++\", \"PSPA\"),       ### NEED CHEN + CHEN\n    \n    OR  = exp(estimate),\n    \n    LCL = exp(conf.low),\n    \n    UCL = exp(conf.high),\n    \n    n = n,\n    \n    trial = t, \n    \n    width = UCL - LCL) |>\n  \n  select(Method, OR, LCL, UCL, n, trial, width)\n}\n\nex5_results <- crossing(n = ns, trial = 1:num_trials) |>\n  \n  mutate(out = map2(n, trial, run_one)) |>\n  \n  select(out) |> \n  \n  unnest(out)\n\nglimpse(ex5_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 30\nColumns: 7\n$ Method <fct> Oracle, Classical, Naive, PPI, PPI++, PSPA, Oracle, Classical, …\n$ OR     <dbl> 2.130875, 2.017544, 2.716635, 2.112385, 2.112385, 2.112126, 2.1…\n$ LCL    <dbl> 1.9167544, 0.9386853, 2.4122014, 1.3063051, 1.3063051, 1.307842…\n$ UCL    <dbl> 2.371509, 4.501192, 3.065175, 3.415871, 3.415871, 3.411021, 2.3…\n$ n      <dbl> 200, 200, 200, 200, 200, 200, 400, 400, 400, 400, 400, 400, 800…\n$ trial  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ width  <dbl> 0.4547542, 3.5625063, 0.6529739, 2.1095660, 2.1095660, 2.103179…\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\nWe have now quantified uncertainty across many labeled sample sizes.\n\nThe results table contains:\n\n* OR Estimates\n* Confidence Intervals\n* CI Widths\n* Repeated Trials\n\nThis enables direct comparison of statistical efficiency.\n\nWe should see:\n\n* CI widths decrease as `n` increases for all methods.\n* Classical inference shrinks slowly.\n* PPI and especially PPI++ shrink much faster.\n* Naive imputation remains artificially tight.\n\nThis shows that unlabeled predictions have the potential to provide \n*additional information* when properly calibrated.\n\n:::\n\n### Discussion Questions\n\n* Why do some IPD methods typically outperform others?\n* At small `n`, are methods variable across trials?\n* What does this imply for pilot studies?\n\n### Exercise 6: Visualizing Example Intervals\n\nLet's plot five random trials for `n = 400`, overlaying a reference OR \ncomputed from the full dataset (i.e., the oracle regression results).\n\n::: {.callout-note collapse=\"true\"}\n\n## Answer\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\nn_show <- 400\n\nex_trials <- sample(unique(ex5_results$trial), size = 1)                         ### size = 5)\n\nex6_results <- ex5_results |>\n  \n  filter(n == n_show, trial %in% ex_trials) |>\n  \n  mutate(\n    \n    Method = factor(Method) |>\n      \n      fct_relevel(\"Oracle\", \"Classical\", \"Naive\", \"PPI\", \"PPI++\", \"PSPA\"),\n    \n    trial = factor(trial),\n    \n    covers = if_else((LCL <= oracle_est) & (UCL >= oracle_est), T, F)\n  )\n\nex6_results |>\n  \n  ggplot(aes(x = OR, xmin = LCL, xmax = UCL, y = Method, color = covers)) +\n  \n    theme_bw() +\n  \n    facet_wrap(~ trial, nrow = 1) +\n  \n    geom_vline(xintercept = oracle_est, linetype = 2, linewidth = 1.1) +\n  \n    geom_linerange(linewidth = 5) +\n  \n    geom_point(size = 3, color = \"white\") +\n  \n    scale_y_discrete(limits = rev) +\n  \n    scale_color_manual(values = palette.colors(4)[2:3]) +  \n  \n    labs(\n      \n      x = \"Odds Ratio (95% CI)\", \n      \n      y = NULL,\n      \n      color = \"Covers the Oracle Estimate?\") +\n  \n    theme(\n      \n      legend.position = \"top\",\n    \n      axis.text = element_text(face = \"bold\", size = 12),\n    \n      axis.title.x = element_text(face = \"bold\", size = 14, \n                              \n      margin = margin(t = 20, unit = \"pt\")))\n```\n\n::: {.cell-output-display}\n![](Unit05_AlphaFold_files/figure-html/ex6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.callout-tip}\n\n## Notes\n\nWe should notice:\n\n* Classical intervals fluctuate more.\n* Naive intervals cluster tightly, but do not cover the oracle.\n* IPD-based intervals tend to center near the oracle, with more uncertainty.\n\nThis illustrates an important distinction:\n\n> **Precision alone is meaningless without calibration.**\n\nNaive methods are precise but biased. IPD methods balance both.\n\n:::\n\n### Discussion Questions\n\n* Which methods consistently cover the oracle?\n* How might one diagnose miscalibration in practice?\n\n### Exercise 7: Average CI Widths\n\nPlot mean CI width versus `n` for all methods.\n\nInterpret:\n\n* Which methods shrink fastest?\n* How do IPD methods compare to classical inference?\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nex7_results <- ex5_results |>\n  \n  group_by(Method, n) |>\n  \n  summarise(mn = mean(width, na.rm = TRUE), .groups = \"drop\")\n\nex7_results |>\n  \n  ggplot(aes(x = n, y = mn, group = Method, color = Method)) +\n  \n    theme_bw() +\n  \n    geom_line(linewidth = 0.6) +\n  \n    geom_point(size = 1.6) +\n  \n    scale_x_continuous(breaks = ns) +\n  \n    scale_color_manual(values = palette.colors(6)) +                               ### 7 with CHEN + CHEN\n  \n    labs(\n    \n      x = \"Number of Labeled Observations (n)\",\n    \n      y = \"Mean Confidence Interval Width\") +\n  \n    theme(\n      \n      legend.position = \"top\",\n    \n      axis.text = element_text(face = \"bold\", size = 12),\n    \n      axis.title.x = element_text(face = \"bold\", size = 14, \n                              \n        margin = margin(t = 20, unit = \"pt\")),\n      \n      axis.title.y = element_text(face = \"bold\", size = 14, \n                              \n        margin = margin(r = 20, unit = \"pt\")))\n```\n\n::: {.cell-output-display}\n![](Unit05_AlphaFold_files/figure-html/ex7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n::: {.callout-tip}\n\n## Notes\n\nThis figure summarizes efficiency. Key takeaways:\n\n* Oracle represents the best achievable precision.\n* Classical requires far more labels to approach oracle performance.\n* IPD methods can reduce labeled sample requirements.\n\nThis is the operational value of IPD:\n\n> We can sometimes improve precision with fewer labels.\n\n:::\n\n### Discussion Questions\n\n* When might classical inference still be preferable?\n* Factoring in the cost per residue, how might one weigh labeling \n  versus predicting?\n\n# Part VI: Power Analysis\n\nFinally, suppose we want to test:\n\n$$\n  H_0: \\text{OR} \\le 1,\n$$\n\nwhere we reject when the **lower CI bound exceeds 1**.\n\n### Exercise 8: How Many Labels Do We Need?\n\nWe will now estimate the smallest `n_labeled` such that:\n\n* PPI achieves 80% power\n* Classical inference achieves 80% power\n\nand we will compare the two.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha_pval <- 0.05\n\ntarget_power <- 0.80\n\nn_experiments <- 1                                                               ### Up to 100\n\nrun_one_ci <- function(s, method = c(\"PPI\", \"Classical\")) {\n  \n  method <- match.arg(method)\n\n  if (method == \"PPI\") {\n    \n    fit <- ipd(Y - Yhat ~ Z, method = \"ppi\", model = \"logistic\",\n               \n      data = s, label = \"set_label\", alpha = alpha_pval)\n    \n    tt <- tidy(fit) |> filter(term == \"Z\")\n    \n    c(LCL = exp(tt$conf.low), UCL = exp(tt$conf.high))\n    \n  } else {\n    \n    lab <- s |> filter(set_label == \"labeled\")\n    \n    m <- glm(Y ~ Z, data = lab, family = binomial())\n    \n    tt <- tidy(m, conf.int = TRUE) |> filter(term == \"Z\")\n    \n    c(LCL = exp(tt$conf.low), UCL = exp(tt$conf.high))\n  }\n}\n\npower_at_n <- function(n_labeled, method = c(\"PPI\", \"Classical\")) {\n  \n  method <- match.arg(method)\n\n  rej <- replicate(n_experiments, {\n    \n    s <- make_split(dat, n_labeled = n_labeled)\n    \n    ci <- run_one_ci(s, method = method)\n    \n    if (method == \"PPI\") {\n      \n      as.integer(ci[[\"LCL.Z\"]] > 1)\n      \n    } else {\n      \n      as.integer(ci[[\"LCL\"]] > 1)\n    }\n  })\n\n  mean(rej)\n}\n\nn_grid <- sort(unique(round(seq(50, 1200, by = 50))))\n\npower_tbl <- tibble(\n  \n  n = n_grid,\n  \n  ppi = map_dbl(n_grid, ~ power_at_n(.x, \"PPI\")),\n  \n  classical = map_dbl(n_grid, ~ power_at_n(.x, \"Classical\"))) |>\n  \n  pivot_longer(-n, names_to = \"Method\", values_to = \"Power\")\n\npower_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 48 × 3\n       n Method    Power\n   <dbl> <chr>     <dbl>\n 1    50 ppi           0\n 2    50 classical     0\n 3   100 ppi           0\n 4   100 classical     0\n 5   150 ppi           1\n 6   150 classical     0\n 7   200 ppi           0\n 8   200 classical     0\n 9   250 ppi           1\n10   250 classical     0\n# ℹ 38 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nn80 <- power_tbl |>\n  \n  group_by(Method) |>\n  \n  filter(Power >= target_power) |>\n  \n  summarise(n_for_80 = ifelse(n() == 0, NA_integer_, min(n)), .groups = \"drop\")\n\nn80\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  Method    n_for_80\n  <chr>        <dbl>\n1 classical      350\n2 ppi            150\n```\n\n\n:::\n\n```{.r .cell-code}\npower_tbl |>\n  \n  ggplot(aes(x = n, y = Power, color = Method)) +\n  \n    theme_bw() +\n  \n    geom_hline(yintercept = target_power, linetype = 2) +\n  \n    geom_line(linewidth = 1.1) +\n  \n    geom_point(size = 2) +\n  \n    scale_x_continuous(breaks = n_grid) +\n  \n    scale_color_manual(values = palette.colors(3)[2:3]) +\n  \n    labs(\n\n      x = \"Number of Labeled Observations (n)\",\n    \n      y = \"Estimated Power\") +\n  \n    theme(\n      \n      legend.position = \"top\",\n    \n      axis.text = element_text(face = \"bold\", size = 12),\n    \n      axis.title.x = element_text(face = \"bold\", size = 14, \n                              \n        margin = margin(t = 20, unit = \"pt\")),\n      \n      axis.title.y = element_text(face = \"bold\", size = 14, \n                              \n        margin = margin(r = 20, unit = \"pt\")))\n```\n\n::: {.cell-output-display}\n![](Unit05_AlphaFold_files/figure-html/ex8-1.png){width=672}\n:::\n:::\n\n\n\n\n\n::: {.callout-tip}\n\n## Notes\n\nWe should observe:\n\n* Classical inference requires more labels to reach 80% power.\n* PPI reaches the same power with fewer labels.\n\nThis translates directly into experimental design:\n\n> IPD can potentially reduces sample size requirements for discovery.\n\n:::\n\n### Discussion Questions\n\n* How could this change how we design wet lab validation studies?\n* What risks arise if `Yhat` is systematically biased?\n\n### Exercise 9: Naive Imputation Sensitivity\n\nTo impute `Y`, we previously used a value of `0.5` to threshold the predicted \nprobabilities from AlphaFold. We can also repeat this experiment and vary the\nnaive imputations with thresholds of 0.2, 0.4, 0.6, and 0.8.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthresholds <- seq(0.2, 0.8, 0.2)\n\nnaive_by_threshold <- function(thr) {\n  \n  m <- glm(I(Yhat > thr) ~ Z, data = dat, family = binomial())\n  \n  tidy(m, conf.int = TRUE) |>\n    \n    filter(term == \"Z\") |>\n    \n    transmute(\n      \n      threshold = thr,\n      \n      OR = exp(estimate),\n      \n      LCL = exp(conf.low),\n      \n      UCL = exp(conf.high),\n      \n      width = UCL - LCL\n    )\n}\n\nex9_results <- bind_rows(lapply(thresholds, naive_by_threshold))\n\nex9_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  threshold    OR   LCL   UCL width\n      <dbl> <dbl> <dbl> <dbl> <dbl>\n1       0.2  2.24  2.03  2.48 0.449\n2       0.4  2.58  2.30  2.89 0.589\n3       0.6  2.80  2.47  3.18 0.713\n4       0.8  2.91  2.52  3.36 0.841\n```\n\n\n:::\n\n```{.r .cell-code}\nex9_results |>\n  \n  ggplot(aes(x = OR, xmin = LCL, xmax = UCL, y = threshold)) +\n  \n    theme_bw() +\n  \n    geom_vline(xintercept = 1, linetype = 2, linewidth = 1.1) +\n\n    geom_linerange(linewidth = 5, color = palette.colors(3)[3]) +\n  \n    geom_point(size = 3, color = \"white\") +\n  \n    scale_y_continuous(breaks = thresholds) +\n  \n    labs(\n      \n      x = \"Odds Ratio (95% CI)\", \n      \n      y = \"Threshold for Yhat -> Binary Label\") +\n  \n    theme(\n      \n      legend.position = \"top\",\n    \n      axis.text = element_text(face = \"bold\", size = 12),\n    \n      axis.title.x = element_text(face = \"bold\", size = 14, \n                              \n        margin = margin(t = 20, unit = \"pt\")),\n      \n      axis.title.y = element_text(face = \"bold\", size = 14, \n                              \n        margin = margin(r = 20, unit = \"pt\")))\n```\n\n::: {.cell-output-display}\n![](Unit05_AlphaFold_files/figure-html/ex9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Discussion Question\n\n* Why do the OR estimates change?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Part VII: Biological Extensions\n\n### Exercise 10: Compare PTMs\n\nRepeat the entire workflow for:\n\n* phosphorylation\n* ubiquitination\n* acetylation\n\nWhich PTM shows the strongest enrichment in disorder?\n\n## Take-Home Messages\n\nAlphaFold enables proteome-scale structural annotation—but statistical inference still matters.\n\nBy combining:\n\n* ML predictions (`Yhat`)\n* limited truth (`Y`)\n* IPD methodology\n\nyou can:\n\n* retain valid uncertainty\n* dramatically reduce labeling requirements\n* make principled biological conclusions at scale.\n\nThis paradigm generalizes far beyond protein disorder—to any setting where ML predictions feed into downstream scientific inference.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Next Steps, Conclusions, and Summary\n\n## Key Lessons\n\nThis module demonstrates a general paradigm:\n\n1. Use ML to generate predictions at scale.\n2. Collect limited ground truth data.\n3. Apply IPD to recover valid inference.\n\nIn this AlphaFold/PTM application, we saw that:\n\n* Classical methods waste information.\n* Naive imputation is overconfident.\n* PPI-style approaches preserve validity while dramatically improving efficiency.\n\nThis pattern appears across modern science:\n\n* medical imaging\n* genomics\n* remote sensing\n* clinical risk modeling\n\nAnywhere predictions feed downstream analysis, IPD applies.\n\n---\n\n## Optional Take-Home Assignment (Exercise 10)\n\n### Exercise 10 (Self-Guided): Compare PTMs\n\nRepeat the full workflow for:\n\n* phosphorylation\n* ubiquitination\n* acetylation\n\nCompare:\n\n* estimated ORs\n* CI widths\n* required sample sizes\n\n### Biological context\n\nDifferent PTMs play distinct regulatory roles:\n\n* Phosphorylation is often associated with signaling\n* Ubiquitination with degradation\n* Acetylation with chromatin regulation\n\nYou may find that:\n\n* Some PTMs are more enriched in disorder\n* Some show weaker or noisier associations\n\n### Statistical goal\n\nAssess whether:\n\n* effect sizes differ across PTMs\n* labeling efficiency gains persist\n* conclusions are robust\n\nThis mirrors real exploratory proteomics.\n\n---\n\n## Suggested Extensions\n\nIf you continue this work:\n\n* Add protein-level clustering (robust SEs)\n* Include covariates (amino acid type, position)\n* Compare PPI vs PPI++\n* Inject noise into `Yhat` to study robustness\n* Apply to another ML-derived phenotype\n\n---\n\n\n\n---\n\n## Final Take-Home Message\n\nAlphaFold enables proteome-scale prediction.\n\nIPD enables **proteome-scale inference**.\n\nTogether, they allow you to turn predictions into scientifically valid conclusions—without labeling everything.\n",
    "supporting": [
      "Unit05_AlphaFold_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}