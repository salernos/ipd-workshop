[
  {
    "objectID": "style.html",
    "href": "style.html",
    "title": "Style",
    "section": "",
    "text": "To change the part of the navigation bar that says “OTTR Quarto”, modify the title within the _quarto.yml file."
  },
  {
    "objectID": "style.html#navigation-bar",
    "href": "style.html#navigation-bar",
    "title": "Style",
    "section": "",
    "text": "To change the part of the navigation bar that says “OTTR Quarto”, modify the title within the _quarto.yml file."
  },
  {
    "objectID": "style.html#overall-theme",
    "href": "style.html#overall-theme",
    "title": "Style",
    "section": "Overall theme",
    "text": "Overall theme\nTo change the color scheme/fonts of the website modify the theme in the _site.yml file (see here for options):"
  },
  {
    "objectID": "style.html#change-the-favicon",
    "href": "style.html#change-the-favicon",
    "title": "Style",
    "section": "Change the favicon",
    "text": "Change the favicon\nThe small image that shows up on the browser can also be changed.\nYou can make a small image to replace the existing one by going to https://favicon.io/favicon-converter/ and uploading an image that you would like.\nNext, simply replace the image called favicon.ico in the images directory within the resources directory with the image you just created and downloaded from the favicon converter website."
  },
  {
    "objectID": "style.html#additional-changes",
    "href": "style.html#additional-changes",
    "title": "Style",
    "section": "Additional changes",
    "text": "Additional changes\nTo make additional changes to the style, you can modify the styles.css file with css code. This website has great information about css code.\nAs an example if you wanted to change the color of the blue line to green you could change where it says lightblue to lightgreen in the styles.css file. You can also use a hex color code like those that can be found at this website, such as #00FF9E to get a specific shade.\n\n\n\n\n\n\n\n\n\nNote that if you change the css file with a new element that is not already defined like body then you would need to do it as done with the banner element. This was then added to the index.Rmd file by using:\n&lt;div class = \"banner\"&gt;\nBanner text!  \n&lt;/div&gt;\nAlso checkout the Quarto docs for more customization of the pages."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prediction-Based Inference: Methods & Applications",
    "section": "",
    "text": "Source: https://arxiv.org/pdf/2411.19908v1\n\n\n\n\n\nLearning Goals:\n\nUnderstand limitations in using predicted data for inference.\nLearn how IPD methods adjust for bias and recover valid uncertainty estimates.\nGain practical skills using the ipd R package.\n\nLearning Objectives:\n\nExplore data with AI/ML-predicted outcomes and diagnose bias/variance in predictions.\nApply ipd::ipd() to continuous and binary outcomes.\nInterpret IPD outputs and visualize model results.\n\n\n\n\n\n\n\n\nActivity\nTime\n\n\n\n\nOverview\n30 m\n\n\nShort Break\n5 m\n\n\nUnit 00: Getting Started\n30 m\n\n\nUnit 01: AlphaFold\n30 m\n\n\nWrap-Up and Q&A\n10 m\n\n\n\n\n\n\nThe companion website for this workshop is available at:\nhttps://salernos.github.io/ipd-workshop\nTo use the workshop image:\ndocker run -e PASSWORD=&lt;choose_a_password_for_rstudio&gt; -p 8787:8787 ghcr.io/salernos/ipdworkshop:latest\nOnce running, navigate to http://localhost:8787/ and then log in with rstudio:yourchosenpassword.\nThen begin!\n\n\n\nIn this workshop, we explore the consequences of inference on predicted data across several biomedical applications. Drawing from classical approaches to measurement error and recent developments in bias correction, we will present a suite of prediction-based inference methods that adjust for prediction-related uncertainty and improve inference validity and efficiency. We will also introduce ipd, a user-friendly R package that implements several of these correction methods through a unified interface. The package supports modular integration into existing workflows and includes tidy methods for model inspection and diagnostics.\n\n\n\nThis workshop covers three modules (time permitting), each illustrating IPD in R using the ipd package:1\n\n\n\n\n\n\n\n\n\n\n\nInference with Predicted Data\n\n\nBuild intuition for inference with predicted data by simulating labeled/unlabeled sets and comparing naive, classical, and IPD estimators.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProtein Disorder and PTMs\n\n\nApply IPD methods in a proteomics setting to estimate associations when key outcomes are model-predicted rather than directly measured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI vs. DXA\n\n\nCompare BMI and DXA-based adiposity measures and use IPD corrections to improve regression inference under predicted outcomes.\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nWe have also included some supplemental modules for you to explore on your own:\n\n\n\n\n\n\n\n\n\n\n\nin B-Cell Leukemia\n\n\nStudy BCR-ABL prediction from gene-expression profiles and evaluate how IPD methods calibrate downstream inference in genomics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Earnings with Age Effects\n\n\nUse census-style features to predict income and perform valid OLS inference on age effects with mixed labeled and unlabeled data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni.e., Performance is not Enough\n\n\nExamine how equally predictive models can yield different scientific conclusions and compare naive, classical, and IPD-based inference.\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nThis workshop uses a blended format of instruction and hands-on coding exercises. Participants should:\n\nFollow along in the virtual RStudio environment (see below).\nAttempt to complete brief exercises or run the solution code snippets in real time.\nEngage in Q&A at module boundaries to troubleshoot and discuss concepts.\n\n\n\n\n\nA computer with internet to access the RStudio Virtual Environment (see below).\nFamiliarity with base R and tidyverse syntax (e.g., dplyr, broom).\nBasic understanding of predictive (e.g., randomForest) and regression modeling (e.g., lm, glm).\nOptional: Exposure to Bioconductor’s ExpressionSet, AnnotationDbi, and MLInterfaces is helpful for one of the supplemental modules.\n\n\n\n\n\nDatasets: nhanesA, ALL, golubEsets, AnnotationDbi, hgu95av2.db, hu6800.db\nData Manipulation and Visualization: broom, scales, janitor, GGally, patchwork, tidyverse\nPredictive Modeling: neuralnet, partykit, randomForest, ranger, mgcv, pROC, DALEX, MLInterfaces\nInference with Predicted Data: ipd\n\nPresenters: Jesse Gronsbell ✉︎, Stephen Salerno ✉︎\nContributors (Alphabetical Order): Awan Afiaz ✉︎, David Cheng ✉︎, Jianhui Gao ✉︎, Jesse Gronsbell ✉︎, Kentaro Hoffman ✉︎, Jeff Leek ✉︎, Qiongshi Lu ✉︎, Tyler McCormick ✉︎, Jiacheng Miao ✉︎, Anna Neufeld ✉︎, Stephen Salerno ✉︎",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "Prediction-Based Inference: Methods & Applications",
    "section": "",
    "text": "The companion website for this workshop is available at:\nhttps://salernos.github.io/ipd-workshop\nTo use the workshop image:\ndocker run -e PASSWORD=&lt;choose_a_password_for_rstudio&gt; -p 8787:8787 ghcr.io/salernos/ipdworkshop:latest\nOnce running, navigate to http://localhost:8787/ and then log in with rstudio:yourchosenpassword.\nThen begin!",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#workshop-overview",
    "href": "index.html#workshop-overview",
    "title": "Prediction-Based Inference: Methods & Applications",
    "section": "",
    "text": "In this workshop, we explore the consequences of inference on predicted data across several biomedical applications. Drawing from classical approaches to measurement error and recent developments in bias correction, we will present a suite of prediction-based inference methods that adjust for prediction-related uncertainty and improve inference validity and efficiency. We will also introduce ipd, a user-friendly R package that implements several of these correction methods through a unified interface. The package supports modular integration into existing workflows and includes tidy methods for model inspection and diagnostics.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "Prediction-Based Inference: Methods & Applications",
    "section": "",
    "text": "This workshop covers three modules (time permitting), each illustrating IPD in R using the ipd package:1\n\n\n\n\n\n\n\n\n\n\n\nInference with Predicted Data\n\n\nBuild intuition for inference with predicted data by simulating labeled/unlabeled sets and comparing naive, classical, and IPD estimators.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProtein Disorder and PTMs\n\n\nApply IPD methods in a proteomics setting to estimate associations when key outcomes are model-predicted rather than directly measured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI vs. DXA\n\n\nCompare BMI and DXA-based adiposity measures and use IPD corrections to improve regression inference under predicted outcomes.\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#supplemental-modules",
    "href": "index.html#supplemental-modules",
    "title": "Prediction-Based Inference: Methods & Applications",
    "section": "",
    "text": "We have also included some supplemental modules for you to explore on your own:\n\n\n\n\n\n\n\n\n\n\n\nin B-Cell Leukemia\n\n\nStudy BCR-ABL prediction from gene-expression profiles and evaluate how IPD methods calibrate downstream inference in genomics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Earnings with Age Effects\n\n\nUse census-style features to predict income and perform valid OLS inference on age effects with mixed labeled and unlabeled data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni.e., Performance is not Enough\n\n\nExamine how equally predictive models can yield different scientific conclusions and compare naive, classical, and IPD-based inference.\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nThis workshop uses a blended format of instruction and hands-on coding exercises. Participants should:\n\nFollow along in the virtual RStudio environment (see below).\nAttempt to complete brief exercises or run the solution code snippets in real time.\nEngage in Q&A at module boundaries to troubleshoot and discuss concepts.\n\n\n\n\n\nA computer with internet to access the RStudio Virtual Environment (see below).\nFamiliarity with base R and tidyverse syntax (e.g., dplyr, broom).\nBasic understanding of predictive (e.g., randomForest) and regression modeling (e.g., lm, glm).\nOptional: Exposure to Bioconductor’s ExpressionSet, AnnotationDbi, and MLInterfaces is helpful for one of the supplemental modules.\n\n\n\n\n\nDatasets: nhanesA, ALL, golubEsets, AnnotationDbi, hgu95av2.db, hu6800.db\nData Manipulation and Visualization: broom, scales, janitor, GGally, patchwork, tidyverse\nPredictive Modeling: neuralnet, partykit, randomForest, ranger, mgcv, pROC, DALEX, MLInterfaces\nInference with Predicted Data: ipd\n\nPresenters: Jesse Gronsbell ✉︎, Stephen Salerno ✉︎\nContributors (Alphabetical Order): Awan Afiaz ✉︎, David Cheng ✉︎, Jianhui Gao ✉︎, Jesse Gronsbell ✉︎, Kentaro Hoffman ✉︎, Jeff Leek ✉︎, Qiongshi Lu ✉︎, Tyler McCormick ✉︎, Jiacheng Miao ✉︎, Anna Neufeld ✉︎, Stephen Salerno ✉︎",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Prediction-Based Inference: Methods & Applications",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nModule card cover images were generated by GPT-5.2.↩︎",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "git_actions.html",
    "href": "git_actions.html",
    "title": "Git Actions",
    "section": "",
    "text": "We have set up several checks for website content edits.\nWhen you create a pull request, you will see something like this if everything is successful. You can click on the preview of website here link to see a preview. Please note that some features may not be possible to see in the preview. For example, icons may only show up as a box."
  },
  {
    "objectID": "git_actions.html#rendering-action",
    "href": "git_actions.html#rendering-action",
    "title": "Git Actions",
    "section": "Rendering Action",
    "text": "Rendering Action\nIf the rendering action fails, you will see something like this:\n\n\n\n\n\n\n\n\n\nIf you click on where it says Details on the far right, you will be taken to more information about what may have gone wrong.\n\n\n\n\n\n\n\n\n\nFor example, we can see that an R object was not found in one of the files. You could identify which file by scrolling up."
  },
  {
    "objectID": "git_actions.html#spelling-and-style-action",
    "href": "git_actions.html#spelling-and-style-action",
    "title": "Git Actions",
    "section": "Spelling and Style Action",
    "text": "Spelling and Style Action\nYou may find that you have spelling errors if you get the following message from your pull request (PR):\n\n\n\n\n\n\n\n\n\nIf this happens, click the Download the errors here. link. This will take you to a table with words that the check thought were misspelled, as well as what file they occurred in and the lines in that file.\nAdd words that are not actually misspelled to the dictionary.txt file located in the resources directory. It’s a good idea to try to keep this in alphabetical order.\nFor words that are indeed misspelled, fix the errors and push your changes to your pull request.\nYou should then see that your pull request has a different message that tells you that you have no spelling errors."
  },
  {
    "objectID": "git_actions.html#url-check",
    "href": "git_actions.html#url-check",
    "title": "Git Actions",
    "section": "URL Check",
    "text": "URL Check\nYou may find that you have broken URLs. If so, you will get this message when you create a Pull Request.\n\n\n\n\n\n\n\n\n\nYou can click on the Download the errors here link to see a document with a list of broken URLs and what files they are located in, like this:\n\n\n\n\n\n\n\n\n\nHere we can see that there are two URLs that are broken in the git_actions.Rmd file."
  },
  {
    "objectID": "git_actions.html#completing-a-pull-request",
    "href": "git_actions.html#completing-a-pull-request",
    "title": "Git Actions",
    "section": "Completing a Pull Request",
    "text": "Completing a Pull Request\nOnce all the Git Action checks pass, you can merge your pull request to your main branch for your website.\n\n\n\n\n\n\n\n\n\nIf you are only working on this yourself without others to review your pull request, you can click the Merge without waiting for requirements to be met box, so that you can click the Merge pull request button."
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html",
    "href": "content/Unit05_RashomonQuartet.html",
    "title": "The Rashomon Quartet",
    "section": "",
    "text": "In “Performance is not enough: the story of Rashomon’s quartet” (Biecek et al., 2023), four different models (linear regression, decision trees, random forests, and neural networks) achieve near-identical predictive performance on the same dataset. However, their interpretations vary significantly. In this module, we will:\n\nFit each model on a training set.\nEvaluate their predictive performance on a testing set.\nDemonstrate how the naive, classical, and IPD-based inference results differ.\n\n\nNote: We will replicate the results of the Rashomon Quartet analysis following:\nhttps://github.com/MI2DataLab/rashomon-quartet",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#overview",
    "href": "content/Unit05_RashomonQuartet.html#overview",
    "title": "The Rashomon Quartet",
    "section": "",
    "text": "In “Performance is not enough: the story of Rashomon’s quartet” (Biecek et al., 2023), four different models (linear regression, decision trees, random forests, and neural networks) achieve near-identical predictive performance on the same dataset. However, their interpretations vary significantly. In this module, we will:\n\nFit each model on a training set.\nEvaluate their predictive performance on a testing set.\nDemonstrate how the naive, classical, and IPD-based inference results differ.\n\n\nNote: We will replicate the results of the Rashomon Quartet analysis following:\nhttps://github.com/MI2DataLab/rashomon-quartet",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#background-on-the-rashomon-quartet",
    "href": "content/Unit05_RashomonQuartet.html#background-on-the-rashomon-quartet",
    "title": "The Rashomon Quartet",
    "section": "Background on the Rashomon Quartet",
    "text": "Background on the Rashomon Quartet\nThe underlying datasets are synthetically generated to include three covariates, \\(\\boldsymbol{X} = (X_1, X_2, X_3)^\\top\\), so that\n\\[\n\\boldsymbol{X} = (X_1, X_2, X_3)^\\top \\sim \\mathcal{N}_3 \\left(\\boldsymbol{0}, \\Sigma\\right), \\quad \\Sigma_{ij} = \\begin{cases} 1 & i=j,\\\\ 0.9 & i \\neq j\\end{cases}\n\\] The outcomes, \\(Y\\), are then generated to depend only on \\(X_1\\) and, to a lesser extent, \\(X_2\\), given by\n\\[\nY = \\sin\\left(\\frac{3X_1 + X_2}{5}\\right) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}\\left(0, \\frac{1}{3}\\right).\n\\]\n\nNote: \\(X_3\\) does not appear in the true data generating mechanism, but is highly correlated with \\(X_1\\) and \\(X_2\\).\n\nTwo independent batches of 1,000 observations (a training set and a testing set) are generated under this model. We will fit the four candidate models on the 1,000-row training data, and evaluate them separately on the 1,000-row testing data. As we will show, all four achieve \\(R^2 \\approx 0.729\\) and \\(\\mathrm{RMSE} \\approx 0.354\\) on the testing set. However, their interpretations diverge:\n\nLinear regression captures a near-linear approximation of \\(\\sin\\left(\\tfrac{3X_{1} + X_{2}}{5}\\right)\\).\nDecision tree approximates the nonlinear relationship via a small set of splits.\nRandom forest produces a smooth ensemble derivative of the tree.\nNeural network learns a different nonlinear basis with hidden units.\n\n\n\n\nSource: https://github.com/MI2DataLab/rashomon-quartet\n\n\nIn addition to comparing interpretability across models, we will demonstrate how to perform valid downstream inference for \\(Y\\) on \\(\\boldsymbol{X}\\) when \\(Y\\) is only partially observed. We will:\n\nTrain each predictive model on the full 1,000-row training set.\nPredict on all 1,000 testing observations.\nRandomly split the 1,000 testing rows into two groups:\n\nLabeled subset (with true \\(Y\\) observed).\nUnlabeled subset (with only predicted \\(f\\)).\n\nIn the labeled subset, regress \\(Y\\) on \\(\\boldsymbol{X}\\) (“Classical”).\nIn the unlabeled subset, regress \\(f\\) on \\(\\boldsymbol{X}\\) (“Naive”).\nCombine the two subsets and use ipd::ipd() to correct bias and variance.\n\nThis mirrors realistic scenarios where the analytic dataset (here the testing set) has only some observations labeled, and we want to use model predictions for the rest while still making valid inference.",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#setup-and-data",
    "href": "content/Unit05_RashomonQuartet.html#setup-and-data",
    "title": "The Rashomon Quartet",
    "section": "Setup and Data",
    "text": "Setup and Data\n\n\nCode\n# Install if needed:\n# install.packages(c(\"ipd\",\"DALEX\",\"partykit\",\"randomForest\",\"neuralnet\",\"GGally\",\"tidyverse\"))\n\nlibrary(ipd)           # Inference with Predicted Data\nlibrary(DALEX)         # Model Explanation\nlibrary(partykit)      # Decision Trees\nlibrary(randomForest)  # Random Forests\nlibrary(neuralnet)     # Neural Networks\nlibrary(GGally)        # Pairwise Plots\nlibrary(tidyverse)     # Data Manipulation and Visualization\n\n\n\nRead-In the Training and Testing Data\nWe have included two CSV files from the MI2DataLab/rashomon-quartet repository on GitHub. Note that these CSVs are semicolon-delimited.\n\n\nCode\n# Use read_delim to parse semicolons\ntrain &lt;- read_delim(\"data/rq_train.csv\", delim = \";\", col_types = cols(), show_col_types = FALSE)\ntest  &lt;- read_delim(\"data/rq_test.csv\",  delim = \";\", col_types = cols(), show_col_types = FALSE)\n\n# Show the first few rows of each\nglimpse(train)\n\n\nRows: 1,000\nColumns: 4\n$ y  &lt;dbl&gt; -0.47048454, 0.17177895, 0.07295012, -0.34301361, 0.47167402, 0.191…\n$ x1 &lt;dbl&gt; -0.51211395, 0.49038643, -0.89461065, 0.02725393, 0.44364997, -0.26…\n$ x2 &lt;dbl&gt; -0.34174653, 0.25511508, -0.28744899, -0.09277031, 0.52147200, 0.28…\n$ x3 &lt;dbl&gt; 0.255531783, -0.179610715, -0.310620340, -0.109603650, 0.920225141,…\n\n\nCode\nglimpse(test)\n\n\nRows: 1,000\nColumns: 4\n$ y  &lt;dbl&gt; -0.44547765, -0.67618374, -0.78822993, -0.55024530, -0.49205810, -0…\n$ x1 &lt;dbl&gt; -0.553685719, -0.965436826, -0.962515641, -0.871816734, -1.96565982…\n$ x2 &lt;dbl&gt; -0.74221752, -0.19959118, -1.25801758, -1.08469684, -1.43546177, -0…\n$ x3 &lt;dbl&gt; -1.15266476, -1.00944121, -1.34017902, -0.84322216, -1.29871031, -0…\n\n\n\nConfirm Dimensions and Column Names:\n\nEach dataset should have exactly 1,000 rows and four columns: y, x1, x2, x3.\nWe will use all 1,000 rows of train to fit each predictive model, and then randomly split the 1,000-row test for IPD.",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#predictive-model-training-on-the-training-set",
    "href": "content/Unit05_RashomonQuartet.html#predictive-model-training-on-the-training-set",
    "title": "The Rashomon Quartet",
    "section": "Predictive Model Training on the Training Set",
    "text": "Predictive Model Training on the Training Set\nBelow, we fit four predictive models using the training set:\n\nDecision Tree via ctree() (max depth = 3, minsplit = 250).\nLinear Regression via lm().\nRandom Forest via randomForest() (100 trees).\nNeural Network via neuralnet() (two hidden layers: 8 units, 4 units; threshold = 0.05).\n\nWe then evaluate each model’s performance in the testing set using the DALEX package to extract the model’s \\(R^2\\) and \\(RMSE\\).\n\n\nCode\nset.seed(1568)\n\n# 1) Decision Tree (ctree)\nmodel_dt &lt;- ctree(\n  formula = y ~ x1 + x2 + x3,\n  data    = train,\n  control = ctree_control(maxdepth = 3, minsplit = 250)\n)\n\nexp_dt &lt;- DALEX::explain(\n  model   = model_dt,\n  data    = test |&gt; select(x1, x2, x3),\n  y       = test$y,\n  verbose = FALSE,\n  label   = \"Decision Tree\"\n)\n\nmp_dt &lt;- model_performance(exp_dt)\n\n# 2) Linear Regression\nmodel_lm &lt;- lm(y ~ x1 + x2 + x3, data = train)\n\nexp_lm &lt;- DALEX::explain(\n  model   = model_lm,\n  data    = test |&gt; select(x1, x2, x3),\n  y       = test$y,\n  verbose = FALSE,\n  label   = \"Linear Regression\"\n)\n\nmp_lm &lt;- model_performance(exp_lm)\n\n# 3) Random Forest\nmodel_rf &lt;- randomForest(\n  formula = y ~ x1 + x2 + x3,\n  data    = train,\n  ntree   = 100\n)\n\nexp_rf &lt;- DALEX::explain(\n  model   = model_rf,\n  data    = test |&gt; select(x1, x2, x3),\n  y       = test$y,\n  verbose = FALSE,\n  label   = \"Random Forest\"\n)\n\nmp_rf &lt;- model_performance(exp_rf)\n\n# 4) Neural Network\nmodel_nn &lt;- neuralnet(\n  formula      = y ~ x1 + x2 + x3,\n  data         = train,\n  hidden       = c(8, 4),\n  threshold    = 0.05,\n  linear.output = TRUE\n)\n\npredict_nn &lt;- function(object, newdata) {\n  as.numeric(predict(object, newdata))\n}\n\nexp_nn &lt;- DALEX::explain(\n  model            = model_nn,\n  data             = test |&gt; select(x1, x2, x3),\n  y                = test$y,\n  predict_function = predict_nn,\n  verbose          = FALSE,\n  label            = \"Neural Network\"\n)\n\nmp_nn &lt;- model_performance(exp_nn)\n\n# Save DALEX explainers for later reuse\nsave(exp_dt, exp_lm, exp_rf, exp_nn, file = \"models.RData\")\n\n# Compile performance metrics into one data frame\nmp_all &lt;- list(\n  \"Linear Regression\" = mp_lm,\n  \"Decision Tree\"     = mp_dt,\n  \"Random Forest\"     = mp_rf,\n  \"Neural Network\"    = mp_nn\n)\n\nR2_values   &lt;- sapply(mp_all, function(x) x$measures$r2)\nRMSE_values &lt;- sapply(mp_all, function(x) x$measures$rmse)\n\nperf_df &lt;- tibble(\n  Model = names(R2_values),\n  R2    = round(as.numeric(R2_values),   4),\n  RMSE  = round(as.numeric(RMSE_values), 4)\n)\n\nperf_df\n\n\n# A tibble: 4 × 3\n  Model                R2  RMSE\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 Linear Regression 0.729 0.354\n2 Decision Tree     0.729 0.354\n3 Random Forest     0.728 0.354\n4 Neural Network    0.730 0.352\n\n\n\nInterpretation: All four models (Linear Regression, Decision Tree, Random Forest, Neural Network) deliver essentially identical predictive performance on the testing set:\n\\[\nR^2 \\approx 0.729,\\quad \\mathrm{RMSE} \\approx 0.354.\n\\]",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#inspecting-the-prediction-models",
    "href": "content/Unit05_RashomonQuartet.html#inspecting-the-prediction-models",
    "title": "The Rashomon Quartet",
    "section": "Inspecting the Prediction Models",
    "text": "Inspecting the Prediction Models\nWe now take a quick look at each model’s internal structure or summary.\n\nNote:\n\nOften, in practice, we do not know the operating characteristics of the predictive model or have access to its training data.\nIPD methods are distinct in that they are agnostic to the source or form of these ‘black-box’ predictions\n\n\n\nDecision Tree Visualization\n\n\nCode\nplot(model_dt, main = \"Decision Tree (ctree) Structure\")\n\n\n\n\n\n\n\n\n\n\nThe tree has depth 3, with splits on \\(x_1, x_2, x_3\\) that approximate \\(\\sin\\left(\\tfrac{3X_{1} + X_{2}}{5}\\right)\\) piecewise.\n\n\n\nLinear Regression Summary\n\n\nCode\nsummary(model_lm)\n\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.68045 -0.24213  0.01092  0.23409  1.26450 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.01268    0.01114  -1.138    0.255    \nx1           0.48481    0.03001  16.157  &lt; 2e-16 ***\nx2           0.14316    0.02966   4.826 1.61e-06 ***\nx3          -0.03113    0.02980  -1.045    0.296    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.352 on 996 degrees of freedom\nMultiple R-squared:  0.7268,    Adjusted R-squared:  0.726 \nF-statistic: 883.4 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe OLS coefficients show how the linear model approximates the underlying sine function. In particular, the slope on \\(x_1\\) will be roughly an average of this over the training distribution.\n\n\n\nRandom Forest Summary\n\n\nCode\nmodel_rf\n\n\n\nCall:\n randomForest(formula = y ~ x1 + x2 + x3, data = train, ntree = 100) \n               Type of random forest: regression\n                     Number of trees: 100\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1193538\n                    % Var explained: 73.58\n\n\n\nWe see OOB MSE and variance explained on the training data. The forest builds many trees of depth &gt;3 and averages them to approximate the sine structure.\n\n\n\nNeural Network Topology\n\n\nCode\nplot(model_nn, rep = \"best\", main = \"Neural Network (8,4) Topology\")\n\n\n\n\n\n\n\n\n\n\nThis diagram shows two hidden layers (8 units \\(\\to\\) 4 units) with activation functions that capture nonlinearity. The network approximates \\(\\sin\\left(\\tfrac{3X_{1} + X_{2}}{5}\\right)\\) more flexibly than the decision tree or random forest.",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#variable-importance-dalex",
    "href": "content/Unit05_RashomonQuartet.html#variable-importance-dalex",
    "title": "The Rashomon Quartet",
    "section": "Variable Importance (DALEX)",
    "text": "Variable Importance (DALEX)\nNext, we compute variable importance for each model via DALEX::model_parts(). We request type=\"difference\" so that importance is measured in terms of loss of performance (RMSE) when that variable is permuted.\n\n\nCode\nimp_dt &lt;- model_parts(exp_dt, N = NULL, B = 1, type = \"difference\")\nimp_lm &lt;- model_parts(exp_lm, N = NULL, B = 1, type = \"difference\")\nimp_rf &lt;- model_parts(exp_rf, N = NULL, B = 1, type = \"difference\")\nimp_nn &lt;- model_parts(exp_nn, N = NULL, B = 1, type = \"difference\")\n\nplot(imp_dt, imp_nn, imp_rf, imp_lm)\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nEach model ranks features \\((x_{1},x_{2},x_{3})\\) by their impact on predictive accuracy.\nBecause \\(\\mathbf{x}\\) are correlated, some models (e.g. tree, forest) may split differently than linear regression or neural network.",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#partial-dependence-plots-dalex",
    "href": "content/Unit05_RashomonQuartet.html#partial-dependence-plots-dalex",
    "title": "The Rashomon Quartet",
    "section": "Partial-Dependence Plots (DALEX)",
    "text": "Partial-Dependence Plots (DALEX)\nWe now generate partial-dependence (PD) profiles for each feature under each model via DALEX::model_profile(). This shows how each model’s predicted \\(f\\) changes when we vary one feature at a time, averaging out the others.\n\n\nCode\npd_dt &lt;- model_profile(exp_dt, N = NULL)\npd_lm &lt;- model_profile(exp_lm, N = NULL)\npd_rf &lt;- model_profile(exp_rf, N = NULL)\npd_nn &lt;- model_profile(exp_nn, N = NULL)\n\nplot(pd_dt, pd_nn, pd_rf, pd_lm)\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nFor \\(x_{1}\\), the Linear Regression PD is a straight line.\nThe Decision Tree PD is piecewise-constant.\nThe Random Forest PD is smoother but still stepwise.\nThe Neural Network PD approximates the sine shape.\n\nBecause \\(x_{3}\\) does not appear in the generating formula, its PD should be nearly flat—though correlation can induce slight structure.",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#inference-with-predicted-data-ipd",
    "href": "content/Unit05_RashomonQuartet.html#inference-with-predicted-data-ipd",
    "title": "The Rashomon Quartet",
    "section": "Inference with Predicted Data (IPD)",
    "text": "Inference with Predicted Data (IPD)\nWe now demonstrate how to perform valid inference in the testing set when only some test rows have true \\(Y\\). Specifically, we:\n\nRandomly split the 1,000 testing rows into:\n\nA labeled subset of size \\(n_l\\) (we choose 10%, but this can be adjusted).\nAn unlabeled subset of size \\(n_u = 1,000 - n_l\\).\n\nIn the labeled subset, regress \\(Y\\) on \\(X_1\\), \\(X_2\\), and \\(X_3\\) (Classical).\nIn the unlabeled subset, regress \\(f\\) on \\(X_1\\), \\(X_2\\), and \\(X_3\\) (Naive).\nUse both subsets in an IPD pipeline and run:\nipd_res &lt;- ipd(\n  formula        = y - f ~ x1 + x2 + x3,\n  data           = test_labeled,\n  unlabeled_data = test_unlabeled,\n  method         = \"pspa\",\n  model          = \"ols\"\n)\nto obtain bias-corrected estimates of \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\).\n\n\nNote: Instead of supplying one stacked dataset and the column name for the set labels, we can alternativel supply the labeled and unlabeled data separately.\n\n\n\nCode\nset.seed(12345)\n\n# Predict outcomes in the testing set\nipd_data &lt;- test |&gt;\n  mutate(\n    \n    preds_lm = c(predict(model_lm, newdata = test)),\n    preds_dt = c(predict(model_dt, newdata = test)),\n    preds_rf = c(predict(model_rf, newdata = test)),\n    preds_nn = c(predict(model_nn, newdata = test)))\n\n# Randomly split the testing set into labeled and unlabeled subsets\nfrac_labeled &lt;- 0.1\n\nn_test    &lt;- nrow(ipd_data)\nn_labeled &lt;- floor(frac_labeled * n_test)\n\nlabeled_idx &lt;- sample(seq_len(n_test), size = n_labeled)\n\ntest_labeled &lt;- ipd_data |&gt;\n  slice(labeled_idx) |&gt;\n  mutate(set_label = \"labeled\")\n  \ntest_unlabeled &lt;- ipd_data |&gt;\n  slice(-labeled_idx) |&gt;\n  mutate(set_label = \"unlabeled\")\n\n\n\n\nCode\n# Classical\nclass_fit &lt;- lm(y ~ x1 + x2 + x3, data = test_labeled)\nclass_df  &lt;- broom::tidy(class_fit) |&gt;\n  mutate(method = \"Classical\") \n  \n# Naive (Linear Model)\nnaive_fit_lm &lt;- lm(preds_lm ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_lm  &lt;- broom::tidy(naive_fit_lm) |&gt;\n  mutate(method = \"Naive (lm)\") \n\n# Naive (Decision Tree)\nnaive_fit_dt &lt;- lm(preds_dt ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_dt  &lt;- broom::tidy(naive_fit_dt) |&gt;\n  mutate(method = \"Naive (dt)\") \n\n# Naive (Random Forest)\nnaive_fit_rf &lt;- lm(preds_rf ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_rf  &lt;- broom::tidy(naive_fit_rf) |&gt;\n  mutate(method = \"Naive (rf)\") \n\n# Naive (Neural Net)\nnaive_fit_nn &lt;- lm(preds_nn ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_nn  &lt;- broom::tidy(naive_fit_nn) |&gt;\n  mutate(method = \"Naive (nn)\") \n\n# IPD (Linear Model)\nipd_fit_lm &lt;- ipd(y - preds_lm ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled, \n  method = \"pspa\", model = \"ols\")\nipd_df_lm  &lt;- tidy(ipd_fit_lm) |&gt;\n  mutate(method = \"IPD (lm)\") \n\n# IPD (Decision Tree)\nipd_fit_dt &lt;- ipd(y - preds_dt ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled,\n  method = \"pspa\", model = \"ols\")\nipd_df_dt  &lt;- tidy(ipd_fit_dt) |&gt;\n  mutate(method = \"IPD (dt)\") \n\n# IPD (Random Forest)\nipd_fit_rf &lt;- ipd(y - preds_rf ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled,\n  method = \"pspa\", model = \"ols\")\nipd_df_rf  &lt;- tidy(ipd_fit_rf) |&gt;\n  mutate(method = \"IPD (rf)\") \n\n# IPD (Neural Net)\nipd_fit_nn &lt;- ipd(y - preds_nn ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled,\n  method = \"pspa\", model = \"ols\")\nipd_df_nn  &lt;- tidy(ipd_fit_nn) |&gt;\n  mutate(method = \"IPD (nn)\") \n\ncombined_df &lt;- bind_rows(class_df,\n  naive_df_lm, naive_df_dt, naive_df_rf, naive_df_nn,\n  ipd_df_lm,   ipd_df_dt,   ipd_df_rf,   ipd_df_nn) |&gt;\n  \n  mutate(\n    conf.low  = estimate - 1.96 * std.error,\n    conf.high = estimate + 1.96 * std.error\n  )\n\ncombined_df\n\n\n# A tibble: 36 × 8\n   term        estimate std.error statistic   p.value method  conf.low conf.high\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  0.00872  3.68e- 2  2.37e- 1 8.13e-  1 Classi…  -0.0634   0.0808 \n 2 x1           0.643    1.09e- 1  5.91e+ 0 5.24e-  8 Classi…   0.430    0.856  \n 3 x2           0.0702   8.70e- 2  8.07e- 1 4.22e-  1 Classi…  -0.100    0.241  \n 4 x3          -0.0791   9.94e- 2 -7.96e- 1 4.28e-  1 Classi…  -0.274    0.116  \n 5 (Intercept) -0.0127   4.88e-18 -2.60e+15 0         Naive …  -0.0127  -0.0127 \n 6 x1           0.485    1.29e-17  3.77e+16 0         Naive …   0.485    0.485  \n 7 x2           0.143    1.31e-17  1.09e+16 0         Naive …   0.143    0.143  \n 8 x3          -0.0311   1.31e-17 -2.37e+15 0         Naive …  -0.0311  -0.0311 \n 9 (Intercept) -0.0183   6.76e- 3 -2.71e+ 0 6.89e-  3 Naive …  -0.0315  -0.00506\n10 x1           0.546    1.78e- 2  3.06e+ 1 1.57e-141 Naive …   0.511    0.581  \n# ℹ 26 more rows\n\n\n\nPlotting The Model Estimates\nWe now visualize, for each predictive model, the estimated coefficients and 95% confidence intervals from each inference method:\n\nNote: The vertical dashed line marks the estimated slope for the Classical method, which serves as our benchmark for comparison.\n\n\n\nCode\nref_lines &lt;- combined_df %&gt;% \n  filter(method == \"Classical\") %&gt;% \n  select(term, estimate)\n\nggplot(combined_df, aes(x = estimate, y = method, color = method)) +\n  geom_point(size = 2) +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  facet_wrap(~ term, ncol = 4, scales = \"free_x\") +\n  geom_vline(\n    data = ref_lines,\n    aes(xintercept = estimate),\n    linetype = \"dashed\",\n    color    = \"black\"\n  ) +\n  labs(\n    x = expression(hat(beta)),\n    y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nInterpretation of Results\n\nThe Naive estimates are biased due to the prediction error and has artificially narrow confidence intervals.\nThe Classical is our benchmark, as it is unbiased, but has wider CI’s\nThe IPD methods cover the Classical estimates and have wider CI’s to account for the prediction uncertainty.",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#data-distribution-train-vs-test-ggallyggpairs",
    "href": "content/Unit05_RashomonQuartet.html#data-distribution-train-vs-test-ggallyggpairs",
    "title": "The Rashomon Quartet",
    "section": "Data Distribution: Train vs Test (GGally::ggpairs)",
    "text": "Data Distribution: Train vs Test (GGally::ggpairs)\nFinally, we visualize pairwise relationships among \\((x_{1},x_{2},x_{3},y)\\) for train vs test. This confirms that train and test were drawn from the same synthetic distribution.\n\n\nCode\nboth &lt;- bind_rows(\n  train |&gt; mutate(label = \"train\"),\n  test  |&gt; mutate(label = \"test\")\n)\n\nggpairs(\n  both,\n  columns = c(\"x1\", \"x2\", \"x3\", \"y\"),\n  mapping = aes(color = label, alpha = 0.3),\n  lower = list(\n    continuous = wrap(\"points\", size = 0.5, alpha = 0.3),\n    combo      = wrap(\"facethist\", bins = 20)\n  ),\n  diag = list(\n    continuous = wrap(\"densityDiag\", alpha = 0.4, bw = \"SJ\")\n  ),\n  upper = list(\n    continuous = wrap(\"cor\", size = 3, stars = FALSE)\n  )\n) +\n  theme_minimal() +\n  labs(\n    title    = \"Pairwise Distribution: Train vs Test (x1,x2,x3,y)\",\n    subtitle = \"Color = train (blue) vs test (pink)\"\n  )\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe diagonals show almost identical marginal distributions of \\(x_{1},x_{2},x_{3},y\\) in train vs test.\nThe off-diagonals confirm the covariance structure \\(\\Sigma\\) is consistent across both sets.",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#summary-and-takeaways",
    "href": "content/Unit05_RashomonQuartet.html#summary-and-takeaways",
    "title": "The Rashomon Quartet",
    "section": "Summary and Takeaways",
    "text": "Summary and Takeaways\n\nPredictive Performance:\n\nLinear Regression, Decision Tree, Random Forest, and Neural Network each achieve nearly identical test set performance (\\(R^2 \\approx 0.729, \\mathrm{RMSE} \\approx 0.354\\)).\n\nModel Interpretability:\n\nVariable Importance and Partial-Dependence plots highlight how each model “tells a different story” about which features matter and how \\(y\\) responds to \\(x_{1},x_{2},x_{3}\\).\nThe Neural Network’s PD best tracks the true sine function; the tree’s PD is piecewise-constant; the forest’s PD is smoothed piecewise; the linear PD is a straight line.\n\nInference on Predicted Data (IPD):\n\nBy randomly splitting the test set into “labeled” and “unlabeled” halves, we simulate a real-world scenario where only some new observations have true \\(y\\).\nNaive regression of \\(f \\sim x_{1} + x_{2} + x_{3}\\) on the unlabeled set is biased due to prediction error.\nClassical regression of \\(y \\sim x_{1} + x_{2} + x_{3}\\) on the labeled set is unbiased but less efficient.\nIPD methods combine the labeled/unlabeled sets to correct bias and properly estimate variance.\n\nKey Lesson:\n\n“Performance is not enough.” Even models with identical \\(R^2\\) can lead to very different bias and variance in downstream inference on covariate effects. IPD provides a principled correction when using model predictions in place of true outcomes for some observations.",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit05_RashomonQuartet.html#references",
    "href": "content/Unit05_RashomonQuartet.html#references",
    "title": "The Rashomon Quartet",
    "section": "References",
    "text": "References\n\nBiecek, Przemysław, et al. “Performance is not enough: The story told by a Rashomon Quartet.” Journal of Computational and Graphical Statistics 33.3 (2024): 1118-1121.\nMI2DataLab. “The Rashomon Quartet.” GitHub repository (2023).\n\n\nThis is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org",
    "crumbs": [
      "Supplemental Modules",
      "The Rashomon Quartet"
    ]
  },
  {
    "objectID": "content/Unit03_GeneticData.html",
    "href": "content/Unit03_GeneticData.html",
    "title": "BCR-ABL Fusion",
    "section": "",
    "text": "Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, but it exhibits marked genetic heterogeneity, with distinct chromosomal translocations defining molecular subtypes with divergent prognoses and therapeutic responses. In B-cell lineage ALL, the BCR-ABL1 fusion (“breakpoint cluster region”/“Abelson”, i.e., the “Philadelphia chromosome”) arises from a t(9;22) translocation and encodes a constitutively active tyrosine kinase. This fusion was historically associated with poor outcomes, until the advent of targeted therapies (e.g., imatinib) revolutionized treatment and survival.\n\n\n\n\nSource: https://www.cancer.gov/publications/dictionaries/cancer-terms/def/philadelphia-chromosome\n\n\n\nHigh-density microarray profiling measures expression of over 7,000 genes in leukemic blast cells, enabling a form of “molecular diagnostics” where supervised learning can:\n\nClassify fusion status or other genetic subtypes when cytogenetic assays (PCR, FISH) are unavailable.\nDiscover novel marker genes and pathways dysregulated by specific fusions.\nPredict therapeutic response and stratify risk using expression-derived scores.\n\nHowever, in many retrospective or multi-center cohorts, only a subset of patients undergo gold-standard fusion testing (e.g., RT-PCR for BCR-ABL1), leaving the majority “unlabeled.” A common workaround is to train a gene expression classifier on the small labeled subset and apply it to the larger unlabeled cohort. Yet, naive downstream analyses, treating predicted labels as ground truth, can yield biased effect estimates and understate uncertainty.\nInference with Predicted Data (IPD) offers a principled remedy: it combines a small labeled set with the predicted labels (e.g., with true fusion status) with a larger unlabeled set, adjusting both bias and variance. In this module, we will:\n\nDescribe the ALL and Golub Bioconductor datasets.\nSubset the ALL data to B-cell ALL and filter to the top 500 variable probes.\nHarmonize probes across platforms via gene symbols.\nSplit the ALL data and train three classifiers for BCR-ABL1 fusion status.\nPredict on the holdout ALL and Golub B-cell ALL data using the ALL-trained classifier.\nApply IPD to estimate the association between fusion status and patient sex assigned at birth.\n\nBy the end, you will understand how to leverage expression-based predictions while maintaining valid inference on genetic subtypes in cohorts with partially missing gold-standard labels.\n\nNote: For the data loading and prediction workflows, we will directly follow the Bioconductor MLInterfaces vignette by VJ Carey and P Atieno:\nhttps://www.bioconductor.org/packages/devel/bioc/vignettes/MLInterfaces/inst/doc/MLprac2_2.html",
    "crumbs": [
      "Supplemental Modules",
      "BCR-ABL Fusion"
    ]
  },
  {
    "objectID": "content/Unit03_GeneticData.html#background-motivation",
    "href": "content/Unit03_GeneticData.html#background-motivation",
    "title": "BCR-ABL Fusion",
    "section": "",
    "text": "Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, but it exhibits marked genetic heterogeneity, with distinct chromosomal translocations defining molecular subtypes with divergent prognoses and therapeutic responses. In B-cell lineage ALL, the BCR-ABL1 fusion (“breakpoint cluster region”/“Abelson”, i.e., the “Philadelphia chromosome”) arises from a t(9;22) translocation and encodes a constitutively active tyrosine kinase. This fusion was historically associated with poor outcomes, until the advent of targeted therapies (e.g., imatinib) revolutionized treatment and survival.\n\n\n\n\nSource: https://www.cancer.gov/publications/dictionaries/cancer-terms/def/philadelphia-chromosome\n\n\n\nHigh-density microarray profiling measures expression of over 7,000 genes in leukemic blast cells, enabling a form of “molecular diagnostics” where supervised learning can:\n\nClassify fusion status or other genetic subtypes when cytogenetic assays (PCR, FISH) are unavailable.\nDiscover novel marker genes and pathways dysregulated by specific fusions.\nPredict therapeutic response and stratify risk using expression-derived scores.\n\nHowever, in many retrospective or multi-center cohorts, only a subset of patients undergo gold-standard fusion testing (e.g., RT-PCR for BCR-ABL1), leaving the majority “unlabeled.” A common workaround is to train a gene expression classifier on the small labeled subset and apply it to the larger unlabeled cohort. Yet, naive downstream analyses, treating predicted labels as ground truth, can yield biased effect estimates and understate uncertainty.\nInference with Predicted Data (IPD) offers a principled remedy: it combines a small labeled set with the predicted labels (e.g., with true fusion status) with a larger unlabeled set, adjusting both bias and variance. In this module, we will:\n\nDescribe the ALL and Golub Bioconductor datasets.\nSubset the ALL data to B-cell ALL and filter to the top 500 variable probes.\nHarmonize probes across platforms via gene symbols.\nSplit the ALL data and train three classifiers for BCR-ABL1 fusion status.\nPredict on the holdout ALL and Golub B-cell ALL data using the ALL-trained classifier.\nApply IPD to estimate the association between fusion status and patient sex assigned at birth.\n\nBy the end, you will understand how to leverage expression-based predictions while maintaining valid inference on genetic subtypes in cohorts with partially missing gold-standard labels.\n\nNote: For the data loading and prediction workflows, we will directly follow the Bioconductor MLInterfaces vignette by VJ Carey and P Atieno:\nhttps://www.bioconductor.org/packages/devel/bioc/vignettes/MLInterfaces/inst/doc/MLprac2_2.html",
    "crumbs": [
      "Supplemental Modules",
      "BCR-ABL Fusion"
    ]
  },
  {
    "objectID": "content/Unit03_GeneticData.html#datasets-overview",
    "href": "content/Unit03_GeneticData.html#datasets-overview",
    "title": "BCR-ABL Fusion",
    "section": "Datasets Overview",
    "text": "Datasets Overview\nWe work with two public Bioconductor datasets:\n\nALL (ALL package): 128 samples of acute lymphoblastic leukemia (both B- and T-cell), profiled on the Affymetrix HGU95Av2 microarray. Phenotype data include BT (immunophenotype), mol.biol (molecular subtype including BCR-ABL1 or NEG), age, and sex.\nGolub_Merge (golubEsets package): 72 samples of leukemias (ALL vs. AML), profiled on Affymetrix HGU6800. Phenotype data include ALL.AML, T.B.cell (lineage), age, and sex.\n\nWe will train our classifiers on a subset of the ALL data, test the model on the holdout ALL data, and perform IPD on the holdout ALL and Golub data.",
    "crumbs": [
      "Supplemental Modules",
      "BCR-ABL Fusion"
    ]
  },
  {
    "objectID": "content/Unit03_GeneticData.html#phenotype-reduction-and-feature-filtering",
    "href": "content/Unit03_GeneticData.html#phenotype-reduction-and-feature-filtering",
    "title": "BCR-ABL Fusion",
    "section": "Phenotype Reduction and Feature Filtering",
    "text": "Phenotype Reduction and Feature Filtering\nWe first narrow to B-cell ALL samples and reduce dimensionality by keeping the 500 most variable probes.\n\n\nCode\n# Load the ALL data\ndata(ALL)\n\n# Subset ALL to B-cell lineage (BT codes starting with \"B\")\nbALL &lt;- ALL[, substr(pData(ALL)$BT, 1, 1) == \"B\"]\n\n# Keep only fusion-negative (\"NEG\") and fusion-positive (\"BCR/ABL\") samples\nfus &lt;- bALL[, bALL$mol.biol %in% c(\"NEG\", \"BCR/ABL\")]\n\n# Convert to factor with clear levels: NEG=0, BCR/ABL=1\nfus$mol.biol &lt;- factor(fus$mol.biol, levels = c(\"NEG\", \"BCR/ABL\"))\n\n# Compute median absolute deviation (MAD) for each probe\ndevs &lt;- apply(exprs(fus), 1, mad)\n\n# Select top 500 most variable probes by MAD ranking\ntop500 &lt;- order(devs, decreasing = TRUE)[1:500]\n\n# Subset ExpressionSet to top 500 probes\ndat_filt &lt;- fus[top500, ]\n\n# Confirm dimensions: 500 probes x n samples\ndim(exprs(dat_filt))\n\n\n[1] 500  79\n\n\n\nExplanation:\n\nmad() is robust to outliers and captures variability.\nFiltering saves time on downstream analysis without losing key signals.\nWe now have dat_filt, a filtered ALL dataset focused on B-cell lineage and the most informative probes.",
    "crumbs": [
      "Supplemental Modules",
      "BCR-ABL Fusion"
    ]
  },
  {
    "objectID": "content/Unit03_GeneticData.html#cross-platform-probe-harmonization",
    "href": "content/Unit03_GeneticData.html#cross-platform-probe-harmonization",
    "title": "BCR-ABL Fusion",
    "section": "Cross-Platform Probe Harmonization",
    "text": "Cross-Platform Probe Harmonization\nTo predict on the Golub data, which uses a different array, we map probes from each platform to gene symbols, then intersect to find a common gene set.\n\n\nCode\n# Load the Golub data\ndata(Golub_Merge)\n\n# Map HGU95Av2 probes (ALL) to gene symbols\ngmap_all &lt;- AnnotationDbi::select(\n  hgu95av2.db,\n  keys    = featureNames(dat_filt),\n  columns = c(\"PROBEID\",\"SYMBOL\"),\n  keytype = \"PROBEID\"\n)\n\n# HGU6800 probes (Golub) to gene symbols\ngmap_golub &lt;- AnnotationDbi::select(\n  hu6800.db,\n  keys    = featureNames(Golub_Merge),\n  columns = c(\"PROBEID\",\"SYMBOL\"),\n  keytype = \"PROBEID\"\n)\n\n# Identify common symbols\ncommon_sym &lt;- intersect(gmap_all$SYMBOL, gmap_golub$SYMBOL)\n\n# For each symbol, pick the first associated probe\nprobe_all &lt;- gmap_all |&gt; \n  filter(SYMBOL %in% common_sym) |&gt; \n  group_by(SYMBOL) |&gt; \n  dplyr::slice(1) |&gt; \n  drop_na()\n\nprobe_golub &lt;- gmap_golub |&gt; \n  filter(SYMBOL %in% common_sym) |&gt; \n  group_by(SYMBOL) |&gt; \n  dplyr::slice(1) |&gt; \n  drop_na()\n\n# Subset ExpressionSets to these probes\ndat_train  &lt;- dat_filt[probe_all$PROBEID, ]\neset_golub &lt;- Golub_Merge[probe_golub$PROBEID, ]\n\n# Further subset Golub to B-cell ALL samples\ngolub_bALL &lt;- eset_golub[, pData(eset_golub)$T.B.cell == \"B-cell\" &\n                           pData(eset_golub)$ALL.AML  == \"ALL\"]\n\nfeatureNames(dat_train) &lt;- probe_all$SYMBOL\nrownames(exprs(dat_train)) &lt;- probe_all$SYMBOL\n\nfeatureNames(golub_bALL) &lt;- probe_golub$SYMBOL\nrownames(exprs(golub_bALL)) &lt;- probe_golub$SYMBOL\n\n\n# Confirm feature dimensions\ndim(dat_train)    # features x ALL samples\n\n\nFeatures  Samples \n     354       79 \n\n\nCode\ndim(golub_bALL)   # features x Golub B-cell ALL samples\n\n\nFeatures  Samples \n     354       38 \n\n\n\ndat_train and golub_BALL now share the same gene-symbol feature set, ready for model training and transfer.",
    "crumbs": [
      "Supplemental Modules",
      "BCR-ABL Fusion"
    ]
  },
  {
    "objectID": "content/Unit03_GeneticData.html#exploratory-data-analysis",
    "href": "content/Unit03_GeneticData.html#exploratory-data-analysis",
    "title": "BCR-ABL Fusion",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWith our filtered training set (dat_train), we explore patterns using a heatmap and principal component analysis (PCA).\n\n\nCode\n# Set subtype colors for visualization\nfcol &lt;- ifelse(dat_train$mol.biol == \"NEG\", \"gray\", \"steelblue\")\n\nheatmap(exprs(dat_train), ColSideColors = fcol)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# PCA on samples\nPCg &lt;- prcomp(t(exprs(dat_train)))\n\n# Scree plot\nplot(PCg)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Scatterplots of PCs\npairs(PCg$x[, 1:5], col = fcol, pch = 19)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Biplot of PC1 vs PC2\nbiplot(PCg) \n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe heatmap reveals distinct blocks of probes whose expression differs between BCR-ABL1 fusion-positive and fusion-negative samples.\nThe PCA scatter shows separation (or overlap) of samples by fusion status along the first two principal components, and to a lesser extent, the next three.\nIn the biplot, probes lying far from the origin along PC1 or PC2 represent genes with the largest loadings, i.e., the greatest contribution to those principal components. Samples positioned similarly share expression patterns in those genes.\nOverall, the modest separation in PCA indicates the need for supervised methods to pinpoint the genes most predictive of BCR-ABL1 status.",
    "crumbs": [
      "Supplemental Modules",
      "BCR-ABL Fusion"
    ]
  },
  {
    "objectID": "content/Unit03_GeneticData.html#traintest-split-and-classifier-training",
    "href": "content/Unit03_GeneticData.html#traintest-split-and-classifier-training",
    "title": "BCR-ABL Fusion",
    "section": "Train/Test Split and Classifier Training",
    "text": "Train/Test Split and Classifier Training\nWe split dat_train (ALL) into 60% training and 40% holdout, then train three ML models.\n\n\nCode\nset.seed(2025)                # for reproducibility\nn_all &lt;- ncol(dat_train)      # number of ALL samples\ntrain_idx &lt;- sample(n_all, size = floor(0.6 * n_all))\n# Training and holdout ExpressionSets\ntrain_eset &lt;- dat_train[, train_idx]\ntest_eset  &lt;- dat_train[, -train_idx]\n\n\nIn this section, we compare three supervised learning methods using the MLInterfaces framework:\n\nDiagonal Linear Discriminant Analysis (dldaI)\nNeural Network (nnetI)\nRandom Forest (randomForestI)\n\nWe will use the MLearn() function, which wraps each algorithm, providing a consistent interface to train, predict, and evaluate models on an ExpressionSet. After fitting, we will use confuMat() to display the confusion matrix, showing the true versus predicted class counts.\nWe train each model on the first 40 most variable probes from the filtered ExpressionSet (features 1:40 in train_eset).\n\nDiagonal Linear Discriminant Analysis (dldaI)\nDiagonal LDA assumes each feature is conditionally independent (covariance matrix is diagonal), which can improve stability in high-dimensional, low-sample settings. We use the dldaI interface to fit and evaluate this classifier.\n\n\nCode\n# Train Diagonal LDA on probes 1:40\ndlda_mod &lt;- MLearn(\n  mol.biol ~ ., # mol.biol ~ . specifies BCR/ABL1 fusion status as the outcome\n  train_eset,   # Filtered ExpressionSet\n  dldaI,        # dldaI is the diagonal LDA interface\n  1:40          # 1:40 selects the first 40 probes\n)\n\n\n[1] \"mol.biol\"\n\n\nCode\n# Print a summary of the fitted model\ndlda_mod\n\n\nMLInterfaces classification output container\nThe call was:\nMLearn(formula = mol.biol ~ ., data = train_eset, .method = dldaI, \n    trainInd = 1:40)\nPredicted outcome distribution for test set:\n\nBCR/ABL     NEG \n      3       4 \n\n\nCode\n# Display the confusion matrix (true vs predicted)\ncm_dlda &lt;- confuMat(dlda_mod)\ncm_dlda\n\n\n         predicted\ngiven     BCR/ABL NEG\n  BCR/ABL       3   1\n  NEG           0   3\n\n\nCode\n# Accuracy of Diagonal LDA\nacc_dlda &lt;- sum(diag(cm_dlda)) / sum(cm_dlda)\ncat(\"Accuracy:\", sprintf(\"%.2f%%\", acc_dlda * 100))\n\n\nAccuracy: 85.71%\n\n\n\n\nNeural Network (nnetI)\nA single hidden layer neural network can capture non-linear relationships. We use the nnetI interface with default parameters plus:\n\nsize = 5: number of hidden neurons\ndecay = 0.01: weight-decay regularization\nMaxNWts = 2000: maximum allowed number of weights\n\n\n\nCode\n# Neural Network (nnetI)\nnn_mod &lt;- MLearn(\n  mol.biol ~ ., \n  train_eset, \n  nnetI, \n  1:40, \n  size=5, \n  decay=0.01, \n  MaxNWts=2000\n)\n\n\n[1] \"mol.biol\"\n# weights:  1781\ninitial  value 31.210991 \niter  10 value 19.645950\niter  20 value 12.779243\niter  30 value 5.459003\niter  40 value 1.841191\niter  50 value 1.334631\niter  60 value 1.133200\niter  70 value 0.875814\niter  80 value 0.704137\niter  90 value 0.625722\niter 100 value 0.610470\nfinal  value 0.610470 \nstopped after 100 iterations\n\n\nCode\n# Show model summary\nnn_mod\n\n\nMLInterfaces classification output container\nThe call was:\nMLearn(formula = mol.biol ~ ., data = train_eset, .method = nnetI, \n    trainInd = 1:40, size = 5, decay = 0.01, MaxNWts = 2000)\nPredicted outcome distribution for test set:\n\nBCR/ABL     NEG \n      5       2 \nSummary of scores on test set (use testScores() method for details):\n[1] 0.2948988\n\n\nCode\n# Confusion matrix for Neural Net\ncm_nn &lt;- confuMat(nn_mod)\ncm_nn\n\n\n         predicted\ngiven     BCR/ABL NEG\n  BCR/ABL       4   0\n  NEG           1   2\n\n\nCode\n# Accuracy of Neural Net\nacc_nn &lt;- sum(diag(cm_nn)) / sum(cm_nn)\ncat(\"Accuracy:\", sprintf(\"%.2f%%\", acc_nn * 100))\n\n\nAccuracy: 85.71%\n\n\n\n\nRandom Forest (randomForestI)\nRandom forests build an ensemble of decision trees on bootstrapped samples and random feature subsets, offering robust performance with little tuning. We use the randomForestI to fit a random forest model:\n\n\nCode\n# Random Forest (randomForestI)\nrf_mod &lt;- MLearn(\n  mol.biol ~ ., \n  train_eset, \n  randomForestI, \n  1:40\n)\n\n\n[1] \"mol.biol\"\n\n\nCode\n# Show model summary\nrf_mod\n\n\nMLInterfaces classification output container\nThe call was:\nMLearn(formula = mol.biol ~ ., data = train_eset, .method = randomForestI, \n    trainInd = 1:40)\nPredicted outcome distribution for test set:\n\nBCR/ABL     NEG \n      4       3 \nSummary of scores on test set (use testScores() method for details):\nBCR/ABL     NEG \n  0.524   0.476 \n\n\nCode\n# Confusion matrix\ncm_rf &lt;- confuMat(rf_mod)\ncm_rf\n\n\n         predicted\ngiven     BCR/ABL NEG\n  BCR/ABL       4   0\n  NEG           0   3\n\n\nCode\n# Accuracy of Random Forest\nacc_rf &lt;- sum(diag(cm_rf)) / sum(cm_rf)\ncat(\"Accuracy:\", sprintf(\"%.2f%%\", acc_rf * 100))\n\n\nAccuracy: 100.00%\n\n\n\nInterpretation: Only the Random Forest achieves perfect separation in the validation samples. This highlights the challenge of discriminating BCR-ABL1 status based solely on the top variable probes and underscores why downstream IPD corrections (for regression inference) remain valuable.",
    "crumbs": [
      "Supplemental Modules",
      "BCR-ABL Fusion"
    ]
  },
  {
    "objectID": "content/Unit03_GeneticData.html#predictions-on-all-test-and-golub",
    "href": "content/Unit03_GeneticData.html#predictions-on-all-test-and-golub",
    "title": "BCR-ABL Fusion",
    "section": "Predictions on ALL Test and Golub",
    "text": "Predictions on ALL Test and Golub\nAfter fitting our classifiers on the training subset of ALL, we now generate predictions on both the ALL holdout test and the Golub B-cell ALL datasets.\n\n\nCode\n# Predict BCR-ABL1 status on ALL test holdout using random forest\nall_pred &lt;- MLInterfaces::predict.classifierOutput(rf_mod, test_eset)\n\n# Compare to true labels in test_eset\nconf_mat_test &lt;- table(\n  Truth    = test_eset$mol.biol,\n  Predicted= all_pred$testPredictions\n)\nconf_mat_test\n\n\n         Predicted\nTruth     BCR/ABL NEG\n  NEG           3  16\n  BCR/ABL      11   2\n\n\n\n\nCode\n# Predict BCR-ABL1 status on Golub B-cell ALL\ngolub_pred &lt;- MLInterfaces::predict.classifierOutput(rf_mod, golub_bALL)\n\n# Summarize prediction counts (no ground truth in Golub)\ntable(golub_pred$testPredictions)\n\n\n\nBCR/ABL     NEG \n     25      13 \n\n\n\nNote: We retain true labels in the ALL test set for later IPD; Golub remains unlabeled.",
    "crumbs": [
      "Supplemental Modules",
      "BCR-ABL Fusion"
    ]
  },
  {
    "objectID": "content/Unit03_GeneticData.html#ipd-inference-on-sex-effect",
    "href": "content/Unit03_GeneticData.html#ipd-inference-on-sex-effect",
    "title": "BCR-ABL Fusion",
    "section": "IPD Inference on Sex Effect",
    "text": "IPD Inference on Sex Effect\nWe will now use Inference with Predicted Data (IPD) to estimate the log-odds effect of sex (male vs female) on true BCR-ABL1 status, combining the holdout ALL data with true and predict fusion status (i.e., our labeled data) and the Golub data with the predicted fusion status (i.e., our unlabeled data)\n\n\nCode\n# Build labeled (ALL test) data frame\ndf_test &lt;- tibble(\n  sample = colnames(test_eset),\n  set    = \"labeled\",\n  true   = as.integer(test_eset$mol.biol == \"BCR/ABL\"),\n  pred   = as.integer(all_pred$testPredictions == \"BCR/ABL\"),\n  sex    = factor(pData(test_eset)$sex, levels = c(\"F\",\"M\"))\n)\n\n# Build unlabeled (Golub) data frame\ndf_golub &lt;- tibble(\n  sample = colnames(golub_bALL),\n  set    = \"unlabeled\",\n  true   = NA_real_,\n  pred   = as.integer(golub_pred$testPredictions == \"BCR/ABL\"),\n  sex    = factor(pData(golub_bALL)$Gender, levels = c(\"F\",\"M\"))\n)\n\n# Combine for IPD\nipd_df &lt;- bind_rows(df_test, df_golub) |&gt; drop_na(sex)\n\n\n\n\nCode\n# Classical regression\nclass_fit &lt;- glm(true ~ sex, \"binomial\", df_test)\nclass_df  &lt;- broom::tidy(class_fit) |&gt;\n  mutate(method = \"Classical\")\n\n# Naive regression\nnaive_fit &lt;- glm(pred ~ sex, \"binomial\", df_golub)\nnaive_df  &lt;- broom::tidy(naive_fit) |&gt;\n  mutate(method = \"Naive\")\n\n# IPD logistic regression\nipd_fit &lt;- ipd(\n  formula   = true - pred ~ sex,\n  method    = \"pspa\",\n  model     = \"logistic\",\n  data      = ipd_df,\n  label     = \"set\",\n)\nipd_df  &lt;- tidy(ipd_fit) |&gt;\n  mutate(method = \"IPD\")\n\n# Summarize coefficients\ncombined &lt;- bind_rows(class_df, naive_df, ipd_df) |&gt;\n  mutate(term = recode(term, \"sexM\" = \"Male (vs Female)\"))\n\n# Forest plot\nggplot(combined, aes(x = estimate, y = term, color = method)) +\n  geom_point(position = position_dodge(width = 0.6)) +\n  geom_errorbarh(aes(xmin = estimate - 1.96 * std.error,\n                     xmax = estimate + 1.96 * std.error),\n                 height = 0.2,\n                 position = position_dodge(width = 0.6)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(values  = c(\"#1B365D\", \"#00C1D5\", \"#AA4AC4\")) +\n  scale_color_manual(values = c(\"#1B365D\", \"#00C1D5\", \"#AA4AC4\")) +\n  labs(\n    x = \"Log-Odds Estimate (95% CI)\",\n    y = \"Covariate\",\n    title = \"Comparison of BCR-ABL1 Model Coefficients\",\n    color = \"Model Type\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nInsight: By treating the ALL test set as labeled and Golub as unlabeled, IPD combines information across cohorts to correct bias and improve precision when estimating the association between BCR-ABL1 status and patient sex.\n\n\nThis is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org",
    "crumbs": [
      "Supplemental Modules",
      "BCR-ABL Fusion"
    ]
  },
  {
    "objectID": "content/Unit01_AlphaFold.html",
    "href": "content/Unit01_AlphaFold.html",
    "title": "Proteomics with AlphaFold",
    "section": "",
    "text": "https://www.nature.com/articles/s41586-021-03819-2",
    "crumbs": [
      "Modules",
      "Proteomics with AlphaFold"
    ]
  },
  {
    "objectID": "content/Unit01_AlphaFold.html#background",
    "href": "content/Unit01_AlphaFold.html#background",
    "title": "Proteomics with AlphaFold",
    "section": "Background",
    "text": "Background\nModern protein biology increasingly relies on machine learning predictions. AlphaFold is an AI-powered system developed by Google DeepMind that predicts a protein’s 3D structure from its amino acid sequence with high accuracy. By utilizing deep learning and neural networks, it has modeled nearly all known proteins, accelerating research in drug discovery, disease understanding, and biotechnology.\nAlphaFold now provides structural annotations for essentially the entire proteome, enabling large-scale analyses that were previously impossible. However, many downstream biological questions still depend on statistical inference, for example, asking whether post-translational modifications (PTMs) preferentially occur in intrinsically disordered regions (IDRs).\nThis module walks through such an analysis using the ipd package. You will learn how to:\n\nCombine ML predictions with limited ground truth\nEstimate odds ratios relating PTMs to disorder\nCompare classical inference to various IPD methods\nQuantify efficiency gains and power improvements from unlabeled data\n\nThe module is designed so that you can reproduce the analysis on your own using only the provided dataset and instructions.",
    "crumbs": [
      "Modules",
      "Proteomics with AlphaFold"
    ]
  },
  {
    "objectID": "content/Unit01_AlphaFold.html#biological-motivation",
    "href": "content/Unit01_AlphaFold.html#biological-motivation",
    "title": "Proteomics with AlphaFold",
    "section": "Biological Motivation",
    "text": "Biological Motivation\nAlphaFold predicts 3D protein structure from sequence at near-experimental accuracy for many proteins. These predictions can be post-processed to identify IDRs, i.e., segments that do not adopt a stable fold.\nIDRs play central roles in signaling and regulation. They are flexible, accessible, and often enriched for short linear motifs. Many post-translational modifications, especially phosphorylation, ubiquitination, and acetylation, are hypothesized to concentrate in IDRs, supporting a model in which structural disorder enables rapid regulatory control.\nIn this workshop, each amino acid residue has:\n\nY: A binary “gold-standard” disorder label\nYhat: A predicted probability of being disordered (derived from AlphaFold)\nPTM Indicators:\n\nphosphorylated\nubiquitinated\nacetylated\n\n\nOur scientific question is:\n\nAre residues with a given PTM more likely to lie in intrinsically disordered regions?",
    "crumbs": [
      "Modules",
      "Proteomics with AlphaFold"
    ]
  },
  {
    "objectID": "content/Unit01_AlphaFold.html#statistical-framing",
    "href": "content/Unit01_AlphaFold.html#statistical-framing",
    "title": "Proteomics with AlphaFold",
    "section": "Statistical Framing",
    "text": "Statistical Framing\nFor a chosen PTM, Z:\n\nLet Z = 1 if the residue carries that PTM\nLet Y = 1 if the residue is in an IDR\n\nWe estimate:\n\\[\n  \\text{OR} = \\frac{\\text{Odds}(Y=1\\mid Z=1)}{\\text{Odds}(Y=1\\mid Z=0)}\n\\]\nusing logistic regression.\nHowever, in practice we may have that:\n\nY is only observed on a labeled subset\nYhat is available everywhere\n\nThis is exactly the setting for Inference with Predicted Data (IPD).",
    "crumbs": [
      "Modules",
      "Proteomics with AlphaFold"
    ]
  },
  {
    "objectID": "content/Unit01_AlphaFold.html#learning-objectives",
    "href": "content/Unit01_AlphaFold.html#learning-objectives",
    "title": "Proteomics with AlphaFold",
    "section": "Learning Objectives ",
    "text": "Learning Objectives \nBy the end of this module, you will be able to:\n\nConstruct labeled/unlabeled splits from real biological data\nEstimate PTM-disorder odds ratios\nApply various IPD methods to draw valid inference\nCompare uncertainty across methods\nCompute labeled sample sizes required for 80% power",
    "crumbs": [
      "Modules",
      "Proteomics with AlphaFold"
    ]
  },
  {
    "objectID": "content/Unit01_AlphaFold.html#summary",
    "href": "content/Unit01_AlphaFold.html#summary",
    "title": "Proteomics with AlphaFold",
    "section": "Summary",
    "text": "Summary\nThis module demonstrates how we can use IPD methods in biological workflows:\n\nUse ML to generate predictions at scale.\nCollect limited ground truth data.\nApply IPD to recover valid inference.\n\nIn this AlphaFold/PTM application, we saw that:\n\nClassical methods do not utilize all available information.\nNaive imputation produces biased estimates and underestimated uncertainty.\nIPD-style approaches preserve validity while improving efficiency.\n\nThis setup appears across modern science and has applications in:\n\nMedical imaging\nGenomics\nRemote sensing\nClinical risk modeling\n\nAnywhere predictions feed into downstream analysis, IPD applies.",
    "crumbs": [
      "Modules",
      "Proteomics with AlphaFold"
    ]
  },
  {
    "objectID": "content/Unit01_AlphaFold.html#optional-take-home-assignment-exercise-10",
    "href": "content/Unit01_AlphaFold.html#optional-take-home-assignment-exercise-10",
    "title": "Proteomics with AlphaFold",
    "section": "Optional Take-Home Assignment (Exercise 10)",
    "text": "Optional Take-Home Assignment (Exercise 10)\n\nExercise 10 (Self-Guided): Compare PTMs\nRepeat the full workflow for:\n\nphosphorylation\nubiquitination\nacetylation\n\nCompare:\n\nestimated ORs\nCI widths\nrequired sample sizes\n\n\n\nBiological context\nDifferent PTMs play distinct regulatory roles:\n\nPhosphorylation is often associated with signaling\nUbiquitination with degradation\nAcetylation with chromatin regulation\n\nYou may find that:\n\nSome PTMs are more enriched in disorder\nSome show weaker or noisier associations\n\n\n\nStatistical goal\nAssess whether:\n\neffect sizes differ across PTMs\nlabeling efficiency gains persist\nconclusions are robust\n\nThis mirrors real exploratory proteomics.",
    "crumbs": [
      "Modules",
      "Proteomics with AlphaFold"
    ]
  },
  {
    "objectID": "content/Unit01_AlphaFold.html#suggested-extensions",
    "href": "content/Unit01_AlphaFold.html#suggested-extensions",
    "title": "Proteomics with AlphaFold",
    "section": "Suggested Extensions",
    "text": "Suggested Extensions\nIf you continue this work:\n\nAdd protein-level clustering (robust SEs)\nInclude covariates (amino acid type, position)\nCompare other methods\nInject noise into Yhat to study robustness\nApply to another ML-derived phenotype",
    "crumbs": [
      "Modules",
      "Proteomics with AlphaFold"
    ]
  },
  {
    "objectID": "content/Unit01_AlphaFold.html#useful-references",
    "href": "content/Unit01_AlphaFold.html#useful-references",
    "title": "Proteomics with AlphaFold",
    "section": "Useful References",
    "text": "Useful References",
    "crumbs": [
      "Modules",
      "Proteomics with AlphaFold"
    ]
  },
  {
    "objectID": "content/Unit01_AlphaFold.html#take-home-messages",
    "href": "content/Unit01_AlphaFold.html#take-home-messages",
    "title": "Proteomics with AlphaFold",
    "section": "Take-Home Messages",
    "text": "Take-Home Messages\nAlphaFold enables proteome-scale structural annotation—but statistical inference still matters.\nBy combining:\n\nML predictions (Yhat)\nlimited truth (Y)\nIPD methodology\n\nyou can:\n\nretain valid uncertainty\ndramatically reduce labeling requirements\nmake principled biological conclusions at scale.\n\nThis paradigm generalizes far beyond protein disorder—to any setting where ML predictions feed into downstream scientific inference.",
    "crumbs": [
      "Modules",
      "Proteomics with AlphaFold"
    ]
  },
  {
    "objectID": "content/slides.html",
    "href": "content/slides.html",
    "title": "Overview",
    "section": "",
    "text": "Artificial intelligence and machine learning (AI/ML) have become essential tools in biomedical research, enabling large-scale analyses across diverse domains such as genomics, structural biology, and electronic health records-based research. Increasingly, researchers rely on model-generated predictions, rather than directly measured variables, as inputs for downstream statistical analyses. For example, predicted gene expression values or polygenic risk scores are often used in place of experimental assays, allowing researchers to expand cohort sizes and explore hypotheses when traditional data collection is infeasible, costly, or time-consuming.\nWhile this practice of “using predictions as data” holds promise for accelerating scientific discovery, it presents significant challenges for statistical inference. When predicted values are used in place of true variables, the resulting estimates of association can be biased and misleading if uncertainty in the prediction step is not properly accounted for.\n\n\n\n\n\n\nKey Takeaway\n\n\n\nModern biomedical analyses increasingly rely on machine learning predictions as inputs to downstream statistical models. This can expand scope and improve feasibility, but treating predictions as if they were observed data can bias effect estimates and understate uncertainty.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/slides.html#background-and-motivation",
    "href": "content/slides.html#background-and-motivation",
    "title": "Overview",
    "section": "",
    "text": "Artificial intelligence and machine learning (AI/ML) have become essential tools in biomedical research, enabling large-scale analyses across diverse domains such as genomics, structural biology, and electronic health records-based research. Increasingly, researchers rely on model-generated predictions, rather than directly measured variables, as inputs for downstream statistical analyses. For example, predicted gene expression values or polygenic risk scores are often used in place of experimental assays, allowing researchers to expand cohort sizes and explore hypotheses when traditional data collection is infeasible, costly, or time-consuming.\nWhile this practice of “using predictions as data” holds promise for accelerating scientific discovery, it presents significant challenges for statistical inference. When predicted values are used in place of true variables, the resulting estimates of association can be biased and misleading if uncertainty in the prediction step is not properly accounted for.\n\n\n\n\n\n\nKey Takeaway\n\n\n\nModern biomedical analyses increasingly rely on machine learning predictions as inputs to downstream statistical models. This can expand scope and improve feasibility, but treating predictions as if they were observed data can bias effect estimates and understate uncertainty.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "content/slides.html#slides-pdf",
    "href": "content/slides.html#slides-pdf",
    "title": "Overview",
    "section": "Slides (PDF)",
    "text": "Slides (PDF)\n\n \n\nOpen slides in a new tab",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Us",
    "section": "",
    "text": "If you have questions please contact:\n\nCarrie Wright (cwrigh60@jhu.edu)"
  },
  {
    "objectID": "content/Unit00_GettingStarted.html",
    "href": "content/Unit00_GettingStarted.html",
    "title": "Getting Started",
    "section": "",
    "text": "Welcome to this workshop on Inference with Predicted Data (IPD)! In this module, we will:\n\nIntroduce the ipd package and its main functions.\nDemonstrate how to generate synthetic data for different types of outcomes.\nFit and compare various IPD methods.\nInspect results using built-in tidy, glance, and augment methods.\n\nThroughout the workshop, exercises are provided with solutions for practice.\n\n\n\n\n\n\nHow to Use This Module\n\n\n\n\nTry each Exercise on your own first.\nExpand the Answer callout to view one possible solution.\nUse the Notes callouts to interpret results and connect them to IPD concepts.\n\n\n\n\n\n\n\n\n\nWhy This Unit Uses Synthetic Data\n\n\n\nUnit 00 intentionally uses synthetic examples. The goal is to make the IPD workflow concrete before moving to real data in later modules.\n\n\n\n\n\n\n\n\nKey Takeaway\n\n\n\nUsing predicted outcomes for can improve efficiency, but treating them as if they are real data can bias downstream inference. This unit shows how IPD methods can use a small set of labeled data to recover valid inference while leveraging unlabeled data and predicted outcomes to gain more information.\n\n\n\n\nWhen an outcome, \\(Y\\), is costly or difficult to measure, it can be tempting to replace missing values with predictions, \\(f(\\boldsymbol{X})\\), from a machine learning model (e.g., a random forest or neural network) built on easier-to-measure features, \\(\\boldsymbol{X}\\). However, using \\(f\\) as if it were the true outcome in downstream analyses, e.g., in estimating a regression coefficient, \\(\\beta\\), for the association between \\(Y\\) and \\(\\boldsymbol{X}\\), leads to biased point estimates and underestimated uncertainty. Methods for Inference with Predicted Data (IPD) address this by leveraging a small subset of “labeled” data with true \\(Y\\) values to calibrate inference in a larger “unlabeled” dataset.\n\n\n\nConsider data arising from three sets of observations:\n\nTraining Set: \\(\\{(X_j, Y_j)\\}_{j=1}^{n_t}\\), used to fit a predictive model, \\(f(\\cdot)\\).\nLabeled Set: \\(\\{(X_i, Y_i)\\}_{i=1}^{n_l}\\), smaller sample with true outcomes measured.\nUnlabeled Set: \\(\\{(X_i)\\}_{i=n_l +1}^{n_l + n_u}\\), only features available.\n\nAfter fitting \\(f\\) on the training set, we apply it to the labeled and unlabeled sets to obtain predictions \\(f_i = f(X_i)\\):\n\n\n\n\nOverview of setup for common ‘inference with predicted data’ problems\n\n\n\nEspecially for ‘good’ predictions, it is tempting to treat \\(f_i\\) as surrogate outcomes and use them to estimate quantities such as regression parameters, \\(\\beta\\). However, as we will see, this leads to invalid inference. By combining the predicted \\(f_i\\) with the observed \\(Y_i\\) in the labeled set, we can calibrate our estimates and standard errors to achieve valid inference.\n\n\n\nConsider a simple linear regression model for the association between \\(Y\\) and \\(X\\). We discuss the following potential estimators, which we will later implement using simulated data.\n\n\nUsing only the unlabeled predictions, the naive OLS estimator solves\n\\[\n\\hat\\gamma_{\\text{naive}} = \\arg\\min_\\gamma \\sum_{i\\in U} (f_i - X_i'\\gamma)^2.\n\\]\nWe are careful to write the coefficients for this model as \\(\\gamma\\), because they bear no necessary correspondence with \\(\\beta\\), except under the extremely restrictive scenario when \\(f\\) perfectly captures the true regression function.\n\n\n\nInstead, a valid approach would be to use only the labeled data. This classical estimator solves\n\\[\n\\hat\\beta_{\\text{classical}} = \\arg\\min_\\beta \\sum_{i\\in L} (Y_i - X_i'\\beta)^2.\n\\] While this approach is valid, it has limited precision because \\(n_l\\) is small in practice and we do not utilize any potential information from the (often much larger) unlabeled data.\n\n\n\nMany estimators tailored to inference with predicted data share a similar form, as given in Ji et al. (2025):\n\\[\n\\widehat\\beta_\\text{ipd} = \\arg\\min_\\beta \\frac{1}{n_l}\\sum_{i=1}^{n_l} \\ell(X_i, Y_i) - \\left[\\frac{1}{n_l}\\sum_{i=1}^{n_l} g(X_i, f_i) - \\frac{1}{n_l+n_u}\\sum_{i=n_l+1}^{n_l+n_u} g(X_i, f_i)\\right],\n\\]\nfor some loss function, \\(\\ell(\\cdot)\\), such as the squared error loss for linear regression, and some \\(g(\\cdot)\\), which they call the ‘imputed loss’. Here, the first term is exactly the classical estimator, which anchors these methods on a valid model, and the second term in the square brackets ‘augments’ the estimator with additional information from the predictions. This allows us to have an estimator that is provably unbiased and asymptotically at least as efficient as the classical estimator, which only uses a fraction of the data.\nThe Inference with Predicted Data (IPD) package implements several recent methods for IPD, such as Chen & Chen method of Gronsbell et al., the Prediction-Powered Inference (PPI) and PPI++ methods of Angelopoulos et al. (a) and Angelopoulos et al. (a), the Post-Prediction Inference (PostPI) method of Wang et al., and the Post-Prediction Adaptive Inference (PSPA) method of Miao et al. to conduct valid, efficient inference, even when a large proportion of outcomes are predicted.\nIn this first tutorial, we demonstrate how to:\n\nGenerate fully synthetic data with ipd::simdat().\nFit a simple linear prediction model (e.g., linear regression).\nApply ipd::ipd() to estimate the association, \\(\\beta\\), between \\(Y\\) and \\(X\\) using labeled and unlabeled data.\nCompare the naive, classical, and IPD estimates of \\(\\beta\\).\nVisualize these results.\n\n\n\n\n\nFirst, insure you have the ipd package and some additional packages installed:\n\n\nCode\n# Install these packages if you have not already:\n# install.packages(c(\"ipd\", \"broom\", \"tidyverse\", \"patchwork\"))\n\nlibrary(ipd)\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n\nThroughout the workshop, we will use reproducible seeds and tidyverse conventions.\n\n\n\nBelow is a high-level summary of the core ipd functions.\n\n\nGenerates synthetic datasets for various inferential models.\n\n\nCode\nsimdat(\n  n,        # Numeric vector of length 3: c(n_train, n_labeled, n_unlabeled)\n  effect,   # Numeric: true effect size for simulation\n  sigma_Y,  # Numeric: residual standard deviation\n  model,    # Character: one of \"mean\", \"quantile\", \"ols\", \"logistic\", \"poisson\"\n  ...       # Additional arguments\n)\n\n\nThis function returns a data.frame with columns:\n\nX1, X2, ...: covariates\nY: true outcome (for training, labeled, and unlabeled subsets)\nf: predictions from the model (for labeled and unlabeled subsets)\nset_label: character indicating “training”, “labeled”, or “unlabeled”.\n\n\n\n\nFits IPD methods for downstream inference on predicted data.\n\n\nCode\nipd(\n  formula,      # A formula: e.g., Y - f ~ X1 + X2 + ...\n  method,       # Character: one of \"chen\", \"postpi_boot\", \"postpi_analytic\", \n                # \"ppi\", \"ppi_all\", \"ppi_plusplus\", \"pspa\"\n  model,        # Character: one of \"mean\", \"quantile\", \"ols\", \"logistic\", \n                # \"poisson\"\n  data,         # Data frame containing columns for formula and label\n  label,        # Character: name of the column with set labels (\"labeled\" and \n                # \"unlabeled\")\n  ...           # Additional arguments\n)\n\n\n\n\n\nchen: Chen and Chen estimator (Gronsbell et al., 2025)\npostpi_analytic: analytic post-prediction inference (Wang et al., 2020).\npostpi_boot: bootstrap post-prediction inference Wang et al., 2020).\nppi: prediction-powered inference (Angelopoulos et al., 2023)\nppi_plusplus: PPI++ (PPI with data-driven weighting; Angelopoulos et al., 2024)\nppi_a: PPI using all data (Gronsbell et al., 2025)\npspa: assumption-lean and data-adaptive post-prediction inference (Miao et al., 2024)\n\n\n\n\n\n\nprint() and summary(): display model summaries.\ntidy(): return a tibble of estimates and standard errors.\nglance(): return a one-row tibble of model-level metrics.\naugment(): return the original data with fitted values and residuals.\n\n\n\n\n\nThe ipd::simdat() function makes it easy to generate:\n\nA training set (where you fit your prediction model),\nA labeled set (where you observe the true \\(Y\\)),\nAn unlabeled set (where \\(Y\\) is presumed missing but you compute predictions \\(f\\)).\n\nWe supply the sample sizes, n = c(n_train, n_label, n_unlabel), an effect size (effect), residual standard deviation (sigma_Y; i.e., how much random noise is in the data), and a model type (\"ols\", \"logistic\", etc.). In this tutorial, we focus on a continuous outcome generated from a linear regression model (\"ols\"). We can also optionally shift and scale the predictions (via the shift and scale arguments) to control how the predicted outcomes relate to their true underlying counterparts.\n\n\nLet us generate a synthetic dataset for a linear model with:\n\n5,000 training observations\n500 labeled observations\n1,500 unlabeled observations\nEffect size = 1.5\nResidual SD = 3\nPredictions shifted by 1 and scaled by 2\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\nset.seed(123)\n\n# n_t = 5000, n_l = 500, n_u = 1500\nn &lt;- c(5000, 500, 1500)\n\n# Effect size = 1.5, noise sd = 3, model = \"ols\" (ordinary least squares)\n# We also shift the mean of the predictions by 1 and scale their values by 2\ndat &lt;- simdat(\n  n       = n,\n  effect  = 1.5,\n  sigma_Y = 3,\n  model   = \"ols\",\n  shift   = 1,\n  scale   = 2\n)\n\n\n\n\nCode\n# The resulting data.frame `dat` has columns:\n#  - X1, X2, X3, X4: Four simulated covariates (all numeric ~ N(0,1))\n#  - Y             : True outcome (available in unlabeled set for simulation)\n#  - f             : Predicted outcome (Generated internally in simdat)\n#  - set_label     : {\"training\", \"labeled\", \"unlabeled\"}\n\n# Quick look:\ndat |&gt; \n  group_by(set_label) |&gt; \n  summarize(n = n()) \n\n\n# A tibble: 3 × 2\n  set_label     n\n  &lt;chr&gt;     &lt;int&gt;\n1 labeled     500\n2 training   5000\n3 unlabeled  1500\n\n\n\n\n\nLet us also inspect the first few rows of each subset:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Training set\ndat |&gt;\n  filter(set_label == \"training\") |&gt;\n  glimpse()\n\n\nRows: 5,000\nColumns: 7\n$ X1        &lt;dbl&gt; -0.56047565, -0.23017749, 1.55870831, 0.07050839, 0.12928774…\n$ X2        &lt;dbl&gt; -1.61803670, 0.37918115, 1.90225048, 0.60187427, 1.73234970,…\n$ X3        &lt;dbl&gt; -0.91006117, 0.28066267, -1.03567040, 0.27304874, 0.53779815…\n$ X4        &lt;dbl&gt; -1.119992047, -1.015819127, 1.258052722, -1.001231731, -0.40…\n$ Y         &lt;dbl&gt; 3.8625325, -1.6575634, 4.1872914, -3.3624963, 6.9978916, 1.5…\n$ f         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ set_label &lt;chr&gt; \"training\", \"training\", \"training\", \"training\", \"training\", …\n\n\nCode\n# Labeled set\ndat |&gt;\n  filter(set_label == \"labeled\") |&gt;\n  glimpse()\n\n\nRows: 500\nColumns: 7\n$ X1        &lt;dbl&gt; -0.4941739, 1.1275935, -1.1469495, 1.4810186, 0.9161912, 0.3…\n$ X2        &lt;dbl&gt; -0.15062996, 0.80094056, -1.18671785, 0.43063636, 0.21674709…\n$ X3        &lt;dbl&gt; 2.0279109, -1.4947497, -1.5729492, -0.3002123, -0.7643735, -…\n$ X4        &lt;dbl&gt; 0.53495620, 0.36182362, -1.89096604, -1.40631763, -0.4019282…\n$ Y         &lt;dbl&gt; 2.71822922, 1.72133689, 0.86081066, 3.77173123, -2.77191549,…\n$ f         &lt;dbl&gt; 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n$ set_label &lt;chr&gt; \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labe…\n\n\nCode\n# Unlabeled set\ndat |&gt;\n  filter(set_label == \"unlabeled\") |&gt;\n  glimpse()\n\n\nRows: 1,500\nColumns: 7\n$ X1        &lt;dbl&gt; -1.35723063, -1.29269781, -1.51720731, 0.85917603, -1.214617…\n$ X2        &lt;dbl&gt; 0.01014789, 1.56213812, 0.41284605, -1.18886219, 0.71454993,…\n$ X3        &lt;dbl&gt; -1.42521509, 1.73298966, 1.66085181, -0.85343610, 0.26905593…\n$ X4        &lt;dbl&gt; -1.1645365, -0.2522693, -1.3945975, 0.3959429, 0.5980741, 1.…\n$ Y         &lt;dbl&gt; -3.8296223, -3.0350282, -5.3736718, 2.6076634, -4.7813463, -…\n$ f         &lt;dbl&gt; -1.9014253, -0.1679210, -0.3030749, 0.1249168, -0.8998546, -…\n$ set_label &lt;chr&gt; \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabel…\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nRows where set_label == \"training\" form an internal training set. Here, Y is observed, but f is NA, as we learn the prediction rule in this set.\nRows where set_label == \"labeled\" also have both Y and f. In practice, f will be generated by your own prediction model; for simulation, simdat does so automatically.\nRows where set_label == \"unlabeled\" have Y for posterity (but in a real‐data scenario, you would not know Y); simdat still generates Y, but the IPD routines will not use these. The column f always contains ‘predicted’ values.\n\n\n\n\n\n\n\nIn practice, we would take the training portion and fit an AI/ML model to predict \\(Y\\) from \\((X_1, X_2, X_3, X_4)\\). This is done automatically by the simdat function, but we will do this for demonstration.\n\n\nLet us fit a linear prediction model on the training data:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# 1) Subset training set\ndat_train &lt;- dat |&gt; \n  filter(set_label == \"training\")\n\n# 2) Fit a linear model: Y ~ X1 + X2 + X3 + X4\nlm_pred &lt;- lm(Y ~ X1 + X2 + X3 + X4, data = dat_train)\n\n# 3) Prepare a full-length vector of NA\ndat$f_pred &lt;- NA_real_\n\n# 4) Identify the rows to predict (all non–training rows)\nidx_analytic &lt;- dat$set_label != \"training\"\n\n# 5) Generate predictions just once on that subset (shifted and scaled to match)\npred_vals &lt;- (predict(lm_pred, newdata = dat[idx_analytic, ]) - 1) / 2 \n\n# 6) Insert them back into the full data frame\ndat$f_pred[idx_analytic] &lt;- pred_vals\n\n# 7) Verify: `f_pred` is equal to `f` for the labeled and unlabeled data\ndat |&gt; \n  select(set_label, Y, f, f_pred) |&gt; \n  filter(set_label != \"training\") |&gt;\n  glimpse()\n\n\nRows: 2,000\nColumns: 4\n$ set_label &lt;chr&gt; \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labe…\n$ Y         &lt;dbl&gt; 2.71822922, 1.72133689, 0.86081066, 3.77173123, -2.77191549,…\n$ f         &lt;dbl&gt; 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n$ f_pred    &lt;dbl&gt; 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nlm(Y ~ X1 + X2 + X3 + X4, data = dat_train) fits an ordinary least squares (OLS) regression on the training subset.\npredict(lm_pred, newdata = .) generates a new f (stored as f_pred) for each row outside of the training set.\nIn real workflows, you might use random forests (ranger::ranger()), gradients (xgboost::xgboost()), or any other ML algorithm; the IPD methods only require that you supply a vector of predictions, f, in your data.\n\n\n\n\n\n\n\n\n\nWe now split the data into two subsets:\n\nlabeled: those rows where we retain the true Y (to be used for final inference alongside their predictions).\nunlabeled: those rows where we hide the true Y (we pretend we do not observe them; ipd will still require the f + covariates).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\ndat_ipd &lt;- dat |&gt;\n  filter(set_label != \"training\") |&gt;\n  # Keep only the columns needed for downstream IPD\n  select(set_label, Y, f, X1, X2, X3, X4) \n\n# Show counts:\ndat_ipd |&gt; \n  group_by(set_label) |&gt; \n  summarize(n = n())\n\n\n# A tibble: 2 × 2\n  set_label     n\n  &lt;chr&gt;     &lt;int&gt;\n1 labeled     500\n2 unlabeled  1500\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nAfter this step, dat_ipd has two groups:\nlabeled (500 rows where we observe both Y and f),\nunlabeled (1500 rows where we only ‘observe’ f).\n\n\n\n\n\n\n\nBefore modeling, it is helpful to see graphically how the predicted values, \\(f\\), compare to the true outcomes, \\(Y\\).\n\n\nWe can visually assess the bias and variance of our predicted outcomes, \\(f\\), versus the true outcomes, \\(Y\\), in our analytic data by plotting:\n\nScatterplot of \\(Y\\) and \\(f\\) vs. \\(X_1\\)\n\nDensity plots of \\(Y\\) and \\(f\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Prepare data\ndat_visualize &lt;- dat_ipd |&gt; \n  select(X1, Y, f) |&gt;\n  pivot_longer(Y:f, names_to = \"Measure\", values_to = \"Value\") |&gt;\n  arrange(Measure)\n\n# Scatter + trend lines\nggplot(dat_visualize, aes(x = X1, y = Value, color = Measure)) +\n  theme_minimal() +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_manual(values = c(\"steelblue\", \"gray\")) +\n  labs(\n    x     = \"X1\",\n    y     = \"True Y or Predicted f\",\n    color = \"Measure\"\n  ) +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n\n\n\nCode\n# Density plots\nggplot(dat_visualize, aes(x = Value, fill = Measure)) +\n  theme_minimal() +\n  geom_density(alpha = 0.4) +\n  scale_fill_manual(values = c(\"steelblue\", \"gray\")) +\n  labs(\n    x     = \"Value\",\n    y     = \"Density\",\n    fill  = \"Measure\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nIn the scatterplot, note that the predicted values \\(f\\) (in blue) lie more tightly along the fitted trend line than the true \\(Y\\) (in gray), indicating stronger correlation with \\(X_1\\).\nIn the density plot, you can see that the spread of \\(f\\) is narrower than that of \\(Y\\), illustrating that the predictive model has reduced variance (often due to “regression to the mean”).\n\n\n\n\n\n\n\nBefore applying IPD, let’s see what happens if we:\n\nRegress the unlabeled predicted f on X1 (the naive approach).\nRegress only the labeled true Y on X1 (the classical approach).\n\nWe will compare these to IPD‐corrected estimates.\n\n\nUsing the labeled and unlabeled sets, fit two models:\n\nNaive OLS on the unlabeled set using lm() with f ~ X1.\nClassical OLS on the labeled set using lm() with Y ~ X1.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# 1) Naive: treat f as if it were truth (only on unlabeled)\nnaive_model &lt;- lm(f ~ X1, data = filter(dat_ipd, set_label == \"unlabeled\"))\n\n# 2) Classical: regress true Y on X1, only on the labeled set\nclassical_model &lt;- lm(Y ~ X1, data = filter(dat_ipd, set_label == \"labeled\"))\n\n\n\n\n\nLet’s also extract the coefficient summaries using the tidy method and compare the results of the two approaches:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\nnaive_df &lt;- tidy(naive_model) |&gt;\n  mutate(method = \"Naive\") |&gt;\n  filter(term == \"X1\") |&gt;\n  select(method, estimate, std.error) \n\nclassical_df &lt;- tidy(classical_model) |&gt;\n  mutate(method = \"Classical\") |&gt;\n  filter(term == \"X1\") |&gt;\n  select(method, estimate, std.error)\n\nbind_rows(naive_df, classical_df)\n\n\n# A tibble: 2 × 3\n  method    estimate std.error\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Naive        0.730    0.0145\n2 Classical    1.48     0.155 \n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe naive coefficient is attenuated, or biased toward zero, since the predictions are imperfect.\nThe classical coefficient is unbiased but has a larger standard error due to the smaller sample size.\n\n\n\n\n\n\n\nThe single wrapper function ipd() implements multiple IPD methods (e.g., Chen & Chen, PostPI, PPI, PPI++, PSPA) for various inferential tasks (e.g., mean and quantile estimation, ols, logistic, and poisson regression).\n\n\n\n\n\n\nReminder\n\n\n\nBasic usage of ipd():\n\n\nCode\nipd(\n  formula = Y - f ~ X1,     # The downstream inferential model\n  method  = \"pspa\"          # The IPD method to run \n  model   = \"ols\"           # The type of inferential model\n  data    = dat_ipd,        # A data.frame with columns:\n                            #   - set_label: {\"labeled\", \"unlabeled\"}\n                            #   - Y: true outcomes (for labeled data)\n                            #   - f: predicted outcomes\n                            #   - X covariates (here X1, X2, X3, X4)\n  label   = \"set_label\",    # Column name indicating \"labeled\"/\"unlabeled\"\n)\n\n\n\n\n\n\nLet’s run one method, pspa, proposed by Miao et al., 2024. The PSPA estimator is an IPD method that combines information from:\n\nLabeled data (where the true outcome, \\(Y\\), and model predictions, \\(f\\), are available), and\n\nUnlabeled data (where only model predictions, \\(f\\), are available).\n\nRather than treating the predicted outcomes with the same importance as the true outcomes, the method estimates a data-driven weight, \\(\\hat{\\omega}\\), and applies it to the predicted outcome contributions:\n\\[\n\\hat{\\beta}_\\text{pspa} = \\hat{\\beta}_\\text{classical} - \\hat{\\omega}\\cdot (\\hat{\\gamma}_\\text{naive}^l - \\hat{\\gamma}_\\text{naive}^u),\n\\]\nwhere \\(\\hat{\\beta}_{\\rm classical}\\) is the estimate from the classical regression, \\(\\hat{\\gamma}_{\\rm naive}^l\\) is the estimate from the naive regression in the labeled data, \\(\\hat{\\gamma}_{\\rm naive}^u\\) is the estimate from the naive regression in the unlabeled data, and \\(\\hat{\\omega}\\) reflects the amount of additional information carried by the predictions. By adaptively weighting the unlabeled information, the PSPA estimator achieves greater precision than by using the labeled data alone, without sacrificing validity, even when the predictions are imperfect.\nLet’s call the method using the ipd() function and collect the estimate for the slope of X1 in a linear regression (model = \"ols\"):\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\nset.seed(123)\nipd_model &lt;- ipd(\n  formula = Y - f ~ X1,\n  data    = dat_ipd,\n  label   = \"set_label\",\n  method  = \"pspa\",\n  model   = \"ols\"\n)\n\nipd_model\n\n\nIPD inference summary\n  Method:   pspa \n  Model:    ols \n  Formula:  Y - f ~ X1 \n\nCoefficients:\n            Estimate Std. Error z value  Pr(&gt;|z|)    \n(Intercept)  0.88014    0.14702  5.9865 2.143e-09 ***\nX1           1.43248    0.14628  9.7929 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nThe ipd_model is an S4 object with slots for things like the coefficient, se, ci, coefTable, fit, formula, data_l, data_u, method, model, and intercept. We can extract the coefficient table using ipd’s tidy helper and compare with the naive and classical methods:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Extract the coefficient estimates\nipd_df &lt;- tidy(ipd_model) |&gt;\n  mutate(method = \"IPD\") |&gt;\n  filter(term == \"X1\") |&gt;\n  select(method, estimate, std.error)\n\n# Combine with naive & classical:\ncompare_tab &lt;- bind_rows(naive_df, classical_df, ipd_df)\ncompare_tab\n\n\n# A tibble: 3 × 3\n  method    estimate std.error\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Naive        0.730    0.0145\n2 Classical    1.48     0.155 \n3 IPD          1.43     0.146 \n\n\n\n\n\n\n\n\nLet’s plot the coefficient estimates and 95% CIs for each of the naive, classical, and IPD methods:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Forest plot of estimates and 95% confidence intervals\ncompare_tab |&gt;\n  mutate(\n    lower = estimate - 1.96 * std.error,\n    upper = estimate + 1.96 * std.error\n  ) |&gt;\n  ggplot(aes(x = estimate, y = method)) +\n    geom_point(size = 2) +\n    geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2) +\n    geom_vline(xintercept = 1.5, linetype = \"dashed\", color = \"red\") +\n    labs(\n      title = \"Comparison of slope estimates \\u00B1 1.96·SE\",\n      x = expression(hat(beta)[X1]),\n      y = \"\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe dashed red line at 1.5 is the true data-generating effect for \\(X_1\\).\nCompare how far each method’s interval is from 1.5, and whether 1.5 lies inside each interval.\n‘Naive’ often severely underestimates (biased); ‘Classical’ is unbiased but wide; IPD methods cluster around 1.5 with better coverage than ‘naive,’ often similar to classical but sometimes narrower.\n\n\n\n\n\n\nUse tidy(), glance(), and augment() on ipd_model. Compare the coefficient estimate and standard error for X1 with the naive fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\ntidy(ipd_model)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.880     0.147    0.592      1.17\n2 X1             1.43      0.146    1.15       1.72\n\n\nCode\nglance(ipd_model)\n\n\n# A tibble: 1 × 6\n  method model intercept nobs_labeled nobs_unlabeled call      \n  &lt;chr&gt;  &lt;chr&gt; &lt;lgl&gt;            &lt;int&gt;          &lt;int&gt; &lt;chr&gt;     \n1 pspa   ols   TRUE               500           1500 Y - f ~ X1\n\n\nCode\naugment(ipd_model) |&gt; glimpse()\n\n\nRows: 1,500\nColumns: 9\n$ set_label &lt;chr&gt; \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabel…\n$ Y         &lt;dbl&gt; -3.8296223, -3.0350282, -5.3736718, 2.6076634, -4.7813463, -…\n$ f         &lt;dbl&gt; -1.9014253, -0.1679210, -0.3030749, 0.1249168, -0.8998546, -…\n$ X1        &lt;dbl&gt; -1.35723063, -1.29269781, -1.51720731, 0.85917603, -1.214617…\n$ X2        &lt;dbl&gt; 0.01014789, 1.56213812, 0.41284605, -1.18886219, 0.71454993,…\n$ X3        &lt;dbl&gt; -1.42521509, 1.73298966, 1.66085181, -0.85343610, 0.26905593…\n$ X4        &lt;dbl&gt; -1.1645365, -0.2522693, -1.3945975, 0.3959429, 0.5980741, 1.…\n$ .fitted   &lt;dbl&gt; -1.0640638, -0.9716221, -1.2932266, 2.1108854, -0.8597740, 1…\n$ .resid    &lt;dbl&gt; -2.7655584, -2.0634061, -4.0804452, 0.4967781, -3.9215723, -…\n\n\nCode\n# Compare with naive\nbroom::tidy(naive_model)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -0.118    0.0146     -8.07 1.40e-15\n2 X1             0.730    0.0145     50.3  0       \n\n\n\n\n\n\n\n\n\n\nNaive regression on predicted outcomes is biased (point estimates are pulled toward zero and SEs are artificially small).\nClassical regression on the labeled data alone is unbiased but inefficient when the labeled set is small.\nIPD methods (Chen & Chen, PPI, PPI++, PostPI, PSPA) strike a balance: they use predictions to effectively enlarge sample size but adjust for prediction error to avoid bias.\nEven with ‘simple’ linear prediction models, IPD corrections can drastically improve inference on downstream regression coefficients.\n\n\n\n\n\nTry other methods such as PPI++ (\"ppi_plusplus\"). How do the results compare?\nRepeat the analysis for a logistic model by setting model = \"logistic\" in both simdat() and ipd().\n\nHappy coding! Feel free to modify and extend these exercises for your own data.\n\nThis is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#background-and-motivation",
    "href": "content/Unit00_GettingStarted.html#background-and-motivation",
    "title": "Getting Started",
    "section": "",
    "text": "When an outcome, \\(Y\\), is costly or difficult to measure, it can be tempting to replace missing values with predictions, \\(f(\\boldsymbol{X})\\), from a machine learning model (e.g., a random forest or neural network) built on easier-to-measure features, \\(\\boldsymbol{X}\\). However, using \\(f\\) as if it were the true outcome in downstream analyses, e.g., in estimating a regression coefficient, \\(\\beta\\), for the association between \\(Y\\) and \\(\\boldsymbol{X}\\), leads to biased point estimates and underestimated uncertainty. Methods for Inference with Predicted Data (IPD) address this by leveraging a small subset of “labeled” data with true \\(Y\\) values to calibrate inference in a larger “unlabeled” dataset.",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#the-ipd-framework",
    "href": "content/Unit00_GettingStarted.html#the-ipd-framework",
    "title": "Getting Started",
    "section": "",
    "text": "Consider data arising from three sets of observations:\n\nTraining Set: \\(\\{(X_j, Y_j)\\}_{j=1}^{n_t}\\), used to fit a predictive model, \\(f(\\cdot)\\).\nLabeled Set: \\(\\{(X_i, Y_i)\\}_{i=1}^{n_l}\\), smaller sample with true outcomes measured.\nUnlabeled Set: \\(\\{(X_i)\\}_{i=n_l +1}^{n_l + n_u}\\), only features available.\n\nAfter fitting \\(f\\) on the training set, we apply it to the labeled and unlabeled sets to obtain predictions \\(f_i = f(X_i)\\):\n\n\n\n\nOverview of setup for common ‘inference with predicted data’ problems\n\n\n\nEspecially for ‘good’ predictions, it is tempting to treat \\(f_i\\) as surrogate outcomes and use them to estimate quantities such as regression parameters, \\(\\beta\\). However, as we will see, this leads to invalid inference. By combining the predicted \\(f_i\\) with the observed \\(Y_i\\) in the labeled set, we can calibrate our estimates and standard errors to achieve valid inference.",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#key-formulas",
    "href": "content/Unit00_GettingStarted.html#key-formulas",
    "title": "Getting Started",
    "section": "",
    "text": "Consider a simple linear regression model for the association between \\(Y\\) and \\(X\\). We discuss the following potential estimators, which we will later implement using simulated data.\n\n\nUsing only the unlabeled predictions, the naive OLS estimator solves\n\\[\n\\hat\\gamma_{\\text{naive}} = \\arg\\min_\\gamma \\sum_{i\\in U} (f_i - X_i'\\gamma)^2.\n\\]\nWe are careful to write the coefficients for this model as \\(\\gamma\\), because they bear no necessary correspondence with \\(\\beta\\), except under the extremely restrictive scenario when \\(f\\) perfectly captures the true regression function.\n\n\n\nInstead, a valid approach would be to use only the labeled data. This classical estimator solves\n\\[\n\\hat\\beta_{\\text{classical}} = \\arg\\min_\\beta \\sum_{i\\in L} (Y_i - X_i'\\beta)^2.\n\\] While this approach is valid, it has limited precision because \\(n_l\\) is small in practice and we do not utilize any potential information from the (often much larger) unlabeled data.\n\n\n\nMany estimators tailored to inference with predicted data share a similar form, as given in Ji et al. (2025):\n\\[\n\\widehat\\beta_\\text{ipd} = \\arg\\min_\\beta \\frac{1}{n_l}\\sum_{i=1}^{n_l} \\ell(X_i, Y_i) - \\left[\\frac{1}{n_l}\\sum_{i=1}^{n_l} g(X_i, f_i) - \\frac{1}{n_l+n_u}\\sum_{i=n_l+1}^{n_l+n_u} g(X_i, f_i)\\right],\n\\]\nfor some loss function, \\(\\ell(\\cdot)\\), such as the squared error loss for linear regression, and some \\(g(\\cdot)\\), which they call the ‘imputed loss’. Here, the first term is exactly the classical estimator, which anchors these methods on a valid model, and the second term in the square brackets ‘augments’ the estimator with additional information from the predictions. This allows us to have an estimator that is provably unbiased and asymptotically at least as efficient as the classical estimator, which only uses a fraction of the data.\nThe Inference with Predicted Data (IPD) package implements several recent methods for IPD, such as Chen & Chen method of Gronsbell et al., the Prediction-Powered Inference (PPI) and PPI++ methods of Angelopoulos et al. (a) and Angelopoulos et al. (a), the Post-Prediction Inference (PostPI) method of Wang et al., and the Post-Prediction Adaptive Inference (PSPA) method of Miao et al. to conduct valid, efficient inference, even when a large proportion of outcomes are predicted.\nIn this first tutorial, we demonstrate how to:\n\nGenerate fully synthetic data with ipd::simdat().\nFit a simple linear prediction model (e.g., linear regression).\nApply ipd::ipd() to estimate the association, \\(\\beta\\), between \\(Y\\) and \\(X\\) using labeled and unlabeled data.\nCompare the naive, classical, and IPD estimates of \\(\\beta\\).\nVisualize these results.",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#installation-and-setup",
    "href": "content/Unit00_GettingStarted.html#installation-and-setup",
    "title": "Getting Started",
    "section": "",
    "text": "First, insure you have the ipd package and some additional packages installed:\n\n\nCode\n# Install these packages if you have not already:\n# install.packages(c(\"ipd\", \"broom\", \"tidyverse\", \"patchwork\"))\n\nlibrary(ipd)\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n\nThroughout the workshop, we will use reproducible seeds and tidyverse conventions.",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#function-references",
    "href": "content/Unit00_GettingStarted.html#function-references",
    "title": "Getting Started",
    "section": "",
    "text": "Below is a high-level summary of the core ipd functions.\n\n\nGenerates synthetic datasets for various inferential models.\n\n\nCode\nsimdat(\n  n,        # Numeric vector of length 3: c(n_train, n_labeled, n_unlabeled)\n  effect,   # Numeric: true effect size for simulation\n  sigma_Y,  # Numeric: residual standard deviation\n  model,    # Character: one of \"mean\", \"quantile\", \"ols\", \"logistic\", \"poisson\"\n  ...       # Additional arguments\n)\n\n\nThis function returns a data.frame with columns:\n\nX1, X2, ...: covariates\nY: true outcome (for training, labeled, and unlabeled subsets)\nf: predictions from the model (for labeled and unlabeled subsets)\nset_label: character indicating “training”, “labeled”, or “unlabeled”.\n\n\n\n\nFits IPD methods for downstream inference on predicted data.\n\n\nCode\nipd(\n  formula,      # A formula: e.g., Y - f ~ X1 + X2 + ...\n  method,       # Character: one of \"chen\", \"postpi_boot\", \"postpi_analytic\", \n                # \"ppi\", \"ppi_all\", \"ppi_plusplus\", \"pspa\"\n  model,        # Character: one of \"mean\", \"quantile\", \"ols\", \"logistic\", \n                # \"poisson\"\n  data,         # Data frame containing columns for formula and label\n  label,        # Character: name of the column with set labels (\"labeled\" and \n                # \"unlabeled\")\n  ...           # Additional arguments\n)\n\n\n\n\n\nchen: Chen and Chen estimator (Gronsbell et al., 2025)\npostpi_analytic: analytic post-prediction inference (Wang et al., 2020).\npostpi_boot: bootstrap post-prediction inference Wang et al., 2020).\nppi: prediction-powered inference (Angelopoulos et al., 2023)\nppi_plusplus: PPI++ (PPI with data-driven weighting; Angelopoulos et al., 2024)\nppi_a: PPI using all data (Gronsbell et al., 2025)\npspa: assumption-lean and data-adaptive post-prediction inference (Miao et al., 2024)\n\n\n\n\n\n\nprint() and summary(): display model summaries.\ntidy(): return a tibble of estimates and standard errors.\nglance(): return a one-row tibble of model-level metrics.\naugment(): return the original data with fitted values and residuals.",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#simulating-data",
    "href": "content/Unit00_GettingStarted.html#simulating-data",
    "title": "Getting Started",
    "section": "",
    "text": "The ipd::simdat() function makes it easy to generate:\n\nA training set (where you fit your prediction model),\nA labeled set (where you observe the true \\(Y\\)),\nAn unlabeled set (where \\(Y\\) is presumed missing but you compute predictions \\(f\\)).\n\nWe supply the sample sizes, n = c(n_train, n_label, n_unlabel), an effect size (effect), residual standard deviation (sigma_Y; i.e., how much random noise is in the data), and a model type (\"ols\", \"logistic\", etc.). In this tutorial, we focus on a continuous outcome generated from a linear regression model (\"ols\"). We can also optionally shift and scale the predictions (via the shift and scale arguments) to control how the predicted outcomes relate to their true underlying counterparts.\n\n\nLet us generate a synthetic dataset for a linear model with:\n\n5,000 training observations\n500 labeled observations\n1,500 unlabeled observations\nEffect size = 1.5\nResidual SD = 3\nPredictions shifted by 1 and scaled by 2\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\nset.seed(123)\n\n# n_t = 5000, n_l = 500, n_u = 1500\nn &lt;- c(5000, 500, 1500)\n\n# Effect size = 1.5, noise sd = 3, model = \"ols\" (ordinary least squares)\n# We also shift the mean of the predictions by 1 and scale their values by 2\ndat &lt;- simdat(\n  n       = n,\n  effect  = 1.5,\n  sigma_Y = 3,\n  model   = \"ols\",\n  shift   = 1,\n  scale   = 2\n)\n\n\n\n\nCode\n# The resulting data.frame `dat` has columns:\n#  - X1, X2, X3, X4: Four simulated covariates (all numeric ~ N(0,1))\n#  - Y             : True outcome (available in unlabeled set for simulation)\n#  - f             : Predicted outcome (Generated internally in simdat)\n#  - set_label     : {\"training\", \"labeled\", \"unlabeled\"}\n\n# Quick look:\ndat |&gt; \n  group_by(set_label) |&gt; \n  summarize(n = n()) \n\n\n# A tibble: 3 × 2\n  set_label     n\n  &lt;chr&gt;     &lt;int&gt;\n1 labeled     500\n2 training   5000\n3 unlabeled  1500\n\n\n\n\n\nLet us also inspect the first few rows of each subset:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Training set\ndat |&gt;\n  filter(set_label == \"training\") |&gt;\n  glimpse()\n\n\nRows: 5,000\nColumns: 7\n$ X1        &lt;dbl&gt; -0.56047565, -0.23017749, 1.55870831, 0.07050839, 0.12928774…\n$ X2        &lt;dbl&gt; -1.61803670, 0.37918115, 1.90225048, 0.60187427, 1.73234970,…\n$ X3        &lt;dbl&gt; -0.91006117, 0.28066267, -1.03567040, 0.27304874, 0.53779815…\n$ X4        &lt;dbl&gt; -1.119992047, -1.015819127, 1.258052722, -1.001231731, -0.40…\n$ Y         &lt;dbl&gt; 3.8625325, -1.6575634, 4.1872914, -3.3624963, 6.9978916, 1.5…\n$ f         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ set_label &lt;chr&gt; \"training\", \"training\", \"training\", \"training\", \"training\", …\n\n\nCode\n# Labeled set\ndat |&gt;\n  filter(set_label == \"labeled\") |&gt;\n  glimpse()\n\n\nRows: 500\nColumns: 7\n$ X1        &lt;dbl&gt; -0.4941739, 1.1275935, -1.1469495, 1.4810186, 0.9161912, 0.3…\n$ X2        &lt;dbl&gt; -0.15062996, 0.80094056, -1.18671785, 0.43063636, 0.21674709…\n$ X3        &lt;dbl&gt; 2.0279109, -1.4947497, -1.5729492, -0.3002123, -0.7643735, -…\n$ X4        &lt;dbl&gt; 0.53495620, 0.36182362, -1.89096604, -1.40631763, -0.4019282…\n$ Y         &lt;dbl&gt; 2.71822922, 1.72133689, 0.86081066, 3.77173123, -2.77191549,…\n$ f         &lt;dbl&gt; 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n$ set_label &lt;chr&gt; \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labe…\n\n\nCode\n# Unlabeled set\ndat |&gt;\n  filter(set_label == \"unlabeled\") |&gt;\n  glimpse()\n\n\nRows: 1,500\nColumns: 7\n$ X1        &lt;dbl&gt; -1.35723063, -1.29269781, -1.51720731, 0.85917603, -1.214617…\n$ X2        &lt;dbl&gt; 0.01014789, 1.56213812, 0.41284605, -1.18886219, 0.71454993,…\n$ X3        &lt;dbl&gt; -1.42521509, 1.73298966, 1.66085181, -0.85343610, 0.26905593…\n$ X4        &lt;dbl&gt; -1.1645365, -0.2522693, -1.3945975, 0.3959429, 0.5980741, 1.…\n$ Y         &lt;dbl&gt; -3.8296223, -3.0350282, -5.3736718, 2.6076634, -4.7813463, -…\n$ f         &lt;dbl&gt; -1.9014253, -0.1679210, -0.3030749, 0.1249168, -0.8998546, -…\n$ set_label &lt;chr&gt; \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabel…\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nRows where set_label == \"training\" form an internal training set. Here, Y is observed, but f is NA, as we learn the prediction rule in this set.\nRows where set_label == \"labeled\" also have both Y and f. In practice, f will be generated by your own prediction model; for simulation, simdat does so automatically.\nRows where set_label == \"unlabeled\" have Y for posterity (but in a real‐data scenario, you would not know Y); simdat still generates Y, but the IPD routines will not use these. The column f always contains ‘predicted’ values.",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#generating-predictions",
    "href": "content/Unit00_GettingStarted.html#generating-predictions",
    "title": "Getting Started",
    "section": "",
    "text": "In practice, we would take the training portion and fit an AI/ML model to predict \\(Y\\) from \\((X_1, X_2, X_3, X_4)\\). This is done automatically by the simdat function, but we will do this for demonstration.\n\n\nLet us fit a linear prediction model on the training data:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# 1) Subset training set\ndat_train &lt;- dat |&gt; \n  filter(set_label == \"training\")\n\n# 2) Fit a linear model: Y ~ X1 + X2 + X3 + X4\nlm_pred &lt;- lm(Y ~ X1 + X2 + X3 + X4, data = dat_train)\n\n# 3) Prepare a full-length vector of NA\ndat$f_pred &lt;- NA_real_\n\n# 4) Identify the rows to predict (all non–training rows)\nidx_analytic &lt;- dat$set_label != \"training\"\n\n# 5) Generate predictions just once on that subset (shifted and scaled to match)\npred_vals &lt;- (predict(lm_pred, newdata = dat[idx_analytic, ]) - 1) / 2 \n\n# 6) Insert them back into the full data frame\ndat$f_pred[idx_analytic] &lt;- pred_vals\n\n# 7) Verify: `f_pred` is equal to `f` for the labeled and unlabeled data\ndat |&gt; \n  select(set_label, Y, f, f_pred) |&gt; \n  filter(set_label != \"training\") |&gt;\n  glimpse()\n\n\nRows: 2,000\nColumns: 4\n$ set_label &lt;chr&gt; \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labe…\n$ Y         &lt;dbl&gt; 2.71822922, 1.72133689, 0.86081066, 3.77173123, -2.77191549,…\n$ f         &lt;dbl&gt; 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n$ f_pred    &lt;dbl&gt; 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nlm(Y ~ X1 + X2 + X3 + X4, data = dat_train) fits an ordinary least squares (OLS) regression on the training subset.\npredict(lm_pred, newdata = .) generates a new f (stored as f_pred) for each row outside of the training set.\nIn real workflows, you might use random forests (ranger::ranger()), gradients (xgboost::xgboost()), or any other ML algorithm; the IPD methods only require that you supply a vector of predictions, f, in your data.",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#creating-labeled-and-unlabeled-datasets",
    "href": "content/Unit00_GettingStarted.html#creating-labeled-and-unlabeled-datasets",
    "title": "Getting Started",
    "section": "",
    "text": "We now split the data into two subsets:\n\nlabeled: those rows where we retain the true Y (to be used for final inference alongside their predictions).\nunlabeled: those rows where we hide the true Y (we pretend we do not observe them; ipd will still require the f + covariates).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\ndat_ipd &lt;- dat |&gt;\n  filter(set_label != \"training\") |&gt;\n  # Keep only the columns needed for downstream IPD\n  select(set_label, Y, f, X1, X2, X3, X4) \n\n# Show counts:\ndat_ipd |&gt; \n  group_by(set_label) |&gt; \n  summarize(n = n())\n\n\n# A tibble: 2 × 2\n  set_label     n\n  &lt;chr&gt;     &lt;int&gt;\n1 labeled     500\n2 unlabeled  1500\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nAfter this step, dat_ipd has two groups:\nlabeled (500 rows where we observe both Y and f),\nunlabeled (1500 rows where we only ‘observe’ f).",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#comparison-of-the-true-vs-predicted-outcomes",
    "href": "content/Unit00_GettingStarted.html#comparison-of-the-true-vs-predicted-outcomes",
    "title": "Getting Started",
    "section": "",
    "text": "Before modeling, it is helpful to see graphically how the predicted values, \\(f\\), compare to the true outcomes, \\(Y\\).\n\n\nWe can visually assess the bias and variance of our predicted outcomes, \\(f\\), versus the true outcomes, \\(Y\\), in our analytic data by plotting:\n\nScatterplot of \\(Y\\) and \\(f\\) vs. \\(X_1\\)\n\nDensity plots of \\(Y\\) and \\(f\\)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Prepare data\ndat_visualize &lt;- dat_ipd |&gt; \n  select(X1, Y, f) |&gt;\n  pivot_longer(Y:f, names_to = \"Measure\", values_to = \"Value\") |&gt;\n  arrange(Measure)\n\n# Scatter + trend lines\nggplot(dat_visualize, aes(x = X1, y = Value, color = Measure)) +\n  theme_minimal() +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_manual(values = c(\"steelblue\", \"gray\")) +\n  labs(\n    x     = \"X1\",\n    y     = \"True Y or Predicted f\",\n    color = \"Measure\"\n  ) +\n  theme(legend.position = \"bottom\") \n\n\n\n\n\n\n\n\n\n\n\nCode\n# Density plots\nggplot(dat_visualize, aes(x = Value, fill = Measure)) +\n  theme_minimal() +\n  geom_density(alpha = 0.4) +\n  scale_fill_manual(values = c(\"steelblue\", \"gray\")) +\n  labs(\n    x     = \"Value\",\n    y     = \"Density\",\n    fill  = \"Measure\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nIn the scatterplot, note that the predicted values \\(f\\) (in blue) lie more tightly along the fitted trend line than the true \\(Y\\) (in gray), indicating stronger correlation with \\(X_1\\).\nIn the density plot, you can see that the spread of \\(f\\) is narrower than that of \\(Y\\), illustrating that the predictive model has reduced variance (often due to “regression to the mean”).",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#some-baselines-naive-vs-classical-inference",
    "href": "content/Unit00_GettingStarted.html#some-baselines-naive-vs-classical-inference",
    "title": "Getting Started",
    "section": "",
    "text": "Before applying IPD, let’s see what happens if we:\n\nRegress the unlabeled predicted f on X1 (the naive approach).\nRegress only the labeled true Y on X1 (the classical approach).\n\nWe will compare these to IPD‐corrected estimates.\n\n\nUsing the labeled and unlabeled sets, fit two models:\n\nNaive OLS on the unlabeled set using lm() with f ~ X1.\nClassical OLS on the labeled set using lm() with Y ~ X1.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# 1) Naive: treat f as if it were truth (only on unlabeled)\nnaive_model &lt;- lm(f ~ X1, data = filter(dat_ipd, set_label == \"unlabeled\"))\n\n# 2) Classical: regress true Y on X1, only on the labeled set\nclassical_model &lt;- lm(Y ~ X1, data = filter(dat_ipd, set_label == \"labeled\"))\n\n\n\n\n\nLet’s also extract the coefficient summaries using the tidy method and compare the results of the two approaches:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\nnaive_df &lt;- tidy(naive_model) |&gt;\n  mutate(method = \"Naive\") |&gt;\n  filter(term == \"X1\") |&gt;\n  select(method, estimate, std.error) \n\nclassical_df &lt;- tidy(classical_model) |&gt;\n  mutate(method = \"Classical\") |&gt;\n  filter(term == \"X1\") |&gt;\n  select(method, estimate, std.error)\n\nbind_rows(naive_df, classical_df)\n\n\n# A tibble: 2 × 3\n  method    estimate std.error\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Naive        0.730    0.0145\n2 Classical    1.48     0.155 \n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe naive coefficient is attenuated, or biased toward zero, since the predictions are imperfect.\nThe classical coefficient is unbiased but has a larger standard error due to the smaller sample size.",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#ipd-corrected-inference-with-ipdipd",
    "href": "content/Unit00_GettingStarted.html#ipd-corrected-inference-with-ipdipd",
    "title": "Getting Started",
    "section": "",
    "text": "The single wrapper function ipd() implements multiple IPD methods (e.g., Chen & Chen, PostPI, PPI, PPI++, PSPA) for various inferential tasks (e.g., mean and quantile estimation, ols, logistic, and poisson regression).\n\n\n\n\n\n\nReminder\n\n\n\nBasic usage of ipd():\n\n\nCode\nipd(\n  formula = Y - f ~ X1,     # The downstream inferential model\n  method  = \"pspa\"          # The IPD method to run \n  model   = \"ols\"           # The type of inferential model\n  data    = dat_ipd,        # A data.frame with columns:\n                            #   - set_label: {\"labeled\", \"unlabeled\"}\n                            #   - Y: true outcomes (for labeled data)\n                            #   - f: predicted outcomes\n                            #   - X covariates (here X1, X2, X3, X4)\n  label   = \"set_label\",    # Column name indicating \"labeled\"/\"unlabeled\"\n)\n\n\n\n\n\n\nLet’s run one method, pspa, proposed by Miao et al., 2024. The PSPA estimator is an IPD method that combines information from:\n\nLabeled data (where the true outcome, \\(Y\\), and model predictions, \\(f\\), are available), and\n\nUnlabeled data (where only model predictions, \\(f\\), are available).\n\nRather than treating the predicted outcomes with the same importance as the true outcomes, the method estimates a data-driven weight, \\(\\hat{\\omega}\\), and applies it to the predicted outcome contributions:\n\\[\n\\hat{\\beta}_\\text{pspa} = \\hat{\\beta}_\\text{classical} - \\hat{\\omega}\\cdot (\\hat{\\gamma}_\\text{naive}^l - \\hat{\\gamma}_\\text{naive}^u),\n\\]\nwhere \\(\\hat{\\beta}_{\\rm classical}\\) is the estimate from the classical regression, \\(\\hat{\\gamma}_{\\rm naive}^l\\) is the estimate from the naive regression in the labeled data, \\(\\hat{\\gamma}_{\\rm naive}^u\\) is the estimate from the naive regression in the unlabeled data, and \\(\\hat{\\omega}\\) reflects the amount of additional information carried by the predictions. By adaptively weighting the unlabeled information, the PSPA estimator achieves greater precision than by using the labeled data alone, without sacrificing validity, even when the predictions are imperfect.\nLet’s call the method using the ipd() function and collect the estimate for the slope of X1 in a linear regression (model = \"ols\"):\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\nset.seed(123)\nipd_model &lt;- ipd(\n  formula = Y - f ~ X1,\n  data    = dat_ipd,\n  label   = \"set_label\",\n  method  = \"pspa\",\n  model   = \"ols\"\n)\n\nipd_model\n\n\nIPD inference summary\n  Method:   pspa \n  Model:    ols \n  Formula:  Y - f ~ X1 \n\nCoefficients:\n            Estimate Std. Error z value  Pr(&gt;|z|)    \n(Intercept)  0.88014    0.14702  5.9865 2.143e-09 ***\nX1           1.43248    0.14628  9.7929 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nThe ipd_model is an S4 object with slots for things like the coefficient, se, ci, coefTable, fit, formula, data_l, data_u, method, model, and intercept. We can extract the coefficient table using ipd’s tidy helper and compare with the naive and classical methods:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Extract the coefficient estimates\nipd_df &lt;- tidy(ipd_model) |&gt;\n  mutate(method = \"IPD\") |&gt;\n  filter(term == \"X1\") |&gt;\n  select(method, estimate, std.error)\n\n# Combine with naive & classical:\ncompare_tab &lt;- bind_rows(naive_df, classical_df, ipd_df)\ncompare_tab\n\n\n# A tibble: 3 × 3\n  method    estimate std.error\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Naive        0.730    0.0145\n2 Classical    1.48     0.155 \n3 IPD          1.43     0.146 \n\n\n\n\n\n\n\n\nLet’s plot the coefficient estimates and 95% CIs for each of the naive, classical, and IPD methods:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Forest plot of estimates and 95% confidence intervals\ncompare_tab |&gt;\n  mutate(\n    lower = estimate - 1.96 * std.error,\n    upper = estimate + 1.96 * std.error\n  ) |&gt;\n  ggplot(aes(x = estimate, y = method)) +\n    geom_point(size = 2) +\n    geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2) +\n    geom_vline(xintercept = 1.5, linetype = \"dashed\", color = \"red\") +\n    labs(\n      title = \"Comparison of slope estimates \\u00B1 1.96·SE\",\n      x = expression(hat(beta)[X1]),\n      y = \"\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe dashed red line at 1.5 is the true data-generating effect for \\(X_1\\).\nCompare how far each method’s interval is from 1.5, and whether 1.5 lies inside each interval.\n‘Naive’ often severely underestimates (biased); ‘Classical’ is unbiased but wide; IPD methods cluster around 1.5 with better coverage than ‘naive,’ often similar to classical but sometimes narrower.\n\n\n\n\n\n\nUse tidy(), glance(), and augment() on ipd_model. Compare the coefficient estimate and standard error for X1 with the naive fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\ntidy(ipd_model)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.880     0.147    0.592      1.17\n2 X1             1.43      0.146    1.15       1.72\n\n\nCode\nglance(ipd_model)\n\n\n# A tibble: 1 × 6\n  method model intercept nobs_labeled nobs_unlabeled call      \n  &lt;chr&gt;  &lt;chr&gt; &lt;lgl&gt;            &lt;int&gt;          &lt;int&gt; &lt;chr&gt;     \n1 pspa   ols   TRUE               500           1500 Y - f ~ X1\n\n\nCode\naugment(ipd_model) |&gt; glimpse()\n\n\nRows: 1,500\nColumns: 9\n$ set_label &lt;chr&gt; \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabel…\n$ Y         &lt;dbl&gt; -3.8296223, -3.0350282, -5.3736718, 2.6076634, -4.7813463, -…\n$ f         &lt;dbl&gt; -1.9014253, -0.1679210, -0.3030749, 0.1249168, -0.8998546, -…\n$ X1        &lt;dbl&gt; -1.35723063, -1.29269781, -1.51720731, 0.85917603, -1.214617…\n$ X2        &lt;dbl&gt; 0.01014789, 1.56213812, 0.41284605, -1.18886219, 0.71454993,…\n$ X3        &lt;dbl&gt; -1.42521509, 1.73298966, 1.66085181, -0.85343610, 0.26905593…\n$ X4        &lt;dbl&gt; -1.1645365, -0.2522693, -1.3945975, 0.3959429, 0.5980741, 1.…\n$ .fitted   &lt;dbl&gt; -1.0640638, -0.9716221, -1.2932266, 2.1108854, -0.8597740, 1…\n$ .resid    &lt;dbl&gt; -2.7655584, -2.0634061, -4.0804452, 0.4967781, -3.9215723, -…\n\n\nCode\n# Compare with naive\nbroom::tidy(naive_model)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -0.118    0.0146     -8.07 1.40e-15\n2 X1             0.730    0.0145     50.3  0",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#summary-and-key-takeaways",
    "href": "content/Unit00_GettingStarted.html#summary-and-key-takeaways",
    "title": "Getting Started",
    "section": "",
    "text": "Naive regression on predicted outcomes is biased (point estimates are pulled toward zero and SEs are artificially small).\nClassical regression on the labeled data alone is unbiased but inefficient when the labeled set is small.\nIPD methods (Chen & Chen, PPI, PPI++, PostPI, PSPA) strike a balance: they use predictions to effectively enlarge sample size but adjust for prediction error to avoid bias.\nEven with ‘simple’ linear prediction models, IPD corrections can drastically improve inference on downstream regression coefficients.",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#further-exploration",
    "href": "content/Unit00_GettingStarted.html#further-exploration",
    "title": "Getting Started",
    "section": "",
    "text": "Try other methods such as PPI++ (\"ppi_plusplus\"). How do the results compare?\nRepeat the analysis for a logistic model by setting model = \"logistic\" in both simdat() and ipd().\n\nHappy coding! Feel free to modify and extend these exercises for your own data.\n\nThis is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org",
    "crumbs": [
      "Modules",
      "Getting Started"
    ]
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html",
    "href": "content/Unit02_MeasuringAdiposity.html",
    "title": "Measuring Adiposity",
    "section": "",
    "text": "Body mass index (BMI) is the most commonly used anthropometric proxy for adiposity in epidemiologic studies and clinical settings. It is simple to calculate, weight in kilograms divided by height in meters squared, and has well-established cut-points for classifying patients as overweight or obese. However, BMI does not distinguish between fat mass and lean mass, nor does it capture fat distribution (visceral versus subcutaneous). As a result, BMI can both under- and over-estimate true adiposity in key subgroups. For example muscular individuals (e.g., athletes) may be misclassified as “obese,” while older adults with sarcopenia (low muscle mass) may fall below BMI thresholds despite having high percent body fat.\n\n\n\n\nSource: https://www.medrxiv.org/content/10.1101/2025.04.01.25325037v1.full.pdf\n\n\n\nWaist circumference (WC) is another simple measure of central adiposity that may better reflect visceral fat, a key driver of metabolic risk. Standard WC cut-points (e.g., ≥ 102 cm in men, ≥ 88 cm in women) identify individuals at increased cardiometabolic risk, even when their BMI is in the normal or overweight range. Yet WC also does not directly quantify total body fat and is influenced by body build, posture, and measurement error.\nDual-energy X-ray absorptiometry (DXA) provides a more accurate “gold-standard” measure of body composition by directly quantifying total and regional fat and lean mass. DXA-derived percent fat thresholds (e.g., &gt; 30% in men, &gt; 42% in women) have been validated against metabolic outcomes. Comparing BMI-based obesity (BMI ≥ 30 kg/m²) and WC-based obesity (WC ≥ 102 cm men, ≥ 88 cm women) with DXA-defined obesity reveals where and how often these common anthropometric proxies misclassify true adiposity.\nIt is worth noting that BMI is often defended as a valid measure for population-level inference, under the assumption that individual-level misclassification errors are harmless when estimating group-level trends. This justification appears frequently in epidemiology, where BMI is treated as a convenient stand-in for adiposity in regression models. But this reasoning masks a deeper issue central to the IPD framework: BMI is itself a prediction model, a crude one, but a model nonetheless. It encodes a deterministic function (weight divided by height squared) to approximate an unobserved latent quantity (body fat), and like any prediction model, it introduces systematic biases that may not vanish with aggregation. IPD adopts a prediction-agnostic perspective: whether the surrogate is a simple index like BMI or a high-dimensional neural network, the challenge is the same: how to draw valid statistical inference when the outcome is a model-based proxy. By treating BMI as a prediction rather than a ground truth, we clarify the role of uncertainty, calibration, and correction, and demonstrate how IPD methods can help recover valid estimates even when only such surrogates are available.\nIn this module, we will use data from the National Health and Nutrition Examination Survey (NHANES) to:\n\nLoad BMI, WC, and DXA-based measures of adiposity and their associated features\nDefine obesity by three standards:\n\nBMI (≥ 30 kg/m²)\nWaist circumference (men ≥ 102 cm; women ≥ 88 cm)\nDXA % body fat (&gt; 30% men; &gt; 42% women)\n\nVisualize misclassification rates overall and by age, sex, and race/ethnicity\nCalibrate both BMI and WC-based obesity measures to DXA using the ipd package\nDiscuss implications for research and practice\n\nBy the end, we may have a better understanding of the strengths and limitations of BMI and WC as proxies for adiposity, know how to assess their sensitivity and specificity versus DXA, and be equipped to correct for measurement error using IPD when only BMI/WC are available.\n\n\n\nThe National Health and Nutrition Examination Survey (NHANES) is a nationally representative program conducted by the U.S. Centers for Disease Control and Prevention that combines in-home interviews with standardized physical examinations and laboratory tests to assess the health and nutritional status of Americans. Initiated in the early 1960s and carried out continuously since 1999 in two-year cycles, NHANES informs public health policy, tracks trends in chronic conditions and risk factors, and support research on diet, disease, and health disparities. NHANES provides:\n\nDXA-measured percent body fat (DXDTOPF) for a subset of participants\nBMI (BMXBMI), derived from measured height and weight\nWaist circumference (BMXWAIST), a potential measure of central fat\n\nAs DXA scans are costly and time-consuming, many studies only record BMI and WC. When our scientific question involves true adiposity (e.g., its association with certain risk factors), but we only have BMI and WC in most participants, we can use ipd to correct our downstream inference on % body fat to account for the fact that these proxies are being used in place of the true DXA measurement.\nNote: DXA scans were collected through the 2017 - 2018 NHANES cycle but were suspended during the COVID-19 pandemic. As a result, subsequent waves, including the August 2021 - August 2023 data, do not include DXA measurements. In this tutorial, we will therefore:\n\nUse the 2017-2018 wave as our labeled data (which includes DXDTOPF).\n\nUse IPD to correct our inference when estimating associations between % body fat and other covariates in the absence of direct DXA measurements in the unlabeled August 2021 - August 2023 wave.\n\nThis approach lets us leverage historic DXA data to predict adiposity in more recent participants, enabling unbiased inference across pre- and post-pandemic cohorts.",
    "crumbs": [
      "Modules",
      "Measuring Adiposity"
    ]
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#background-and-motivation",
    "href": "content/Unit02_MeasuringAdiposity.html#background-and-motivation",
    "title": "Measuring Adiposity",
    "section": "",
    "text": "Body mass index (BMI) is the most commonly used anthropometric proxy for adiposity in epidemiologic studies and clinical settings. It is simple to calculate, weight in kilograms divided by height in meters squared, and has well-established cut-points for classifying patients as overweight or obese. However, BMI does not distinguish between fat mass and lean mass, nor does it capture fat distribution (visceral versus subcutaneous). As a result, BMI can both under- and over-estimate true adiposity in key subgroups. For example muscular individuals (e.g., athletes) may be misclassified as “obese,” while older adults with sarcopenia (low muscle mass) may fall below BMI thresholds despite having high percent body fat.\n\n\n\n\nSource: https://www.medrxiv.org/content/10.1101/2025.04.01.25325037v1.full.pdf\n\n\n\nWaist circumference (WC) is another simple measure of central adiposity that may better reflect visceral fat, a key driver of metabolic risk. Standard WC cut-points (e.g., ≥ 102 cm in men, ≥ 88 cm in women) identify individuals at increased cardiometabolic risk, even when their BMI is in the normal or overweight range. Yet WC also does not directly quantify total body fat and is influenced by body build, posture, and measurement error.\nDual-energy X-ray absorptiometry (DXA) provides a more accurate “gold-standard” measure of body composition by directly quantifying total and regional fat and lean mass. DXA-derived percent fat thresholds (e.g., &gt; 30% in men, &gt; 42% in women) have been validated against metabolic outcomes. Comparing BMI-based obesity (BMI ≥ 30 kg/m²) and WC-based obesity (WC ≥ 102 cm men, ≥ 88 cm women) with DXA-defined obesity reveals where and how often these common anthropometric proxies misclassify true adiposity.\nIt is worth noting that BMI is often defended as a valid measure for population-level inference, under the assumption that individual-level misclassification errors are harmless when estimating group-level trends. This justification appears frequently in epidemiology, where BMI is treated as a convenient stand-in for adiposity in regression models. But this reasoning masks a deeper issue central to the IPD framework: BMI is itself a prediction model, a crude one, but a model nonetheless. It encodes a deterministic function (weight divided by height squared) to approximate an unobserved latent quantity (body fat), and like any prediction model, it introduces systematic biases that may not vanish with aggregation. IPD adopts a prediction-agnostic perspective: whether the surrogate is a simple index like BMI or a high-dimensional neural network, the challenge is the same: how to draw valid statistical inference when the outcome is a model-based proxy. By treating BMI as a prediction rather than a ground truth, we clarify the role of uncertainty, calibration, and correction, and demonstrate how IPD methods can help recover valid estimates even when only such surrogates are available.\nIn this module, we will use data from the National Health and Nutrition Examination Survey (NHANES) to:\n\nLoad BMI, WC, and DXA-based measures of adiposity and their associated features\nDefine obesity by three standards:\n\nBMI (≥ 30 kg/m²)\nWaist circumference (men ≥ 102 cm; women ≥ 88 cm)\nDXA % body fat (&gt; 30% men; &gt; 42% women)\n\nVisualize misclassification rates overall and by age, sex, and race/ethnicity\nCalibrate both BMI and WC-based obesity measures to DXA using the ipd package\nDiscuss implications for research and practice\n\nBy the end, we may have a better understanding of the strengths and limitations of BMI and WC as proxies for adiposity, know how to assess their sensitivity and specificity versus DXA, and be equipped to correct for measurement error using IPD when only BMI/WC are available.\n\n\n\nThe National Health and Nutrition Examination Survey (NHANES) is a nationally representative program conducted by the U.S. Centers for Disease Control and Prevention that combines in-home interviews with standardized physical examinations and laboratory tests to assess the health and nutritional status of Americans. Initiated in the early 1960s and carried out continuously since 1999 in two-year cycles, NHANES informs public health policy, tracks trends in chronic conditions and risk factors, and support research on diet, disease, and health disparities. NHANES provides:\n\nDXA-measured percent body fat (DXDTOPF) for a subset of participants\nBMI (BMXBMI), derived from measured height and weight\nWaist circumference (BMXWAIST), a potential measure of central fat\n\nAs DXA scans are costly and time-consuming, many studies only record BMI and WC. When our scientific question involves true adiposity (e.g., its association with certain risk factors), but we only have BMI and WC in most participants, we can use ipd to correct our downstream inference on % body fat to account for the fact that these proxies are being used in place of the true DXA measurement.\nNote: DXA scans were collected through the 2017 - 2018 NHANES cycle but were suspended during the COVID-19 pandemic. As a result, subsequent waves, including the August 2021 - August 2023 data, do not include DXA measurements. In this tutorial, we will therefore:\n\nUse the 2017-2018 wave as our labeled data (which includes DXDTOPF).\n\nUse IPD to correct our inference when estimating associations between % body fat and other covariates in the absence of direct DXA measurements in the unlabeled August 2021 - August 2023 wave.\n\nThis approach lets us leverage historic DXA data to predict adiposity in more recent participants, enabling unbiased inference across pre- and post-pandemic cohorts.",
    "crumbs": [
      "Modules",
      "Measuring Adiposity"
    ]
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#data-preparation-and-exploration",
    "href": "content/Unit02_MeasuringAdiposity.html#data-preparation-and-exploration",
    "title": "Measuring Adiposity",
    "section": "Data Preparation and Exploration",
    "text": "Data Preparation and Exploration\nWe will now load a pre-compiled NHANES dataset, explore key variables (BMI, waist circumference, DXA % body fat) by cohort, and define obesity categories for later misclassification analyses and IPD.\n\nLoading the Pre-Compiled NHANES Data\nFor convenience, we prepared and saved a data file, data/NHANES.rda, which contains a tibble, NHANES, with the following columns:\n\nSEQN - Respondent ID\n\nCohort - Factor: “2017-2018” or “2021-2023”\n\nAge - Factor: “Under 20”, “20-39”, “40-59”, or “60+”\nSex - Factor: “Male” or “Female”\n\nRace - Factor: “Non-Hispanic White”, “Hispanic”, “Non-Hispanic Black”, “Non-Hispanic Asian”, or “Other Race - Including Multi-Racial”\nSmoking - Factor: “Never Smoker”, “Former Smoker”, or “Current Smoker”\nEducation - Factor: “Less than high school”, “High school graduate/GED or equivalent”, “Some college or AA degree”, “College graduate or above”, or “Refused/Unknown”\nBMXBMI - Body Mass Index (kg/m²)\n\nBMXWAIST - Waist Circumference (cm)\n\nDXDTOPF - DXA Percent Body Fat (only measured in 2017-2018; NA in 2021-2023)\nWTMEC4YR - Four-Year Adjusted Survey Weights\n\nNote: The the code used to produce this dataset is available in this repository at inst/NHANES_DATA.R.\n\n\nExercise 1: Install and Load the Necessary Packages\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Install these packages if you have not already:\n# install.packages(c(\"ipd\", \"broom\", \"scales\", \"tidyverse\"))\n\nlibrary(ipd)        # Inference with predicted data\nlibrary(broom)      # Convert model objects (lm, glm, ipd) into tidy data.frames\nlibrary(scales)     # Formatting scales and labels for ggplot2\nlibrary(tidyverse)  # Meta‐package for data manipulation and visualization\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nLoad packages once here so all remaining exercises run cleanly.\n\n\n\n\nExercise 2: Load the Data\nWe first load the prepared dataset and take a look at its features:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Load the dataset\nload(\"data/NHANES.RData\")\n\n# Inspect its structure\nglimpse(NHANES)\n\n\nRows: 11,782\nColumns: 11\n$ SEQN      &lt;dbl&gt; 93706, 93707, 93711, 93712, 93714, 93717, 93719, 93725, 9372…\n$ Cohort    &lt;fct&gt; 2017-2018, 2017-2018, 2017-2018, 2017-2018, 2017-2018, 2017-…\n$ Age       &lt;fct&gt; Under 20, Under 20, 40-59, Under 20, 40-59, 20-39, Under 20,…\n$ Sex       &lt;fct&gt; Male, Male, Male, Male, Female, Male, Female, Female, Male, …\n$ Race      &lt;fct&gt; Non-Hispanic Asian, Other Race - Including Multi-Racial, Non…\n$ Smoking   &lt;fct&gt; NA, NA, NA, NA, Former Smoker, NA, NA, NA, NA, NA, NA, NA, N…\n$ Education &lt;fct&gt; Refused/Unknown, Refused/Unknown, College graduate or above,…\n$ BMXBMI    &lt;dbl&gt; 21.5, 18.1, 21.3, 19.7, 39.9, 24.5, 26.0, 16.1, 27.6, 28.6, …\n$ BMXWAIST  &lt;dbl&gt; 79.3, 64.1, 86.6, 72.0, 118.4, 86.2, 86.0, 66.8, 101.5, 96.3…\n$ DXDTOPF   &lt;dbl&gt; 22.7, 19.0, 22.8, 16.7, 42.1, 20.4, 33.4, 26.9, 29.4, 22.8, …\n$ WTMEC4YR  &lt;dbl&gt; 4361.720, 3532.305, 6195.460, 15168.327, 7739.791, 30058.968…\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nVerify the data schema now to avoid debugging later steps.\n\n\n\n\nExercise 3: Overview by Cohort\nWe can now get a sense of the different cohorts and our variables of interest:\n\nSample Sizes\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\nNHANES |&gt;\n  count(Cohort) |&gt;\n  mutate(pct = n / sum(n) * 100)\n\n\n     Cohort    n      pct\n1 2017-2018 3617 30.69937\n2 2021-2023 8165 69.30063\n\n\n\nDXA Missingness\n\n\nCode\nNHANES |&gt;\n  group_by(Cohort) |&gt;\n  summarize(\n    total       = n(),\n    pct_missing = mean(is.na(DXDTOPF)) * 100)\n\n\n# A tibble: 2 × 3\n  Cohort    total pct_missing\n  &lt;fct&gt;     &lt;int&gt;       &lt;dbl&gt;\n1 2017-2018  3617           0\n2 2021-2023  8165         100\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nExpected:\n\n2017-2018: DXA (DXDTOPF) present for analytic participants.\n2021-2023: DXDTOPF entirely missing (post-pandemic no DXA).\n\n\n\n\n\n\nDiscussion Questions\n\nWhy is this a natural labeled/unlabeled split for IPD?\nWhat inferential bias would appear if we ignored the structured missingness?\n\n\n\nExercise 4: Descriptive Statistics\n\nBMI, Waist Circumference, and DXA % Fat\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\nNHANES |&gt;\n  group_by(Cohort) |&gt;\n  summarize(\n    BMI_mean = mean(BMXBMI),\n    BMI_sd   = sd(BMXBMI),\n    WC_mean  = mean(BMXWAIST),\n    WC_sd    = sd(BMXWAIST),\n    DXA_mean = mean(DXDTOPF),\n    DXA_sd   = sd(DXDTOPF)\n  ) |&gt;\n  pivot_longer(-Cohort)|&gt;\n  separate(name, c(\"Metric\", \"Statistic\")) |&gt;\n  pivot_wider(names_from = Statistic, values_from = value)\n\n\n# A tibble: 6 × 4\n  Cohort    Metric  mean    sd\n  &lt;fct&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 2017-2018 BMI     26.5  7.24\n2 2017-2018 WC      89.3 18.8 \n3 2017-2018 DXA     32.3  8.70\n4 2021-2023 BMI     27.1  7.96\n5 2021-2023 WC      92.1 22.0 \n6 2021-2023 DXA     NA   NA   \n\n\n\nDistributions of Continuous Measures of Adiposity\n\n\nCode\nNHANES |&gt;\n  pivot_longer(c(BMXBMI, BMXWAIST, DXDTOPF)) |&gt;\n  ggplot(aes(x = value, fill = Cohort)) +\n    facet_wrap(~ name, scales = \"free\") +\n    geom_histogram(alpha = 0.6, position = \"identity\", bins = 30) +\n    scale_fill_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n    labs(\n      title = \"Measures of Adiposity by Cohort\", \n      x = \"BMI (kg/m2), WC (cm), or % Fat\", \n      y = \"Count\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nDXA vs Anthropometry in 2017-2018\nOnly the 2017-2018 cohort has DXDTOPF, so we examine how BMI and WC relate to true % body fat among the study participants in this wave:\n\n\nCode\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  pivot_longer(c(BMXBMI, BMXWAIST)) |&gt;\n  ggplot(aes(x = value, y = DXDTOPF)) +\n    facet_wrap(~ name, scales = \"free_x\") +\n    geom_point(alpha = 0.4) +\n    geom_smooth(method = \"lm\", color = \"#1B365D\") +\n    labs(\n      title = \"DXA % Body Fat vs BMI and WC (2017-2018)\",\n      x = \"BMI (kg/m2) or WC (cm)\", y = \"DXDTOPF (%)\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nLet’s also compare these measures by Sex. We can also add reference lines to see where the measure-specific obesity cut-offs would be:\n\n\nCode\ncutoffs &lt;- tibble(\n  name = c(\"BMXBMI\", \"BMXBMI\", \"BMXWAIST\", \"BMXWAIST\"),\n  Sex  = c(\"Male\", \"Female\", \"Male\", \"Female\"),\n  xint = c(30, 30, 102, 88),\n  yint = c(30, 42,  30, 42)\n)\n\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  pivot_longer(c(BMXBMI, BMXWAIST)) |&gt;\n  ggplot(aes(x = value, y = DXDTOPF, group = Sex, fill = Sex, color = Sex)) +\n    facet_wrap(~ name, scales = \"free_x\") +\n    geom_point(alpha = 0.4) +\n    geom_smooth(method = \"lm\") +\n    geom_vline(data = cutoffs, \n      aes(xintercept = xint, color = Sex), linetype = \"dashed\") +\n    geom_hline(data = cutoffs, \n      aes(yintercept = yint, color = Sex), linetype = \"dashed\") +\n    scale_fill_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n    scale_color_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n    labs(\n      title = \"DXA % Body Fat vs BMI and WC (2017-2018)\",\n      x = \"BMI (kg/m2) or WC (cm)\", y = \"DXDTOPF (%)\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nBMI and WC both show strong positive association with DXA % fat.\nSlopes differ by sex, suggesting different proxy behavior across groups.\nMismatched quadrants indicate real proxy misclassification.\n\n\n\n\n\n\nDiscussion Questions\n\nWhich proxy looks better aligned with DXA in this cohort?\nHow might sex-specific thresholding improve proxy calibration?\n\n\n\nExercise 5: Defining Obesity Categories\nLet us create three binary indicators:\n\nobese_BMI: BMI ≥ 30 kg/m²\nobese_WC: WC ≥ 102 cm (men) or ≥ 88 cm (women)\nobese_DXA: DXA % Fat &gt; 30% (men) or &gt; 42% (women)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\nNHANES &lt;- NHANES |&gt;\n  mutate(\n    obese_BMI = BMXBMI &gt;= 30,\n    obese_WC  = case_when(\n      Sex == \"Male\"   & BMXWAIST &gt;= 102 ~ TRUE,\n      Sex == \"Female\" & BMXWAIST &gt;=  88 ~ TRUE,\n      .default                          = FALSE),\n    obese_DXA = case_when(\n      is.na(DXDTOPF)                 ~ NA,\n      Sex == \"Male\"   & DXDTOPF &gt; 30 ~ TRUE,\n      Sex == \"Female\" & DXDTOPF &gt; 42 ~ TRUE,\n      .default                       = FALSE)\n  )\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nThese indicators define proxy outcomes (obese_BMI, obese_WC) and the reference outcome (obese_DXA) for downstream analyses.\n\n\n\n\nExercise 6: Misclassification in 2017-2018\nCompare BMI-defined vs DXA-defined obesity:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Overall\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  count(obese_DXA, obese_BMI) |&gt;\n  mutate(percent = sprintf(\"%.1f%%\", 100 * n / sum(n)))\n\n\n  obese_DXA obese_BMI    n percent\n1     FALSE     FALSE 2180   60.3%\n2     FALSE      TRUE  307    8.5%\n3      TRUE     FALSE  415   11.5%\n4      TRUE      TRUE  715   19.8%\n\n\nCode\n# By Sex\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  count(Sex, obese_DXA, obese_BMI) |&gt;\n  group_by(Sex) |&gt;\n  mutate(percent = sprintf(\"%.1f%%\", 100 * n / sum(n))) |&gt;\n  ungroup()\n\n\n# A tibble: 8 × 5\n  Sex    obese_DXA obese_BMI     n percent\n  &lt;fct&gt;  &lt;lgl&gt;     &lt;lgl&gt;     &lt;int&gt; &lt;chr&gt;  \n1 Male   FALSE     FALSE      1003 56.1%  \n2 Male   FALSE     TRUE        138 7.7%   \n3 Male   TRUE      FALSE       302 16.9%  \n4 Male   TRUE      TRUE        345 19.3%  \n5 Female FALSE     FALSE      1177 64.4%  \n6 Female FALSE     TRUE        169 9.2%   \n7 Female TRUE      FALSE       113 6.2%   \n8 Female TRUE      TRUE        370 20.2%  \n\n\nAnd WC-defined vs DXA-defined:\n\n\nCode\n# Overall\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  count(obese_DXA, obese_WC) |&gt;\n  mutate(percent = sprintf(\"%.1f%%\", 100 * n / sum(n)))\n\n\n  obese_DXA obese_WC    n percent\n1     FALSE    FALSE 1959   54.2%\n2     FALSE     TRUE  528   14.6%\n3      TRUE    FALSE  319    8.8%\n4      TRUE     TRUE  811   22.4%\n\n\nCode\n# By Sex\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  count(Sex, obese_DXA, obese_WC) |&gt;\n  group_by(Sex) |&gt;\n  mutate(percent = sprintf(\"%.1f%%\", 100 * n / sum(n))) |&gt;\n  ungroup()\n\n\n# A tibble: 8 × 5\n  Sex    obese_DXA obese_WC     n percent\n  &lt;fct&gt;  &lt;lgl&gt;     &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n1 Male   FALSE     FALSE     1020 57.0%  \n2 Male   FALSE     TRUE       121 6.8%   \n3 Male   TRUE      FALSE      286 16.0%  \n4 Male   TRUE      TRUE       361 20.2%  \n5 Female FALSE     FALSE      939 51.3%  \n6 Female FALSE     TRUE       407 22.3%  \n7 Female TRUE      FALSE       33 1.8%   \n8 Female TRUE      TRUE       450 24.6%  \n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nBMI vs DXA: meaningful misclassification, with sex-specific asymmetry.\nWC vs DXA: also misclassifies, with a different error profile from BMI.\nProxy choice changes both bias direction and uncertainty in downstream models.\n\n\n\n\n\nDiscussion Questions\n\nWhich error pattern would most distort regression coefficients?\nHow might subgroup-specific calibration reduce these errors?\n\n\n\nExercise 7: Additional Stratified Comparisons by Measure\nLet us reshape the data to long format so that BMI, WC, and DXA-defined obesity are all in one “Measure” column:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Assume NHANES is already loaded and has Cohort, Age, Sex, Race, \n# and the three obesity variables\nnhanes_long &lt;- NHANES |&gt;\n  select(Cohort, Age, Sex, Race, obese_BMI, obese_WC, obese_DXA) |&gt;\n  pivot_longer(\n    cols = starts_with(\"obese_\"),\n    names_to = \"Measure\",\n    values_to = \"Obese\"\n  ) |&gt;\n  mutate(\n    Measure = recode(Measure,\n      obese_BMI = \"BMI\",\n      obese_WC  = \"Waist Circumference\",\n      obese_DXA = \"DXA\"\n    )\n  ) |&gt;\n  filter(!is.na(Obese))\n\n\n\nBy Age Group\n\n\nCode\nprop_age &lt;- nhanes_long |&gt;\n  group_by(Cohort, Age, Measure, Obese) |&gt;\n  summarize(n = n(), .groups = \"drop\") |&gt;\n  group_by(Cohort, Age, Measure) |&gt;\n  mutate(prop = n / sum(n))\n\nggplot(prop_age, aes(x = Age, y = prop, fill = Obese)) +\n  geom_col(position = \"stack\") +\n  facet_grid(Cohort ~ Measure) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  scale_color_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  labs(\n    x     = \"Age Group\",\n    y     = \"Percent\",\n    fill  = \"Obesity Status\",\n    title = \"Obesity Classification by Age, Cohort, and Measure\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBy Sex\n\n\nCode\nprop_sex &lt;- nhanes_long |&gt;\n  group_by(Cohort, Sex, Measure, Obese) |&gt;\n  summarize(n = n(), .groups = \"drop\") |&gt;\n  group_by(Cohort, Sex, Measure) |&gt;\n  mutate(prop = n / sum(n))\n\nggplot(prop_sex, aes(x = Sex, y = prop, fill = Obese)) +\n  geom_col(position = \"stack\") +\n  facet_grid(Cohort ~ Measure) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  scale_color_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  labs(\n    x     = \"Sex\",\n    y     = \"Percent\",\n    fill  = \"Obesity Status\",\n    title = \"Obesity Classification by Sex, Cohort, and Measure\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBy Race/Ethnicity\n\n\nCode\nprop_race &lt;- nhanes_long |&gt;\n  group_by(Cohort, Race, Measure, Obese) |&gt;\n  summarize(n = n(), .groups = \"drop\") |&gt;\n  group_by(Cohort, Race, Measure) |&gt;\n  mutate(prop = n / sum(n))\n\nggplot(prop_race, aes(x = Race, y = prop, fill = Obese)) +\n  geom_col(position = \"stack\") +\n  facet_grid(Cohort ~ Measure) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  scale_color_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  labs(\n    x     = \"Race / Ethnicity\",\n    y     = \"Percent\",\n    fill  = \"Obesity Status\",\n    title = \"Obesity Classification by Race/Ethnicity, Cohort, and Measure\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nStratified charts reveal where proxy-defined obesity diverges most from DXA by subgroup.\n\n\n\n\nDiscussion Questions\nBMI misclassification varies systematically across subgroups:\n\nAge: Young adults (18-34) often have lower lean mass, so normal-BMI individuals can still have high body fat (“normal-weight obesity”).\nSex: Women generally have higher percent-fat at the same BMI; sex-specific DXA thresholds help adjust for this.\nRace/Ethnicity: Differences in body composition and fat distribution mean that a single BMI cut-point may not correspond to the same adiposity level across groups.\n\nImplications: Reliance on BMI alone can bias epidemiologic associations with true adiposity-driven outcomes (e.g., diabetes, cardiovascular disease). When DXA or other body-composition measures are impractical, consider calibration equations or subgroup-specific BMI thresholds.\nNext: we will split the combined NHANES data into labeled (2017-2018 with DXDTOPF) and unlabeled (2021-2023 without DXA), and then proceed with IPD using BMI or WC as our proxy.",
    "crumbs": [
      "Modules",
      "Measuring Adiposity"
    ]
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#demographic-associations-naive-vs-classical-vs-ipd",
    "href": "content/Unit02_MeasuringAdiposity.html#demographic-associations-naive-vs-classical-vs-ipd",
    "title": "Measuring Adiposity",
    "section": "Demographic Associations: Naive vs Classical vs IPD",
    "text": "Demographic Associations: Naive vs Classical vs IPD\n\nExercise 8: Naive vs Classical vs IPD Inference\nWe are interested in studying the association between true percent body fat (DXA % fat) and risk factors such as age, sex, and race. We model the binary obesity outcome (DXA-defined “true” vs BMI/WC “proxy”) as a function of Age, Sex, and Race using logistic regression:\n\nNaive: proxy-only model on unlabeled data (2021-2023)\n\nClassical: true-only model on labeled data (2017-2018)\n\nIPD: combine both while correcting for proxy error\n\n\nExercise 8a: Setup Labeled vs Unlabeled Data\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Split NHANES into labeled (DXA available) vs unlabeled\nlabeled   &lt;- NHANES |&gt; filter(Cohort == \"2017-2018\")\nunlabeled &lt;- NHANES |&gt; filter(Cohort == \"2021-2023\")\n\n# Stack for IPD\ncombined &lt;- bind_rows(\n  labeled   |&gt; mutate(set_label = \"labeled\"),\n  unlabeled |&gt; mutate(set_label = \"unlabeled\")\n)\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nThis split defines the labeled and unlabeled sets used by IPD.\n\n\n\n\nExercise 8b: Naive versus Classical Regressions\nWe fit:\n\nNaive-BMI: lm(BMI ~ Age + Sex + Race,   data = unlabeled)\nNaive-WC: lm(WC  ~ Age + Sex + Race,   data = unlabeled)\nClassical: lm(DXDTOPF ~ Age + Sex + Race, data = labeled)\n\nFor the naive method, we treat BMI- and WC-based obesity (our ’predictions) as the true outcomes, fit on the unlabeled participants:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Naive on unlabeled using BMI\nnaive_bmi_fit &lt;- glm(obese_BMI ~ Age + Sex + Race, \n  family = binomial, data = unlabeled)\nnaive_bmi_df  &lt;- broom::tidy(naive_bmi_fit) |&gt;\n  mutate(method = \"Naive (BMI)\")\n\n# Naive on unlabeled using WC\nnaive_wc_fit &lt;- glm(obese_WC ~ Age + Sex + Race,\n  family = binomial, data = unlabeled)\nnaive_wc_df  &lt;- broom::tidy(naive_wc_fit) |&gt;\n  mutate(method = \"Naive (WC)\")\n\n\n\nInterpretation:\n\nBecause we ‘predicted’ obesity using BMI or WC, the “naive” coefficient are biased and the confidence intervals are artifically too narrow.\n\n\nFor the classical approach, we fit the true model on the labeled subset (with actual DXA % fat). That is:\n\n\nCode\n# Classical on labeled using DXA\nclass_fit &lt;- glm(obese_DXA ~ Age + Sex + Race,\n  family = binomial, data = labeled)\nclass_df  &lt;- broom::tidy(class_fit) |&gt;\n  mutate(method = \"Classical (DXA)\")\n\n# Combine\ncoef_df &lt;- bind_rows(naive_bmi_df, naive_wc_df, class_df) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(\n    term = recode(term,\n      \"Age20-39\" = \"Age 20-39 (vs Under 20)\",\n      \"Age40-59\" = \"Age 40-59 (vs Under 20)\",\n      \"Age60+\"   = \"Age 60+ (vs Under 20)\",\n      \"SexFemale\" = \"Female (vs Male)\",\n      \"RaceNon-Hispanic Black\" = \"Non-Hispanic Black (vs Non-Hispanic White)\",\n      \"RaceNon-Hispanic Asian\" = \"Non-Hispanic Asian (vs Non-Hispanic White)\",\n      \"RaceHispanic\" = \"Hispanic (vs Non-Hispanic White)\",\n      \"RaceOther Race - Including Multi-Racial\" = \"Other Race - Including Multi-Racial (vs Non-Hispanic White)\"\n    ),\n    method = factor(method, levels = c(\"Classical (DXA)\", \"Naive (BMI)\", \"Naive (WC)\"))\n  )\n    \n# Forest plot\nggplot(coef_df, aes(x = estimate, y = term, color = method)) +\n  geom_point(position = position_dodge(width = 0.6)) +\n  geom_errorbarh(aes(xmin = estimate - 1.96 * std.error,\n                     xmax = estimate + 1.96 * std.error),\n                 height = 0.2,\n                 position = position_dodge(width = 0.6)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(values  = c(\"#1B365D\", \"#00C1D5\", \"#AA4AC4\")) +\n  scale_color_manual(values = c(\"#1B365D\", \"#00C1D5\", \"#AA4AC4\")) +\n  labs(\n    x = \"Log-Odds Estimate (95% CI)\",\n    y = \"Covariate\",\n    title = \"Comparison of Obesity Model Coefficients\",\n    color = \"Model Type\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nNaive models can appear precise but are biased by proxy error.\nClassical DXA-only estimates are valid but use fewer observations.\nCompare sign/magnitude shifts across covariates to diagnose proxy bias.\n\n\n\n\n\n\nDiscussion Questions\n\nWhich coefficients are most sensitive to proxy choice?\nDo BMI and WC induce similar bias patterns?\n\n\nExercise 9: IPD Corrections\nWe now apply IPD to leverage all participants (labeled + unlabeled) while adjusting for prediction error. We run two IPD analyses, one using BMI as our ‘f’ and one using WC.\n\nA note on two calling styles:\nWe can either provide a single, combined dataset to the data argument and the name of the column that gives the set labels in label:\nipd_bmi_1 &lt;- ipd( formula = obese_DXA - obese_BMI ~ Age + Sex + Race, data = combined, label = “set_label”, model = “logistic”, method = “pspa” )\nor we can provide the labeled set to data and the unlabeled set to unlabeled_data separately:\nipd_bmi_2 &lt;- ipd( formula = obese_DXA - obese_BMI ~ Age + Sex + Race, data = labeled, unlabeled_data = unlabeled, model = “logistic”, method = “pspa” )\n\nNow we can try to run the IPD model:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Note: This code chunk results in an error, but we have an informative warning\n# We can try running it and see!\n\n# ipd_bmi_fit &lt;- ipd(\n#     formula = obese_DXA - obese_BMI ~ Age + Sex + Race,\n#     method  = \"pspa\",\n#     model   = \"logistic\",\n#     data    = combined,\n#     label   = \"set_label\"\n# )\n# \n# ipd_wc_fit &lt;- ipd(\n#     formula = obese_DXA - obese_WC ~ Age + Sex + Race,\n#     method  = \"pspa\",\n#     model   = \"logistic\",\n#     data    = labeled,\n#     unlabeled_data = unlabeled\n# )\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nThe warning is expected: labeled and unlabeled datasets must share factor levels for IPD model fitting.\n\n\n\n\n\nDiscussion Questions\n\nWhy do mismatched factor levels break IPD fitting?\nWhat preprocessing checks should you run before modeling?\n\nWe can see that the labeled subset does not include any \"60+\" observations, but IPD expects the same factor levels in both sets in order to fit the model using all the data. To fix this, let us refactor both the labeled and unlabeled data so they have consistent age categories:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Recode age\nNHANES2 &lt;- NHANES |&gt;\n  mutate(Age_recode = fct_collapse(Age, `40+` = c(\"40-59\", \"60+\")))\n      \n# Split NHANES into labeled (DXA available) vs unlabeled\nlabeled2   &lt;- NHANES2 |&gt; \n  filter(Cohort == \"2017-2018\") |&gt; \n  select(obese_DXA, obese_BMI, obese_WC, Age_recode, Sex, Race)\nunlabeled2 &lt;- NHANES2 |&gt; filter(Cohort == \"2021-2023\") |&gt; \n  select(obese_DXA, obese_BMI, obese_WC, Age_recode, Sex, Race)\n\n# Stack for IPD\ncombined2 &lt;- bind_rows(\n  labeled2   |&gt; mutate(set_label = \"labeled\"),\n  unlabeled2 |&gt; mutate(set_label = \"unlabeled\")\n)\n\n\n\n\n\n\n\nExercise 10: Run IPD and Compare Against Naive/Classical\nNow let us rerun ipd::ipd() and compare our results to the Naive and Classical models:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nCode\n# Note: This code chunk now runs without error!\nipd_bmi_fit &lt;- ipd(\n    formula = obese_DXA - obese_BMI ~ Age_recode + Sex + Race,\n    method  = \"pspa\",\n    model   = \"logistic\",\n    data    = combined2,\n    label   = \"set_label\"\n)\n\nipd_wc_fit &lt;- ipd(\n    formula = obese_DXA - obese_WC ~ Age_recode + Sex + Race,\n    method  = \"pspa\",\n    model   = \"logistic\",\n    data    = labeled2,\n    unlabeled_data = unlabeled2\n)\n\n# Collect results using the tidy() method\nipd_bmi_df &lt;- tidy(ipd_bmi_fit) |&gt;\n  mutate(method = \"IPD (BMI)\")\n\nipd_wc_df  &lt;- tidy(ipd_wc_fit) |&gt;\n  mutate(method = \"IPD (WC)\")\n\n\n\n\nCode\n# Rerun previous models\n# Naive on unlabeled using BMI\nnaive_bmi_fit &lt;- glm(obese_BMI ~ Age_recode + Sex + Race, \n  family = binomial, data = unlabeled2)\nnaive_bmi_df  &lt;- broom::tidy(naive_bmi_fit) |&gt;\n  mutate(method = \"Naive (BMI)\")\n\n# Naive on unlabeled using WC\nnaive_wc_fit &lt;- glm(obese_WC ~ Age_recode + Sex + Race,\n  family = binomial, data = unlabeled2)\nnaive_wc_df  &lt;- broom::tidy(naive_wc_fit) |&gt;\n  mutate(method = \"Naive (WC)\")\n\n# Classical on labeled using DXA\nclass_fit &lt;- glm(obese_DXA ~ Age_recode + Sex + Race,\n  family = binomial, data = labeled2)\nclass_df  &lt;- broom::tidy(class_fit) |&gt;\n  mutate(method = \"Classical (DXA)\")\n\n# Combine\ncoef_df &lt;- bind_rows(\n  ipd_bmi_df, ipd_wc_df, naive_bmi_df, naive_wc_df, class_df) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(\n    term = recode(term,\n      \"Age_recode20-39\" = \"Age 20-39 (vs Under 20)\",\n      \"Age_recode40+\" = \"Age 40+ (vs Under 20)\",\n      \"SexFemale\" = \"Female (vs Male)\",\n      \"RaceNon-Hispanic Black\" = \"Non-Hispanic Black (vs Non-Hispanic White)\",\n      \"RaceNon-Hispanic Asian\" = \"Non-Hispanic Asian (vs Non-Hispanic White)\",\n      \"RaceHispanic\" = \"Hispanic (vs Non-Hispanic White)\",\n      \"RaceOther Race - Including Multi-Racial\" = \"Other Race - Including Multi-Racial (vs Non-Hispanic White)\"\n    ),\n    method = factor(method, \n      levels = c(\"IPD (BMI)\", \"IPD (WC)\", \n                 \"Classical (DXA)\", \"Naive (BMI)\", \"Naive (WC)\"))\n  )\n    \n# Forest plot\nggplot(coef_df, aes(x = estimate, y = term, \n    color = method, fill = method, shape = method)) +\n  geom_point(position = position_dodge(width = 0.6)) +\n  geom_errorbarh(aes(xmin = estimate - 1.96 * std.error,\n                     xmax = estimate + 1.96 * std.error),\n                 height = 0.2,\n                 position = position_dodge(width = 0.6)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(\n    values = c(\n      \"Classical (DXA)\" = \"#1B365D\", \n      \"Naive (BMI)\"     = \"#00C1D5\", \n      \"Naive (WC)\"      = \"#AA4AC4\",\n      \"IPD (BMI)\"       = \"#00C1D5\",\n      \"IPD (WC)\"        = \"#AA4AC4\"\n    )\n  ) +\n  scale_color_manual(\n    values = c(\n      \"Classical (DXA)\" = \"#1B365D\", \n      \"Naive (BMI)\"     = \"#00C1D5\", \n      \"Naive (WC)\"      = \"#AA4AC4\",\n      \"IPD (BMI)\"       = \"#00C1D5\",\n      \"IPD (WC)\"        = \"#AA4AC4\"\n    )\n  ) +\n  scale_shape_manual(\n    values = c(\n      \"Classical (DXA)\" = 16, \n      \"Naive (BMI)\"     = 16, \n      \"Naive (WC)\"      = 16,\n      \"IPD (BMI)\"       = 17,\n      \"IPD (WC)\"        = 17\n    )\n  ) +\n  labs(\n    x = \"Log-Odds Estimate (95% CI)\",\n    y = \"Covariate\",\n    title = \"Comparison of Obesity Model Coefficients\",\n    color = \"Model Type\", fill = \"Model Type\", shape = \"Model Type\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nIPD combines both cohorts to correct proxy-induced bias while leveraging unlabeled sample size for efficiency.\n\n\n\n\nDiscussion Questions\n\nWhich covariates move closest to classical after IPD correction?\nDo IPD(BMI) and IPD(WC) differ meaningfully in uncertainty?",
    "crumbs": [
      "Modules",
      "Measuring Adiposity"
    ]
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#summary-and-key-takeaways",
    "href": "content/Unit02_MeasuringAdiposity.html#summary-and-key-takeaways",
    "title": "Measuring Adiposity",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nBMI and WC are imperfect proxies for DXA-based % body fat.\nNaive regression on BMI or WC leads to biased estimates for associations between obesity and risk factors such as age, sex, and race.\nClassical regression on true DXA-based % fat is unbiased but costly.\nIPD allows you to leverage a large cohort with only BMI + covariates, correct for prediction error, and recover unbiased estimates with more precision than the Classical approach.\nUnderstanding how different proxies like BMI and WC affect population-level inference is important for epidemiologic and clinical studies.",
    "crumbs": [
      "Modules",
      "Measuring Adiposity"
    ]
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#references",
    "href": "content/Unit02_MeasuringAdiposity.html#references",
    "title": "Measuring Adiposity",
    "section": "References",
    "text": "References\n\nVisokay, Adam, et al. “How to measure obesity in public health research? Problems with using BMI for population inference.” medRxiv (2025): 2025-04.\n\n\nThis is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org",
    "crumbs": [
      "Modules",
      "Measuring Adiposity"
    ]
  },
  {
    "objectID": "content/Unit04_Income.html",
    "href": "content/Unit04_Income.html",
    "title": "Income Inequality",
    "section": "",
    "text": "By the end of this module you will be able to:\n\nSet up labeled vs unlabeled splits when income is predicted but only some true incomes are observed.\nEstimate the OLS coefficient for age while controlling for sex.\nUse ipd() for:\n\npostpi_analytic, ppi, ppi_plusplus, pspa, chen\n\nCompare CI widths across labeled sample sizes.\nRun a power experiment to find the smallest labeled n achieving 80% power for a one-sided null.\n\n\n\n\n\n\n\nBackground + references (foldable)\n\n\n\n\n\nThe ipd package provides a unified ipd() interface for inference when outcomes are partially predicted (e.g., ML predictions available broadly, true labels only on a subset). This module mirrors a Folktables/Census income setup where predictions are generated via gradient boosting (e.g., XGBoost), but the inferential target is a downstream linear regression coefficient.",
    "crumbs": [
      "Supplemental Modules",
      "Income Inequality"
    ]
  },
  {
    "objectID": "content/Unit04_Income.html#learning-objectives",
    "href": "content/Unit04_Income.html#learning-objectives",
    "title": "Income Inequality",
    "section": "",
    "text": "By the end of this module you will be able to:\n\nSet up labeled vs unlabeled splits when income is predicted but only some true incomes are observed.\nEstimate the OLS coefficient for age while controlling for sex.\nUse ipd() for:\n\npostpi_analytic, ppi, ppi_plusplus, pspa, chen\n\nCompare CI widths across labeled sample sizes.\nRun a power experiment to find the smallest labeled n achieving 80% power for a one-sided null.\n\n\n\n\n\n\n\nBackground + references (foldable)\n\n\n\n\n\nThe ipd package provides a unified ipd() interface for inference when outcomes are partially predicted (e.g., ML predictions available broadly, true labels only on a subset). This module mirrors a Folktables/Census income setup where predictions are generated via gradient boosting (e.g., XGBoost), but the inferential target is a downstream linear regression coefficient.",
    "crumbs": [
      "Supplemental Modules",
      "Income Inequality"
    ]
  },
  {
    "objectID": "content/Unit04_Income.html#setup",
    "href": "content/Unit04_Income.html#setup",
    "title": "Income Inequality",
    "section": "1. Setup",
    "text": "1. Setup\n\nPackages\n\n\nCode\n# install.packages(\"ipd\")\nlibrary(ipd)\n\nlibrary(tidyverse)\n\n\n\n\nData: expected columns\nWe assume a dataset with:\n\nY: true yearly income (numeric), observed for labeled subset\nYhat: predicted income (numeric), observed for everyone\ncovariates: age (numeric) and sex (binary or factor)\n(optional) other covariates\n\n\n\n\n\n\n\nPlug in your real dataset\n\n\n\nNEED REAL DATA\n\n\n\n\nCode\nuse_simulated &lt;- TRUE\n\nif (!use_simulated) {\n  dat_full &lt;- read.csv(\"data/census_income_ca2019.csv\")\n  stopifnot(all(c(\"Y\",\"Yhat\",\"age\",\"sex\") %in% names(dat_full)))\n  dat_full &lt;- as_tibble(dat_full)\n} else {\n  set.seed(1)\n  n &lt;- 15000\n\n  # Simulate: income depends on age and sex\n  sex &lt;- rbinom(n, 1, 0.48)\n  age &lt;- pmin(pmax(round(rnorm(n, mean = 42, sd = 12)), 18), 80)\n\n  # True income model (linear signal + noise)\n  beta0 &lt;- 15000\n  beta_age &lt;- 900          # target coefficient ~ 900 dollars/year per year of age\n  beta_sex &lt;- 6000         # illustrative\n  eps &lt;- rnorm(n, sd = 20000)\n\n  Y &lt;- beta0 + beta_age * age + beta_sex * sex + eps\n\n  # Predicted income: biased/noisy proxy (as if from an ML model trained on prior year)\n  Yhat &lt;- Y + rnorm(n, sd = 12000) + 1500   # add some bias\n\n  dat_full &lt;- tibble(Y = Y, Yhat = Yhat, age = age, sex = sex)\n}\n\n# Build the downstream design variables.\ndat_full &lt;- dat_full %&gt;%\n  mutate(\n    sex = factor(sex, levels = c(0,1), labels = c(\"F\",\"M\"))\n  )",
    "crumbs": [
      "Supplemental Modules",
      "Income Inequality"
    ]
  },
  {
    "objectID": "content/Unit04_Income.html#labeled-vs-unlabeled-splits",
    "href": "content/Unit04_Income.html#labeled-vs-unlabeled-splits",
    "title": "Income Inequality",
    "section": "2. Labeled vs unlabeled splits",
    "text": "2. Labeled vs unlabeled splits\nWe create a stacked dataset where the unlabeled rows have Y = NA but retain Yhat and covariates.\n\n\nCode\nmake_split &lt;- function(dat, n_labeled, seed = NULL) {\n  if (!is.null(seed)) set.seed(seed)\n\n  idx &lt;- sample.int(nrow(dat), size = n_labeled, replace = FALSE)\n  labeled   &lt;- dat[idx, ] %&gt;% mutate(set_label = \"labeled\")\n  unlabeled &lt;- dat[-idx,] %&gt;% mutate(Y = NA_real_, set_label = \"unlabeled\")\n\n  bind_rows(labeled, unlabeled)\n}",
    "crumbs": [
      "Supplemental Modules",
      "Income Inequality"
    ]
  },
  {
    "objectID": "content/Unit04_Income.html#fit-many-methods-with-ipd",
    "href": "content/Unit04_Income.html#fit-many-methods-with-ipd",
    "title": "Income Inequality",
    "section": "3. Fit many methods with ipd()",
    "text": "3. Fit many methods with ipd()\nWe target the coefficient on age in:\nY ~ age + sex\nWith ipd, we write:\nY - Yhat ~ age + sex (where Yhat is the prediction) (rdrr.io)\n\n\nCode\nfit_ipd_methods &lt;- function(stacked_df, alpha = 0.05, n_t = Inf) {\n\n  methods &lt;- c(\"postpi_analytic\", \"postpi_boot\", \"ppi\", \"ppi_plusplus\", \"pspa\")\n\n  fits &lt;- map(methods, ~ ipd(\n    formula = Y - Yhat ~ age + sex,\n    method  = .x,\n    model   = \"linear\",\n    data    = stacked_df,\n    label   = \"set_label\",\n    alpha   = alpha,\n    n_t     = n_t,\n    alternative = \"two-sided\"\n  ))\n\n  names(fits) &lt;- methods\n  fits\n}\n\nfit_classical_labeled &lt;- function(stacked_df, alpha = 0.05) {\n  labeled &lt;- stacked_df %&gt;% filter(set_label == \"labeled\")\n  m &lt;- lm(Y ~ age + sex, data = labeled)\n\n  est &lt;- coef(m)[[\"age\"]]\n  se  &lt;- sqrt(vcov(m)[[\"age\",\"age\"]])\n  tcrit &lt;- qt(1 - alpha/2, df = df.residual(m))\n  ci_beta &lt;- c(est - tcrit*se, est + tcrit*se)\n\n  list(\n    coefficients = coef(m),\n    se = sqrt(diag(vcov(m))),\n    ci = rbind(age = ci_beta),\n    fit = m\n  )\n}\n\nfit_naive_imputation &lt;- function(stacked_df, alpha = 0.05) {\n  # naive: treat Yhat as if it were Y and run standard OLS\n  m &lt;- lm(Yhat ~ age + sex, data = stacked_df)\n\n  est &lt;- coef(m)[[\"age\"]]\n  se  &lt;- sqrt(vcov(m)[[\"age\",\"age\"]])\n  tcrit &lt;- qt(1 - alpha/2, df = df.residual(m))\n  ci_beta &lt;- c(est - tcrit*se, est + tcrit*se)\n\n  list(\n    coefficients = coef(m),\n    se = sqrt(diag(vcov(m))),\n    ci = rbind(age = ci_beta),\n    fit = m\n  )\n}\n\nextract_ci_for_term &lt;- function(fit_obj, term = \"age\") {\n  # ipd objects have $ci as matrix; baselines mimic that structure\n  ci &lt;- fit_obj$ci[term, ]\n  c(lower = ci[1], upper = ci[2])\n}",
    "crumbs": [
      "Supplemental Modules",
      "Income Inequality"
    ]
  },
  {
    "objectID": "content/Unit04_Income.html#ci-experiment-width-vs-labeled-sample-size",
    "href": "content/Unit04_Income.html#ci-experiment-width-vs-labeled-sample-size",
    "title": "Income Inequality",
    "section": "4. CI experiment: width vs labeled sample size",
    "text": "4. CI experiment: width vs labeled sample size\nThis mirrors your Python loop over ns and num_trials, storing lower, upper, width.\n\n\nCode\nalpha &lt;- 0.05\nns &lt;- as.integer(seq(100, 2000, length.out = 10))\nnum_trials &lt;- 100\n\n# Truth from the fully-labeled benchmark dataset (only for evaluation)\ntrue_theta &lt;- coef(lm(Y ~ age + sex, data = dat_full))[[\"age\"]]\n\nrun_one &lt;- function(n_labeled, trial) {\n  stacked &lt;- make_split(dat_full, n_labeled = n_labeled, seed = 10000 + 10*n_labeled + trial)\n\n  ipd_fits &lt;- fit_ipd_methods(stacked, alpha = alpha)\n  classical &lt;- fit_classical_labeled(stacked, alpha = alpha)\n  naive_imp &lt;- fit_naive_imputation(stacked, alpha = alpha)\n\n  rows &lt;- list(\n    tibble(method = names(ipd_fits)) %&gt;%\n      mutate(ci = map(ipd_fits, extract_ci_for_term)) %&gt;%\n      unnest_wider(ci),\n\n    tibble(method = \"classical\") %&gt;%\n      mutate(!!!extract_ci_for_term(classical)),\n\n    tibble(method = \"imputation\") %&gt;%\n      mutate(!!!extract_ci_for_term(naive_imp))\n  )\n\n  bind_rows(rows) %&gt;%\n    mutate(n = n_labeled, trial = trial, width = upper - lower)\n}\n\nres_ci &lt;- crossing(n = ns, trial = seq_len(num_trials)) %&gt;%\n  mutate(out = map2(n, trial, run_one)) %&gt;%\n  select(out) %&gt;%\n  unnest(out)\n\n\n\nPlot 1: example intervals (5 random trials)\n\n\nCode\nset.seed(123)\nexample_trials &lt;- sample(unique(res_ci$trial), size = 5)\n\nex &lt;- res_ci %&gt;%\n  filter(trial %in% example_trials, n == ns[2]) %&gt;%\n  mutate(method = factor(method, levels = c(\"classical\",\"imputation\",\"postpi_analytic\",\"postpi_boot\",\"ppi\",\"ppi_plusplus\",\"pspa\")))\n\nggplot(ex, aes(y = method, x = (lower + upper)/2, xmin = lower, xmax = upper)) +\n  geom_vline(xintercept = true_theta, linetype = 2) +\n  geom_errorbarh(height = 0.2) +\n  geom_point(size = 1.8) +\n  labs(\n    title = paste0(\"Age coefficient CIs (n_labeled = \", ns[2], \", 5 random trials)\"),\n    x = \"OLS coefficient on age (dollars/year per year of age)\",\n    y = NULL,\n    caption = \"Dashed line = benchmark OLS coefficient from the full labeled data.\"\n  ) +\n  theme_bw()\n\n\n\n\nPlot 2: average CI width vs n\n\n\nCode\navg_width &lt;- res_ci %&gt;%\n  group_by(method, n) %&gt;%\n  summarize(mean_width = mean(width, na.rm = TRUE), .groups = \"drop\")\n\nggplot(avg_width, aes(x = n, y = mean_width, group = method)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Average CI width vs labeled sample size\",\n    x = \"Number of labeled observations\",\n    y = \"Mean CI width (coefficient scale)\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nInterpretation checklist\n\n\n\n\nIf ppi / ppi_plusplus CIs are narrower than classical at the same n, you’re leveraging unlabeled predictions to reduce variance while staying valid.\nimputation often looks appealing but can be biased and anti-conservative (too narrow).\n:::\n\n\n5. Power experiment: smallest n for 80% power\nWe test the one-sided null:\n[ H_0: ^* &lt; 800]\nReject if the lower CI bound &gt; 800.\n\n\nCode\nalpha_pval &lt;- alpha\nnum_experiments &lt;- 100\nthreshold &lt;- 800\n\npower_at_n &lt;- function(n_labeled, method = c(\"ppi\",\"classical\")) {\n  method &lt;- match.arg(method)\n\n  rej &lt;- replicate(num_experiments, {\n    stacked &lt;- make_split(dat_full, n_labeled = n_labeled)\n\n    if (method == \"ppi\") {\n      fit &lt;- ipd(\n        Y - Yhat ~ age + sex,\n        method = \"ppi\",\n        model = \"linear\",\n        data = stacked,\n        label = \"set_label\",\n        alpha = alpha_pval,\n        alternative = \"one-sided\"\n      )\n      ci &lt;- extract_ci_for_term(fit, term = \"age\")\n      as.integer(ci[\"lower\"] &gt; threshold)\n    } else {\n      fit &lt;- fit_classical_labeled(stacked, alpha = alpha_pval)\n      ci &lt;- extract_ci_for_term(fit, term = \"age\")\n      as.integer(ci[\"lower\"] &gt; threshold)\n    }\n  })\n\n  mean(rej)\n}\n\nfind_n_for_power &lt;- function(method = c(\"ppi\",\"classical\"),\n                             n_min = 100, n_max = 2000, target = 0.80) {\n  method &lt;- match.arg(method)\n\n  f &lt;- function(n) power_at_n(as.integer(n), method = method) - target\n\n  grid &lt;- unique(round(seq(n_min, n_max, length.out = 20)))\n  vals &lt;- map_dbl(grid, f)\n\n  if (all(vals &lt; 0)) return(NA_integer_)\n  if (all(vals &gt; 0)) return(n_min)\n\n  i &lt;- which(vals &gt;= 0)[1]\n  lo &lt;- grid[max(1, i-1)]\n  hi &lt;- grid[i]\n\n  as.integer(uniroot(f, lower = lo, upper = hi)$root)\n}\n\nn_ppi &lt;- find_n_for_power(\"ppi\", n_min = 100, n_max = 2000)\nn_cls &lt;- find_n_for_power(\"classical\", n_min = 100, n_max = 2000)\n\ntibble(\n  method = c(\"ppi\",\"classical\"),\n  n_for_80_power = c(n_ppi, n_cls)\n)\n\n\n\n\n6. Exercises\n\n\n\n\n\n\nExercise 1 (core): change target coefficient\n\n\n\nChange the null threshold from 800 to 600 or 1000:\n\nHow does the required labeled n change?\nDoes PPI keep a consistent advantage over classical?\n:::\n\n\n\n\n\n\n\nExercise 2 (intermediate): add interactions\n\n\n\n\n\nFit:\nY - Yhat ~ age * sex\nInterpret the age effect for each sex and compare how much unlabeled data helps in each subgroup.\n\n\n\n\n\n\n\n\n\nExercise 3 (advanced): heteroskedastic-robust SEs\n\n\n\n\n\nModify the classical baseline to use robust (HC) SEs (e.g., sandwich::vcovHC) and compare with IPD methods. Discuss what changes you expect when prediction errors vary with age.",
    "crumbs": [
      "Supplemental Modules",
      "Income Inequality"
    ]
  },
  {
    "objectID": "content/Unit04_Income.html#power-experiment-smallest-n-for-80-power",
    "href": "content/Unit04_Income.html#power-experiment-smallest-n-for-80-power",
    "title": "Income Inequality",
    "section": "5. Power experiment: smallest n for 80% power",
    "text": "5. Power experiment: smallest n for 80% power\nWe test the one-sided null:\n[ H_0: ^* &lt; 800]\nReject if the lower CI bound &gt; 800.\n\n\nCode\nalpha_pval &lt;- alpha\nnum_experiments &lt;- 100\nthreshold &lt;- 800\n\npower_at_n &lt;- function(n_labeled, method = c(\"ppi\",\"classical\")) {\n  method &lt;- match.arg(method)\n\n  rej &lt;- replicate(num_experiments, {\n    stacked &lt;- make_split(dat_full, n_labeled = n_labeled)\n\n    if (method == \"ppi\") {\n      fit &lt;- ipd(\n        Y - Yhat ~ age + sex,\n        method = \"ppi\",\n        model = \"linear\",\n        data = stacked,\n        label = \"set_label\",\n        alpha = alpha_pval,\n        alternative = \"one-sided\"\n      )\n      ci &lt;- extract_ci_for_term(fit, term = \"age\")\n      as.integer(ci[\"lower\"] &gt; threshold)\n    } else {\n      fit &lt;- fit_classical_labeled(stacked, alpha = alpha_pval)\n      ci &lt;- extract_ci_for_term(fit, term = \"age\")\n      as.integer(ci[\"lower\"] &gt; threshold)\n    }\n  })\n\n  mean(rej)\n}\n\nfind_n_for_power &lt;- function(method = c(\"ppi\",\"classical\"),\n                             n_min = 100, n_max = 2000, target = 0.80) {\n  method &lt;- match.arg(method)\n\n  f &lt;- function(n) power_at_n(as.integer(n), method = method) - target\n\n  grid &lt;- unique(round(seq(n_min, n_max, length.out = 20)))\n  vals &lt;- map_dbl(grid, f)\n\n  if (all(vals &lt; 0)) return(NA_integer_)\n  if (all(vals &gt; 0)) return(n_min)\n\n  i &lt;- which(vals &gt;= 0)[1]\n  lo &lt;- grid[max(1, i-1)]\n  hi &lt;- grid[i]\n\n  as.integer(uniroot(f, lower = lo, upper = hi)$root)\n}\n\nn_ppi &lt;- find_n_for_power(\"ppi\", n_min = 100, n_max = 2000)\nn_cls &lt;- find_n_for_power(\"classical\", n_min = 100, n_max = 2000)\n\ntibble(\n  method = c(\"ppi\",\"classical\"),\n  n_for_80_power = c(n_ppi, n_cls)\n)",
    "crumbs": [
      "Supplemental Modules",
      "Income Inequality"
    ]
  },
  {
    "objectID": "content/Unit04_Income.html#exercises",
    "href": "content/Unit04_Income.html#exercises",
    "title": "Income Inequality",
    "section": "6. Exercises",
    "text": "6. Exercises\n\n\n\n\n\n\nExercise 1 (core): change target coefficient\n\n\n\nChange the null threshold from 800 to 600 or 1000:\n\nHow does the required labeled n change?\nDoes PPI keep a consistent advantage over classical?\n:::\n\n\n\n\n\n\n\nExercise 2 (intermediate): add interactions\n\n\n\n\n\nFit:\nY - Yhat ~ age * sex\nInterpret the age effect for each sex and compare how much unlabeled data helps in each subgroup.\n\n\n\n\n\n\n\n\n\nExercise 3 (advanced): heteroskedastic-robust SEs\n\n\n\n\n\nModify the classical baseline to use robust (HC) SEs (e.g., sandwich::vcovHC) and compare with IPD methods. Discuss what changes you expect when prediction errors vary with age.",
    "crumbs": [
      "Supplemental Modules",
      "Income Inequality"
    ]
  },
  {
    "objectID": "editing.html",
    "href": "editing.html",
    "title": "Editing",
    "section": "",
    "text": "Now that your website is alive and hosted, how do you start customizing it to have your content?\nIf you already know how to file pull requests, feel free to start making edits as you see fit using some of the structure points we’ve noted below.\nIf you are not familiar with pull requests, read this guide to get started."
  },
  {
    "objectID": "editing.html#title",
    "href": "editing.html#title",
    "title": "Editing",
    "section": "Title",
    "text": "Title\nTo change the title of the website, modify the name line of the _site.yml file."
  },
  {
    "objectID": "editing.html#modifying-pages",
    "href": "editing.html#modifying-pages",
    "title": "Editing",
    "section": "Modifying pages",
    "text": "Modifying pages\nPages are specified in the navigation bar by the lines that say -text: and href: .\nThe -text: specifies what the navigation bar will say for that tab.\nThe href: specifies which rendered Rmd file to use for that tab, it needs to be the html version of this file.\nThe tabs are specified to be aligned to the left (as specified by the left on line 5).\nThe tabs will appear in the order listed.\nYou can also add icons to these tabs using font awesome as is shown for the contact page on line 18.\n\n\n\n\n\n\n\n\n\nOther icon options include Bootstap glyphicons or ion icons. Note that not all icons will work because they are not all set up with the packages that make rendering the website possible, so this may require some trial and error. Here is an example of how you would use all of these icon options to add more:\n\n\n\n\n\n\n\n\n\nThis would result in a navigation bar with these icons:"
  },
  {
    "objectID": "hosting.html",
    "href": "hosting.html",
    "title": "Hosting",
    "section": "",
    "text": "To host your website on GitHub, you will need to go to settings and click on the pages tab.\nAgain to go to settings click on the far upper right corner:\n\n\n\n\n\n\n\n\n\nClick on Pages:\n\n\n\n\n\n\n\n\n\nSelect the main branch and the root directory and press save. Be sure to also check the “Enforce HTTPS” box. Afterwards your repository should look like this:\n\n\n\n\n\n\n\n\n\nNote that in general, your website will be published to a URL like this:\nhttps://username.github.io/repository_name/\nIf you have multiple websites published underneath your username or organization, this should still publish fine. This website will be a subdirectory that is named whatever you have named this repository.\nIf you’ve published this website underneath a GitHub organization (not your own personal GitHub profile), then in the above example URL the organization name will be where we’ve put username.\nFor more about GitHub pages (including how to personalize your URL) see the GitHub pages documentation here: https://pages.github.com/\nSometimes, GitHub page publishing will take a bit of time. You can click on the Actions tab in your repository and see if there is a pages and deployment action currently running (indicated by a yellow circle next to the action name). If this is the case, you will need to wait until this becomes a green check mark before your GitHub page will be published."
  },
  {
    "objectID": "hosting.html#hosting-setup",
    "href": "hosting.html#hosting-setup",
    "title": "Hosting",
    "section": "",
    "text": "To host your website on GitHub, you will need to go to settings and click on the pages tab.\nAgain to go to settings click on the far upper right corner:\n\n\n\n\n\n\n\n\n\nClick on Pages:\n\n\n\n\n\n\n\n\n\nSelect the main branch and the root directory and press save. Be sure to also check the “Enforce HTTPS” box. Afterwards your repository should look like this:\n\n\n\n\n\n\n\n\n\nNote that in general, your website will be published to a URL like this:\nhttps://username.github.io/repository_name/\nIf you have multiple websites published underneath your username or organization, this should still publish fine. This website will be a subdirectory that is named whatever you have named this repository.\nIf you’ve published this website underneath a GitHub organization (not your own personal GitHub profile), then in the above example URL the organization name will be where we’ve put username.\nFor more about GitHub pages (including how to personalize your URL) see the GitHub pages documentation here: https://pages.github.com/\nSometimes, GitHub page publishing will take a bit of time. You can click on the Actions tab in your repository and see if there is a pages and deployment action currently running (indicated by a yellow circle next to the action name). If this is the case, you will need to wait until this becomes a green check mark before your GitHub page will be published."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "Getting started\nCreate your repository by clicking on the Use this Template button at OTTR_Template_Website repository\n\nYou’ll need to make your repository public.\n\n\nSet your GH_PAT\nTo enable the GitHub actions, your repository needs to be setup in a specific way.\nFor OTTR GitHub actions to run, they need to have credentials through a personal access token.\n\nSet up your own personal access token following these instructions - but create a classic token. Keep this personal access token handy for the next step. When you get to the permissions page, check the box that says repo and select all that is underneath that header. No other permissions are necessary.\n\n\n\nClick here for more detailed instructions. The instructions for this step may change with updates to GitHub.\n\nFirst, go to your username settings, by clicking on your user icon (upper right corner) and scrolling down to settings.\n\n\n\n\n\n\n\n\n\nNext, scroll all the way down on the far right menu to “Developer Settings”.\n\n\n\n\n\n\n\n\n\nThen select “Personal Access Tokens” and “Tokens (classic)”\n\n\n\n\n\n\n\n\n\nThen click “Generate new token” and confirm that you want classic.\n\n\n\n\n\n\n\n\n\nFinally, add a name select all the repo scopes and scroll down to the green button to generate the token. Copy this somewhere safe to then paste into your repository settings.\n\n\n\n\n\n\n\n\n\n\n\nIn your new OTTR_Template_Website derived repository, go to Settings &gt; Secrets and variables &gt; Actions. Click New Repository Secret.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the window opened, name this new secret GH_PAT and paste the personal access token in the box below. (Note that the name GH_PAT is specific to how OTTR works and other secret names cannot be used and for OTTR to still work).\nClick the green button to add the secret.\n\n\n\n\n\n\n\n\n\n\n\nAllow GitHub Actions\nGo to the settings menu for your repository that you created from the template. This should be located at the top of GitHub on the right side.\nScroll down to the “Actions” button and click it, then click “General”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScroll down to the workflow permissions section and select “Read and write permissions”, then click “Allow GitHub actions to create and approve pull requests.\nFinally, click “save”.\n\n\n\n\n\n\n\n\n\n\n\nProtect branches\nAlthough this isn’t entirely required, its strongly recommended that you use these settings to protect your main branches.\nClick on settings in the far upper right corner:\n\n\n\n\n\n\n\n\n\nClick on branches:\n\n\n\n\n\n\n\n\n\nClick the add rule button.\n\n\n\n\n\n\n\n\n\nType “main” as the branch name pattern:\n\n\n\n\n\n\n\n\n\nClick on the following boxes to require pull requests before merging:\n\n\n\n\n\n\n\n\n\nNote that if you have admin privileges on this repository, you will likely still be able to override these branch protections so use caution when git pushing!"
  }
]