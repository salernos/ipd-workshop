[
  {
    "objectID": "style.html",
    "href": "style.html",
    "title": "Style",
    "section": "",
    "text": "To change the part of the navigation bar that says “OTTR Quarto”, modify the title within the _quarto.yml file."
  },
  {
    "objectID": "style.html#navigation-bar",
    "href": "style.html#navigation-bar",
    "title": "Style",
    "section": "",
    "text": "To change the part of the navigation bar that says “OTTR Quarto”, modify the title within the _quarto.yml file."
  },
  {
    "objectID": "style.html#overall-theme",
    "href": "style.html#overall-theme",
    "title": "Style",
    "section": "Overall theme",
    "text": "Overall theme\nTo change the color scheme/fonts of the website modify the theme in the _site.yml file (see here for options):"
  },
  {
    "objectID": "style.html#change-the-favicon",
    "href": "style.html#change-the-favicon",
    "title": "Style",
    "section": "Change the favicon",
    "text": "Change the favicon\nThe small image that shows up on the browser can also be changed.\nYou can make a small image to replace the existing one by going to https://favicon.io/favicon-converter/ and uploading an image that you would like.\nNext, simply replace the image called favicon.ico in the images directory within the resources directory with the image you just created and downloaded from the favicon converter website."
  },
  {
    "objectID": "style.html#additional-changes",
    "href": "style.html#additional-changes",
    "title": "Style",
    "section": "Additional changes",
    "text": "Additional changes\nTo make additional changes to the style, you can modify the styles.css file with css code. This website has great information about css code.\nAs an example if you wanted to change the color of the blue line to green you could change where it says lightblue to lightgreen in the styles.css file. You can also use a hex color code like those that can be found at this website, such as #00FF9E to get a specific shade.\n\n\n\n\n\n\n\n\n\nNote that if you change the css file with a new element that is not already defined like body then you would need to do it as done with the banner element. This was then added to the index.Rmd file by using:\n&lt;div class = \"banner\"&gt;\nBanner text!  \n&lt;/div&gt;\nAlso checkout the Quarto docs for more customization of the pages."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inference with Predicted Data (IPD) Workshop",
    "section": "",
    "text": "What do we do after we have machine learned everything?\n\nPresenters: Jesse Gronsbell1 Stephen Salerno2\nContributors (Alphabetical Order): Awan Afiaz3, David Cheng4, Jianhui Gao5, Jesse Gronsbell6, Kentaro Hoffman7, Jeff Leek8, Qiongshi Lu9, Tyler McCormick10, Jiacheng Miao11,\nAnna Neufeld12, Stephen Salerno13\nWorkshop Date: June 24, 2025\n\n\n\nArtificial intelligence and machine learning (AI/ML) have become essential tools in biomedical research, enabling large-scale analyses across diverse domains such as genomics, structural biology, and electronic health records-based research. Increasingly, researchers rely on model-generated predictions, rather than directly measured variables, as inputs for downstream statistical analyses. For example, predicted gene expression values or polygenic risk scores are often used in place of experimental assays, allowing researchers to expand cohort sizes and explore hypotheses when traditional data collection is infeasible, costly, or time-consuming.\nWhile this practice of “using predictions as data” holds promise for accelerating scientific discovery, it presents significant challenges for statistical inference. When predicted values are used in place of true variables, the resulting estimates of association can be biased and misleading if uncertainty in the prediction step is not properly accounted for.\n\n\n\nIn this workshop, we explore the consequences of inference on predicted data across several biomedical applications. Drawing from classical approaches to measurement error and recent developments in bias correction, we will present a suite of prediction-based inference methods that adjust for prediction-related uncertainty and improve inference validity and efficiency. We will also introduce ipd, a user-friendly R package that implements several of these correction methods through a unified interface. The package supports modular integration into existing workflows and includes tidy methods for model inspection and diagnostics.\nThis workshop covers four modules (time permitting), each illustrating IPD in R using the ipd package:\n\nUnit 00: Getting Started\n\nIntroduce IPD concepts and core ipd package functions\nSimulate data and explore the bias and variance of AI/ML predictions versus ‘real’ data\nFit naive and classical inference models and compare with IPD methods\n\n\n\n\nUnit 01: The Rashomon Quartet\n\nTrain multiple prediction models on the Rashomon Quartet training set\nCompare the performances of the upstream predictions on the Rashomon Quartet testing set\nRecover classical estimates using IPD and contrast with naive estimates\n\n\n\n\nUnit 02: Different Measures of Adiposity\n\nExplore the National Health and Nutrition Examination Survey (NHANES) pre- and post- COVID-19\nDefine obesity based body mass index, waist circumference, and gold-standard dual-energy X-ray absorptiometry\nDemonstrate how conclusions differ for naive, classical, and IPD logistic regression\n\n\n\n\nUnit 03: BCR-ABL Fusion in B-Cell Leukemia\n\nLearn gene expression classifiers for acute lymphoblastic leukemia (ALL) genetic subtypes\nHarmonize features across two studies (ALL and Golub) and predict BCR-ABL1 (Philadelphia chromosome) fusion status\nPerform IPD to estimate associations between fusion status and clinical risk factors\n\n\n\n\nThis 90-minute workshop uses a blended format of instruction and hands-on coding exercises. Participants should:\n\nFollow along in the virtual RStudio environment (see below).\nAttempt to complete brief exercises or run the solution code snippets in real time.\nEngage in Q&A at module boundaries to troubleshoot and discuss concepts.\n\n\n\n\n\nA computer with internet to access the RStudio Virtual Environment (see below).\nFamiliarity with base R and tidyverse syntax (e.g., dplyr, broom).\nBasic understanding of predictive (e.g., randomForest) and regression modeling (e.g., lm, glm).\nExposure to Bioconductor’s ExpressionSet, AnnotationDbi, and MLInterfaces is helpful for the last module.\n\n\n\n\n\nDatasets: nhanesA, ALL, golubEsets, AnnotationDbi, hgu95av2.db, hu6800.db\nData Manipulation and Visualization: broom, scales, janitor, GGally, patchwork, tidyverse\nPredictive Modeling: neuralnet, partykit, randomForest, ranger, mgcv, pROC, DALEX, MLInterfaces\nInference with Predicted Data: ipd\n\n\n\n\n\n\n\nActivity\nTime\n\n\n\n\nBrief Overview of the Problem\n15 m\n\n\nUnit 00: Getting Started\n15 m\n\n\nUnit 01: The Rashomon Quartet\n15 m\n\n\nUnit 02: Different Measures of Adiposity\n15 m\n\n\nUnit 03: BCR-ABL Fusion in B-Cell Leukemia\n15 m\n\n\nWrap-Up and Q&A\n15 m\n\n\n\n\n\n\nLearning Goals:\n\nUnderstand the limitations of using predicted data for inference.\nLearn how IPD methods adjust for bias and recover valid uncertainty estimates.\nGain practical skills with the ipd R package across simulated and real datasets.\n\nLearning Objectives: By the end of the workshop, participants will be able to:\n\nTrain and evaluate predictive models (LDA, neural nets, random forests) using R and Bioconductor workflows.\nExplore data with AI/ML-predicted outcomes and diagnose bias/variance in predictions.\nApply ipd::ipd() for continuous and binary outcomes to correct inference using predicted data.\nInterpret IPD outputs and visualize adjusted coefficient estimates with confidence intervals.\n\n\n\n\n\nThe companion website for this workshop is available at:\nhttps://salernos.github.io/ipdworkshop\nTo use the workshop image:\ndocker run -e PASSWORD=&lt;choose_a_password_for_rstudio&gt; -p 8787:8787 ghcr.io/salernos/ipdworkshop:latest\nOnce running, navigate to http://localhost:8787/ and then login with rstudio:yourchosenpassword."
  },
  {
    "objectID": "index.html#background-and-motivation",
    "href": "index.html#background-and-motivation",
    "title": "Inference with Predicted Data (IPD) Workshop",
    "section": "",
    "text": "Artificial intelligence and machine learning (AI/ML) have become essential tools in biomedical research, enabling large-scale analyses across diverse domains such as genomics, structural biology, and electronic health records-based research. Increasingly, researchers rely on model-generated predictions, rather than directly measured variables, as inputs for downstream statistical analyses. For example, predicted gene expression values or polygenic risk scores are often used in place of experimental assays, allowing researchers to expand cohort sizes and explore hypotheses when traditional data collection is infeasible, costly, or time-consuming.\nWhile this practice of “using predictions as data” holds promise for accelerating scientific discovery, it presents significant challenges for statistical inference. When predicted values are used in place of true variables, the resulting estimates of association can be biased and misleading if uncertainty in the prediction step is not properly accounted for."
  },
  {
    "objectID": "index.html#workshop-overview",
    "href": "index.html#workshop-overview",
    "title": "Inference with Predicted Data (IPD) Workshop",
    "section": "",
    "text": "In this workshop, we explore the consequences of inference on predicted data across several biomedical applications. Drawing from classical approaches to measurement error and recent developments in bias correction, we will present a suite of prediction-based inference methods that adjust for prediction-related uncertainty and improve inference validity and efficiency. We will also introduce ipd, a user-friendly R package that implements several of these correction methods through a unified interface. The package supports modular integration into existing workflows and includes tidy methods for model inspection and diagnostics.\nThis workshop covers four modules (time permitting), each illustrating IPD in R using the ipd package:\n\nUnit 00: Getting Started\n\nIntroduce IPD concepts and core ipd package functions\nSimulate data and explore the bias and variance of AI/ML predictions versus ‘real’ data\nFit naive and classical inference models and compare with IPD methods\n\n\n\n\nUnit 01: The Rashomon Quartet\n\nTrain multiple prediction models on the Rashomon Quartet training set\nCompare the performances of the upstream predictions on the Rashomon Quartet testing set\nRecover classical estimates using IPD and contrast with naive estimates\n\n\n\n\nUnit 02: Different Measures of Adiposity\n\nExplore the National Health and Nutrition Examination Survey (NHANES) pre- and post- COVID-19\nDefine obesity based body mass index, waist circumference, and gold-standard dual-energy X-ray absorptiometry\nDemonstrate how conclusions differ for naive, classical, and IPD logistic regression\n\n\n\n\nUnit 03: BCR-ABL Fusion in B-Cell Leukemia\n\nLearn gene expression classifiers for acute lymphoblastic leukemia (ALL) genetic subtypes\nHarmonize features across two studies (ALL and Golub) and predict BCR-ABL1 (Philadelphia chromosome) fusion status\nPerform IPD to estimate associations between fusion status and clinical risk factors\n\n\n\n\nThis 90-minute workshop uses a blended format of instruction and hands-on coding exercises. Participants should:\n\nFollow along in the virtual RStudio environment (see below).\nAttempt to complete brief exercises or run the solution code snippets in real time.\nEngage in Q&A at module boundaries to troubleshoot and discuss concepts.\n\n\n\n\n\nA computer with internet to access the RStudio Virtual Environment (see below).\nFamiliarity with base R and tidyverse syntax (e.g., dplyr, broom).\nBasic understanding of predictive (e.g., randomForest) and regression modeling (e.g., lm, glm).\nExposure to Bioconductor’s ExpressionSet, AnnotationDbi, and MLInterfaces is helpful for the last module.\n\n\n\n\n\nDatasets: nhanesA, ALL, golubEsets, AnnotationDbi, hgu95av2.db, hu6800.db\nData Manipulation and Visualization: broom, scales, janitor, GGally, patchwork, tidyverse\nPredictive Modeling: neuralnet, partykit, randomForest, ranger, mgcv, pROC, DALEX, MLInterfaces\nInference with Predicted Data: ipd\n\n\n\n\n\n\n\nActivity\nTime\n\n\n\n\nBrief Overview of the Problem\n15 m\n\n\nUnit 00: Getting Started\n15 m\n\n\nUnit 01: The Rashomon Quartet\n15 m\n\n\nUnit 02: Different Measures of Adiposity\n15 m\n\n\nUnit 03: BCR-ABL Fusion in B-Cell Leukemia\n15 m\n\n\nWrap-Up and Q&A\n15 m\n\n\n\n\n\n\nLearning Goals:\n\nUnderstand the limitations of using predicted data for inference.\nLearn how IPD methods adjust for bias and recover valid uncertainty estimates.\nGain practical skills with the ipd R package across simulated and real datasets.\n\nLearning Objectives: By the end of the workshop, participants will be able to:\n\nTrain and evaluate predictive models (LDA, neural nets, random forests) using R and Bioconductor workflows.\nExplore data with AI/ML-predicted outcomes and diagnose bias/variance in predictions.\nApply ipd::ipd() for continuous and binary outcomes to correct inference using predicted data.\nInterpret IPD outputs and visualize adjusted coefficient estimates with confidence intervals."
  },
  {
    "objectID": "index.html#workshop-environment",
    "href": "index.html#workshop-environment",
    "title": "Inference with Predicted Data (IPD) Workshop",
    "section": "",
    "text": "The companion website for this workshop is available at:\nhttps://salernos.github.io/ipdworkshop\nTo use the workshop image:\ndocker run -e PASSWORD=&lt;choose_a_password_for_rstudio&gt; -p 8787:8787 ghcr.io/salernos/ipdworkshop:latest\nOnce running, navigate to http://localhost:8787/ and then login with rstudio:yourchosenpassword."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Inference with Predicted Data (IPD) Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nj.gronsbell@utoronto.ca↩︎\nssalerno@fredhutch.org↩︎\naafiaz@uw.edu↩︎\ndcheng@mgh.harvard.edu↩︎\njianhui.gao@mail.utoronto.ca↩︎\nj.gronsbell@utoronto.ca↩︎\nkhoffm3@uw.edu↩︎\njtleek@fredhutch.org↩︎\nqlu@biostat.wisc.edu↩︎\ntylermc@uw.edu↩︎\njmiao24@wisc.edu↩︎\nacn2@williams.edu↩︎\nssalerno@fredhutch.org↩︎"
  },
  {
    "objectID": "git_actions.html",
    "href": "git_actions.html",
    "title": "Git Actions",
    "section": "",
    "text": "We have set up several checks for website content edits.\nWhen you create a pull request, you will see something like this if everything is successful. You can click on the preview of website here link to see a preview. Please note that some features may not be possible to see in the preview. For example, icons may only show up as a box."
  },
  {
    "objectID": "git_actions.html#rendering-action",
    "href": "git_actions.html#rendering-action",
    "title": "Git Actions",
    "section": "Rendering Action",
    "text": "Rendering Action\nIf the rendering action fails, you will see something like this:\n\n\n\n\n\n\n\n\n\nIf you click on where it says Details on the far right, you will be taken to more information about what may have gone wrong.\n\n\n\n\n\n\n\n\n\nFor example, we can see that an R object was not found in one of the files. You could identify which file by scrolling up."
  },
  {
    "objectID": "git_actions.html#spelling-and-style-action",
    "href": "git_actions.html#spelling-and-style-action",
    "title": "Git Actions",
    "section": "Spelling and Style Action",
    "text": "Spelling and Style Action\nYou may find that you have spelling errors if you get the following message from your pull request (PR):\n\n\n\n\n\n\n\n\n\nIf this happens, click the Download the errors here. link. This will take you to a table with words that the check thought were misspelled, as well as what file they occurred in and the lines in that file.\nAdd words that are not actually misspelled to the dictionary.txt file located in the resources directory. It’s a good idea to try to keep this in alphabetical order.\nFor words that are indeed misspelled, fix the errors and push your changes to your pull request.\nYou should then see that your pull request has a different message that tells you that you have no spelling errors."
  },
  {
    "objectID": "git_actions.html#url-check",
    "href": "git_actions.html#url-check",
    "title": "Git Actions",
    "section": "URL Check",
    "text": "URL Check\nYou may find that you have broken URLs. If so, you will get this message when you create a Pull Request.\n\n\n\n\n\n\n\n\n\nYou can click on the Download the errors here link to see a document with a list of broken URLs and what files they are located in, like this:\n\n\n\n\n\n\n\n\n\nHere we can see that there are two URLs that are broken in the git_actions.Rmd file."
  },
  {
    "objectID": "git_actions.html#completing-a-pull-request",
    "href": "git_actions.html#completing-a-pull-request",
    "title": "Git Actions",
    "section": "Completing a Pull Request",
    "text": "Completing a Pull Request\nOnce all the Git Action checks pass, you can merge your pull request to your main branch for your website.\n\n\n\n\n\n\n\n\n\nIf you are only working on this yourself without others to review your pull request, you can click the Merge without waiting for requirements to be met box, so that you can click the Merge pull request button."
  },
  {
    "objectID": "content/Unit03_GeneticData.html",
    "href": "content/Unit03_GeneticData.html",
    "title": "Unit 03: BCR-ABL Fusion in B-Cell Leukemia",
    "section": "",
    "text": "Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, but it exhibits marked genetic heterogeneity, with distinct chromosomal translocations defining molecular subtypes with divergent prognoses and therapeutic responses. In B-cell lineage ALL, the BCR-ABL1 fusion (“breakpoint cluster region”/“Abelson”, i.e., the “Philadelphia chromosome”) arises from a t(9;22) translocation and encodes a constitutively active tyrosine kinase. This fusion was historically associated with poor outcomes, until the advent of targeted therapies (e.g., imatinib) revolutionized treatment and survival.\n\n\n\n\nSource: https://www.cancer.gov/publications/dictionaries/cancer-terms/def/philadelphia-chromosome\n\n\n\nHigh-density microarray profiling measures expression of over 7,000 genes in leukemic blast cells, enabling a form of “molecular diagnostics” where supervised learning can:\n\nClassify fusion status or other genetic subtypes when cytogenetic assays (PCR, FISH) are unavailable.\nDiscover novel marker genes and pathways dysregulated by specific fusions.\nPredict therapeutic response and stratify risk using expression-derived scores.\n\nHowever, in many retrospective or multi-center cohorts, only a subset of patients undergo gold-standard fusion testing (e.g., RT-PCR for BCR-ABL1), leaving the majority “unlabeled.” A common workaround is to train a gene expression classifier on the small labeled subset and apply it to the larger unlabeled cohort. Yet, naive downstream analyses, treating predicted labels as ground truth, can yield biased effect estimates and understate uncertainty.\nInference with Predicted Data (IPD) offers a principled remedy: it combines a small labeled set with the predicted labels (e.g., with true fusion status) with a larger unlabeled set, adjusting both bias and variance. In this module, we will:\n\nDescribe the ALL and Golub Bioconductor datasets.\nSubset the ALL data to B-cell ALL and filter to the top 500 variable probes.\nHarmonize probes across platforms via gene symbols.\nSplit the ALL data and train three classifiers for BCR-ABL1 fusion status.\nPredict on the holdout ALL and Golub B-cell ALL data using the ALL-trained classifier.\nApply IPD to estimate the association between fusion status and patient sex assigned at birth.\n\nBy the end, you will understand how to leverage expression-based predictions while maintaining valid inference on genetic subtypes in cohorts with partially missing gold-standard labels.\n\nNote: For the data loading and prediction workflows, we will directly follow the Bioconductor MLInterfaces vignette by VJ Carey and P Atieno:\nhttps://www.bioconductor.org/packages/devel/bioc/vignettes/MLInterfaces/inst/doc/MLprac2_2.html"
  },
  {
    "objectID": "content/Unit03_GeneticData.html#background-motivation",
    "href": "content/Unit03_GeneticData.html#background-motivation",
    "title": "Unit 03: BCR-ABL Fusion in B-Cell Leukemia",
    "section": "",
    "text": "Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, but it exhibits marked genetic heterogeneity, with distinct chromosomal translocations defining molecular subtypes with divergent prognoses and therapeutic responses. In B-cell lineage ALL, the BCR-ABL1 fusion (“breakpoint cluster region”/“Abelson”, i.e., the “Philadelphia chromosome”) arises from a t(9;22) translocation and encodes a constitutively active tyrosine kinase. This fusion was historically associated with poor outcomes, until the advent of targeted therapies (e.g., imatinib) revolutionized treatment and survival.\n\n\n\n\nSource: https://www.cancer.gov/publications/dictionaries/cancer-terms/def/philadelphia-chromosome\n\n\n\nHigh-density microarray profiling measures expression of over 7,000 genes in leukemic blast cells, enabling a form of “molecular diagnostics” where supervised learning can:\n\nClassify fusion status or other genetic subtypes when cytogenetic assays (PCR, FISH) are unavailable.\nDiscover novel marker genes and pathways dysregulated by specific fusions.\nPredict therapeutic response and stratify risk using expression-derived scores.\n\nHowever, in many retrospective or multi-center cohorts, only a subset of patients undergo gold-standard fusion testing (e.g., RT-PCR for BCR-ABL1), leaving the majority “unlabeled.” A common workaround is to train a gene expression classifier on the small labeled subset and apply it to the larger unlabeled cohort. Yet, naive downstream analyses, treating predicted labels as ground truth, can yield biased effect estimates and understate uncertainty.\nInference with Predicted Data (IPD) offers a principled remedy: it combines a small labeled set with the predicted labels (e.g., with true fusion status) with a larger unlabeled set, adjusting both bias and variance. In this module, we will:\n\nDescribe the ALL and Golub Bioconductor datasets.\nSubset the ALL data to B-cell ALL and filter to the top 500 variable probes.\nHarmonize probes across platforms via gene symbols.\nSplit the ALL data and train three classifiers for BCR-ABL1 fusion status.\nPredict on the holdout ALL and Golub B-cell ALL data using the ALL-trained classifier.\nApply IPD to estimate the association between fusion status and patient sex assigned at birth.\n\nBy the end, you will understand how to leverage expression-based predictions while maintaining valid inference on genetic subtypes in cohorts with partially missing gold-standard labels.\n\nNote: For the data loading and prediction workflows, we will directly follow the Bioconductor MLInterfaces vignette by VJ Carey and P Atieno:\nhttps://www.bioconductor.org/packages/devel/bioc/vignettes/MLInterfaces/inst/doc/MLprac2_2.html"
  },
  {
    "objectID": "content/Unit03_GeneticData.html#datasets-overview",
    "href": "content/Unit03_GeneticData.html#datasets-overview",
    "title": "Unit 03: BCR-ABL Fusion in B-Cell Leukemia",
    "section": "Datasets Overview",
    "text": "Datasets Overview\nWe work with two public Bioconductor datasets:\n\nALL (ALL package): 128 samples of acute lymphoblastic leukemia (both B- and T-cell), profiled on the Affymetrix HGU95Av2 microarray. Phenotype data include BT (immunophenotype), mol.biol (molecular subtype including BCR-ABL1 or NEG), age, and sex.\nGolub_Merge (golubEsets package): 72 samples of leukemias (ALL vs. AML), profiled on Affymetrix HGU6800. Phenotype data include ALL.AML, T.B.cell (lineage), age, and sex.\n\nWe will train our classifiers on a subset of the ALL data, test the model on the holdout ALL data, and perform IPD on the holdout ALL and Golub data."
  },
  {
    "objectID": "content/Unit03_GeneticData.html#phenotype-reduction-and-feature-filtering",
    "href": "content/Unit03_GeneticData.html#phenotype-reduction-and-feature-filtering",
    "title": "Unit 03: BCR-ABL Fusion in B-Cell Leukemia",
    "section": "Phenotype Reduction and Feature Filtering",
    "text": "Phenotype Reduction and Feature Filtering\nWe first narrow to B-cell ALL samples and reduce dimensionality by keeping the 500 most variable probes.\n\n# Load the ALL data\ndata(ALL)\n\n# Subset ALL to B-cell lineage (BT codes starting with \"B\")\nbALL &lt;- ALL[, substr(pData(ALL)$BT, 1, 1) == \"B\"]\n\n# Keep only fusion-negative (\"NEG\") and fusion-positive (\"BCR/ABL\") samples\nfus &lt;- bALL[, bALL$mol.biol %in% c(\"NEG\", \"BCR/ABL\")]\n\n# Convert to factor with clear levels: NEG=0, BCR/ABL=1\nfus$mol.biol &lt;- factor(fus$mol.biol, levels = c(\"NEG\", \"BCR/ABL\"))\n\n# Compute median absolute deviation (MAD) for each probe\ndevs &lt;- apply(exprs(fus), 1, mad)\n\n# Select top 500 most variable probes by MAD ranking\ntop500 &lt;- order(devs, decreasing = TRUE)[1:500]\n\n# Subset ExpressionSet to top 500 probes\ndat_filt &lt;- fus[top500, ]\n\n# Confirm dimensions: 500 probes x n samples\ndim(exprs(dat_filt))\n\n[1] 500  79\n\n\n\nExplanation:\n\nmad() is robust to outliers and captures variability.\nFiltering saves time on downstream analysis without losing key signals.\nWe now have dat_filt, a filtered ALL dataset focused on B-cell lineage and the most informative probes."
  },
  {
    "objectID": "content/Unit03_GeneticData.html#cross-platform-probe-harmonization",
    "href": "content/Unit03_GeneticData.html#cross-platform-probe-harmonization",
    "title": "Unit 03: BCR-ABL Fusion in B-Cell Leukemia",
    "section": "Cross-Platform Probe Harmonization",
    "text": "Cross-Platform Probe Harmonization\nTo predict on the Golub data, which uses a different array, we map probes from each platform to gene symbols, then intersect to find a common gene set.\n\n# Load the Golub data\ndata(Golub_Merge)\n\n# Map HGU95Av2 probes (ALL) to gene symbols\ngmap_all &lt;- AnnotationDbi::select(\n  hgu95av2.db,\n  keys    = featureNames(dat_filt),\n  columns = c(\"PROBEID\",\"SYMBOL\"),\n  keytype = \"PROBEID\"\n)\n\n'select()' returned 1:many mapping between keys and columns\n\n# HGU6800 probes (Golub) to gene symbols\ngmap_golub &lt;- AnnotationDbi::select(\n  hu6800.db,\n  keys    = featureNames(Golub_Merge),\n  columns = c(\"PROBEID\",\"SYMBOL\"),\n  keytype = \"PROBEID\"\n)\n\n'select()' returned 1:many mapping between keys and columns\n\n# Identify common symbols\ncommon_sym &lt;- intersect(gmap_all$SYMBOL, gmap_golub$SYMBOL)\n\n# For each symbol, pick the first associated probe\nprobe_all &lt;- gmap_all |&gt; \n  filter(SYMBOL %in% common_sym) |&gt; \n  group_by(SYMBOL) |&gt; \n  dplyr::slice(1) |&gt; \n  drop_na()\n\nprobe_golub &lt;- gmap_golub |&gt; \n  filter(SYMBOL %in% common_sym) |&gt; \n  group_by(SYMBOL) |&gt; \n  dplyr::slice(1) |&gt; \n  drop_na()\n\n# Subset ExpressionSets to these probes\ndat_train  &lt;- dat_filt[probe_all$PROBEID, ]\neset_golub &lt;- Golub_Merge[probe_golub$PROBEID, ]\n\n# Further subset Golub to B-cell ALL samples\ngolub_bALL &lt;- eset_golub[, pData(eset_golub)$T.B.cell == \"B-cell\" &\n                           pData(eset_golub)$ALL.AML  == \"ALL\"]\n\nfeatureNames(dat_train) &lt;- probe_all$SYMBOL\nrownames(exprs(dat_train)) &lt;- probe_all$SYMBOL\n\nfeatureNames(golub_bALL) &lt;- probe_golub$SYMBOL\nrownames(exprs(golub_bALL)) &lt;- probe_golub$SYMBOL\n\n\n# Confirm feature dimensions\ndim(dat_train)    # features x ALL samples\n\nFeatures  Samples \n     354       79 \n\ndim(golub_bALL)   # features x Golub B-cell ALL samples\n\nFeatures  Samples \n     354       38 \n\n\n\ndat_train and golub_BALL now share the same gene-symbol feature set, ready for model training and transfer."
  },
  {
    "objectID": "content/Unit03_GeneticData.html#exploratory-data-analysis",
    "href": "content/Unit03_GeneticData.html#exploratory-data-analysis",
    "title": "Unit 03: BCR-ABL Fusion in B-Cell Leukemia",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWith our filtered training set (dat_train), we explore patterns using a heatmap and principal component analysis (PCA).\n\n# Set subtype colors for visualization\nfcol &lt;- ifelse(dat_train$mol.biol == \"NEG\", \"gray\", \"steelblue\")\n\nheatmap(exprs(dat_train), ColSideColors = fcol)\n\n\n\n\n\n\n\n\n\n# PCA on samples\nPCg &lt;- prcomp(t(exprs(dat_train)))\n\n# Scree plot\nplot(PCg)\n\n\n\n\n\n\n\n\n\n# Scatterplots of PCs\npairs(PCg$x[, 1:5], col = fcol, pch = 19)\n\n\n\n\n\n\n\n\n\n# Biplot of PC1 vs PC2\nbiplot(PCg) \n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe heatmap reveals distinct blocks of probes whose expression differs between BCR-ABL1 fusion-positive and fusion-negative samples.\nThe PCA scatter shows separation (or overlap) of samples by fusion status along the first two principal components, and to a lesser extent, the next three.\nIn the biplot, probes lying far from the origin along PC1 or PC2 represent genes with the largest loadings, i.e., the greatest contribution to those principal components. Samples positioned similarly share expression patterns in those genes.\nOverall, the modest separation in PCA indicates the need for supervised methods to pinpoint the genes most predictive of BCR-ABL1 status."
  },
  {
    "objectID": "content/Unit03_GeneticData.html#traintest-split-and-classifier-training",
    "href": "content/Unit03_GeneticData.html#traintest-split-and-classifier-training",
    "title": "Unit 03: BCR-ABL Fusion in B-Cell Leukemia",
    "section": "Train/Test Split and Classifier Training",
    "text": "Train/Test Split and Classifier Training\nWe split dat_train (ALL) into 60% training and 40% holdout, then train three ML models.\n\nset.seed(2025)                # for reproducibility\nn_all &lt;- ncol(dat_train)      # number of ALL samples\ntrain_idx &lt;- sample(n_all, size = floor(0.6 * n_all))\n# Training and holdout ExpressionSets\ntrain_eset &lt;- dat_train[, train_idx]\ntest_eset  &lt;- dat_train[, -train_idx]\n\nIn this section, we compare three supervised learning methods using the MLInterfaces framework:\n\nDiagonal Linear Discriminant Analysis (dldaI)\nNeural Network (nnetI)\nRandom Forest (randomForestI)\n\nWe will use the MLearn() function, which wraps each algorithm, providing a consistent interface to train, predict, and evaluate models on an ExpressionSet. After fitting, we will use confuMat() to display the confusion matrix, showing the true versus predicted class counts.\nWe train each model on the first 40 most variable probes from the filtered ExpressionSet (features 1:40 in train_eset).\n\nDiagonal Linear Discriminant Analysis (dldaI)\nDiagonal LDA assumes each feature is conditionally independent (covariance matrix is diagonal), which can improve stability in high-dimensional, low-sample settings. We use the dldaI interface to fit and evaluate this classifier.\n\n# Train Diagonal LDA on probes 1:40\ndlda_mod &lt;- MLearn(\n  mol.biol ~ ., # mol.biol ~ . specifies BCR/ABL1 fusion status as the outcome\n  train_eset,   # Filtered ExpressionSet\n  dldaI,        # dldaI is the diagonal LDA interface\n  1:40          # 1:40 selects the first 40 probes\n)\n\n[1] \"mol.biol\"\n\n# Print a summary of the fitted model\ndlda_mod\n\nMLInterfaces classification output container\nThe call was:\nMLearn(formula = mol.biol ~ ., data = train_eset, .method = dldaI, \n    trainInd = 1:40)\nPredicted outcome distribution for test set:\n\nBCR/ABL     NEG \n      3       4 \n\n# Display the confusion matrix (true vs predicted)\ncm_dlda &lt;- confuMat(dlda_mod)\ncm_dlda\n\n         predicted\ngiven     BCR/ABL NEG\n  BCR/ABL       3   1\n  NEG           0   3\n\n# Accuracy of Diagonal LDA\nacc_dlda &lt;- sum(diag(cm_dlda)) / sum(cm_dlda)\ncat(\"Accuracy:\", sprintf(\"%.2f%%\", acc_dlda * 100))\n\nAccuracy: 85.71%\n\n\n\n\nNeural Network (nnetI)\nA single hidden layer neural network can capture non-linear relationships. We use the nnetI interface with default parameters plus:\n\nsize = 5: number of hidden neurons\ndecay = 0.01: weight-decay regularization\nMaxNWts = 2000: maximum allowed number of weights\n\n\n# Neural Network (nnetI)\nnn_mod &lt;- MLearn(\n  mol.biol ~ ., \n  train_eset, \n  nnetI, \n  1:40, \n  size=5, \n  decay=0.01, \n  MaxNWts=2000\n)\n\n[1] \"mol.biol\"\n# weights:  1781\ninitial  value 31.210991 \niter  10 value 19.645950\niter  20 value 12.779243\niter  30 value 5.459003\niter  40 value 1.841191\niter  50 value 1.334631\niter  60 value 1.133200\niter  70 value 0.875814\niter  80 value 0.704137\niter  90 value 0.625722\niter 100 value 0.610470\nfinal  value 0.610470 \nstopped after 100 iterations\n\n# Show model summary\nnn_mod\n\nMLInterfaces classification output container\nThe call was:\nMLearn(formula = mol.biol ~ ., data = train_eset, .method = nnetI, \n    trainInd = 1:40, size = 5, decay = 0.01, MaxNWts = 2000)\nPredicted outcome distribution for test set:\n\nBCR/ABL     NEG \n      5       2 \nSummary of scores on test set (use testScores() method for details):\n[1] 0.2948988\n\n# Confusion matrix for Neural Net\ncm_nn &lt;- confuMat(nn_mod)\ncm_nn\n\n         predicted\ngiven     BCR/ABL NEG\n  BCR/ABL       4   0\n  NEG           1   2\n\n# Accuracy of Neural Net\nacc_nn &lt;- sum(diag(cm_nn)) / sum(cm_nn)\ncat(\"Accuracy:\", sprintf(\"%.2f%%\", acc_nn * 100))\n\nAccuracy: 85.71%\n\n\n\n\nRandom Forest (randomForestI)\nRandom forests build an ensemble of decision trees on bootstrapped samples and random feature subsets, offering robust performance with little tuning. We use the randomForestI to fit a random forest model:\n\n# Random Forest (randomForestI)\nrf_mod &lt;- MLearn(\n  mol.biol ~ ., \n  train_eset, \n  randomForestI, \n  1:40\n)\n\n[1] \"mol.biol\"\n\n# Show model summary\nrf_mod\n\nMLInterfaces classification output container\nThe call was:\nMLearn(formula = mol.biol ~ ., data = train_eset, .method = randomForestI, \n    trainInd = 1:40)\nPredicted outcome distribution for test set:\n\nBCR/ABL     NEG \n      4       3 \nSummary of scores on test set (use testScores() method for details):\nBCR/ABL     NEG \n  0.524   0.476 \n\n# Confusion matrix\ncm_rf &lt;- confuMat(rf_mod)\ncm_rf\n\n         predicted\ngiven     BCR/ABL NEG\n  BCR/ABL       4   0\n  NEG           0   3\n\n# Accuracy of Random Forest\nacc_rf &lt;- sum(diag(cm_rf)) / sum(cm_rf)\ncat(\"Accuracy:\", sprintf(\"%.2f%%\", acc_rf * 100))\n\nAccuracy: 100.00%\n\n\n\nInterpretation: Only the Random Forest achieves perfect separation in the validation samples. This highlights the challenge of discriminating BCR-ABL1 status based solely on the top variable probes and underscores why downstream IPD corrections (for regression inference) remain valuable."
  },
  {
    "objectID": "content/Unit03_GeneticData.html#predictions-on-all-test-and-golub",
    "href": "content/Unit03_GeneticData.html#predictions-on-all-test-and-golub",
    "title": "Unit 03: BCR-ABL Fusion in B-Cell Leukemia",
    "section": "Predictions on ALL Test and Golub",
    "text": "Predictions on ALL Test and Golub\nAfter fitting our classifiers on the training subset of ALL, we now generate predictions on both the ALL holdout test and the Golub B-cell ALL datasets.\n\n# Predict BCR-ABL1 status on ALL test holdout using random forest\nall_pred &lt;- MLInterfaces::predict.classifierOutput(rf_mod, test_eset)\n\n# Compare to true labels in test_eset\nconf_mat_test &lt;- table(\n  Truth    = test_eset$mol.biol,\n  Predicted= all_pred$testPredictions\n)\nconf_mat_test\n\n         Predicted\nTruth     BCR/ABL NEG\n  NEG           3  16\n  BCR/ABL      11   2\n\n\n\n# Predict BCR-ABL1 status on Golub B-cell ALL\ngolub_pred &lt;- MLInterfaces::predict.classifierOutput(rf_mod, golub_bALL)\n\n# Summarize prediction counts (no ground truth in Golub)\ntable(golub_pred$testPredictions)\n\n\nBCR/ABL     NEG \n     25      13 \n\n\n\nNote: We retain true labels in the ALL test set for later IPD; Golub remains unlabeled."
  },
  {
    "objectID": "content/Unit03_GeneticData.html#ipd-inference-on-sex-effect",
    "href": "content/Unit03_GeneticData.html#ipd-inference-on-sex-effect",
    "title": "Unit 03: BCR-ABL Fusion in B-Cell Leukemia",
    "section": "IPD Inference on Sex Effect",
    "text": "IPD Inference on Sex Effect\nWe will now use Inference with Predicted Data (IPD) to estimate the log-odds effect of sex (male vs female) on true BCR-ABL1 status, combining the holdout ALL data with true and predict fusion status (i.e., our labeled data) and the Golub data with the predicted fusion status (i.e., our unlabeled data)\n\n# Build labeled (ALL test) data frame\ndf_test &lt;- tibble(\n  sample = colnames(test_eset),\n  set    = \"labeled\",\n  true   = as.integer(test_eset$mol.biol == \"BCR/ABL\"),\n  pred   = as.integer(all_pred$testPredictions == \"BCR/ABL\"),\n  sex    = factor(pData(test_eset)$sex, levels = c(\"F\",\"M\"))\n)\n\n# Build unlabeled (Golub) data frame\ndf_golub &lt;- tibble(\n  sample = colnames(golub_bALL),\n  set    = \"unlabeled\",\n  true   = NA_real_,\n  pred   = as.integer(golub_pred$testPredictions == \"BCR/ABL\"),\n  sex    = factor(pData(golub_bALL)$Gender, levels = c(\"F\",\"M\"))\n)\n\n# Combine for IPD\nipd_df &lt;- bind_rows(df_test, df_golub) |&gt; drop_na(sex)\n\n\n# Classical regression\nclass_fit &lt;- glm(true ~ sex, \"binomial\", df_test)\nclass_df  &lt;- broom::tidy(class_fit) |&gt;\n  mutate(method = \"Classical\")\n\n# Naive regression\nnaive_fit &lt;- glm(pred ~ sex, \"binomial\", df_golub)\nnaive_df  &lt;- broom::tidy(naive_fit) |&gt;\n  mutate(method = \"Naive\")\n\n# IPD logistic regression\nipd_fit &lt;- ipd(\n  formula   = true - pred ~ sex,\n  method    = \"pspa\",\n  model     = \"logistic\",\n  data      = ipd_df,\n  label     = \"set\",\n)\nipd_df  &lt;- tidy(ipd_fit) |&gt;\n  mutate(method = \"IPD\")\n\n# Summarize coefficients\ncombined &lt;- bind_rows(class_df, naive_df, ipd_df) |&gt;\n  mutate(term = recode(term, \"sexM\" = \"Male (vs Female)\"))\n\n# Forest plot\nggplot(combined, aes(x = estimate, y = term, color = method)) +\n  geom_point(position = position_dodge(width = 0.6)) +\n  geom_errorbarh(aes(xmin = estimate - 1.96 * std.error,\n                     xmax = estimate + 1.96 * std.error),\n                 height = 0.2,\n                 position = position_dodge(width = 0.6)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(values  = c(\"#1B365D\", \"#00C1D5\", \"#AA4AC4\")) +\n  scale_color_manual(values = c(\"#1B365D\", \"#00C1D5\", \"#AA4AC4\")) +\n  labs(\n    x = \"Log-Odds Estimate (95% CI)\",\n    y = \"Covariate\",\n    title = \"Comparison of BCR-ABL1 Model Coefficients\",\n    color = \"Model Type\"\n  ) +\n  theme_minimal()\n\nWarning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\n\nInsight: By treating the ALL test set as labeled and Golub as unlabeled, IPD combines information across cohorts to correct bias and improve precision when estimating the association between BCR-ABL1 status and patient sex.\n\n\nThis is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org"
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html",
    "href": "content/Unit01_RashomonQuartet.html",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "",
    "text": "In “Performance is not enough: the story of Rashomon’s quartet” (Biecek et al., 2023), four different models (linear regression, decision trees, random forests, and neural networks) achieve near-identical predictive performance on the same dataset. However, their interpretations vary significantly. In this module, we will:\n\nFit each model on a training set.\nEvaluate their predictive performance on a testing set.\nDemonstrate how the naive, classical, and IPD-based inference results differ.\n\n\nNote: We will replicate the results of the Rashomon Quartet analysis following:\nhttps://github.com/MI2DataLab/rashomon-quartet"
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#overview",
    "href": "content/Unit01_RashomonQuartet.html#overview",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "",
    "text": "In “Performance is not enough: the story of Rashomon’s quartet” (Biecek et al., 2023), four different models (linear regression, decision trees, random forests, and neural networks) achieve near-identical predictive performance on the same dataset. However, their interpretations vary significantly. In this module, we will:\n\nFit each model on a training set.\nEvaluate their predictive performance on a testing set.\nDemonstrate how the naive, classical, and IPD-based inference results differ.\n\n\nNote: We will replicate the results of the Rashomon Quartet analysis following:\nhttps://github.com/MI2DataLab/rashomon-quartet"
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#background-on-the-rashomon-quartet",
    "href": "content/Unit01_RashomonQuartet.html#background-on-the-rashomon-quartet",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "Background on the Rashomon Quartet",
    "text": "Background on the Rashomon Quartet\nThe underlying datasets are synthetically generated to include three covariates, \\(\\boldsymbol{X} = (X_1, X_2, X_3)^\\top\\), so that\n\\[\n\\boldsymbol{X} = (X_1, X_2, X_3)^\\top \\sim \\mathcal{N}_3 \\left(\\boldsymbol{0}, \\Sigma\\right), \\quad \\Sigma_{ij} = \\begin{cases} 1 & i=j,\\\\ 0.9 & i \\neq j\\end{cases}\n\\] The outcomes, \\(Y\\), are then generated to depend only on \\(X_1\\) and, to a lesser extent, \\(X_2\\), given by\n\\[\nY = \\sin\\left(\\frac{3X_1 + X_2}{5}\\right) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}\\left(0, \\frac{1}{3}\\right).\n\\]\n\nNote: \\(X_3\\) does not appear in the true data generating mechanism, but is highly correlated with \\(X_1\\) and \\(X_2\\).\n\nTwo independent batches of 1,000 observations (a training set and a testing set) are generated under this model. We will fit the four candidate models on the 1,000-row training data, and evaluate them separately on the 1,000-row testing data. As we will show, all four achieve \\(R^2 \\approx 0.729\\) and \\(\\mathrm{RMSE} \\approx 0.354\\) on the testing set. However, their interpretations diverge:\n\nLinear regression captures a near-linear approximation of \\(\\sin\\left(\\tfrac{3X_{1} + X_{2}}{5}\\right)\\).\nDecision tree approximates the nonlinear relationship via a small set of splits.\nRandom forest produces a smooth ensemble derivative of the tree.\nNeural network learns a different nonlinear basis with hidden units.\n\n\n\n\nSource: https://github.com/MI2DataLab/rashomon-quartet\n\n\nIn addition to comparing interpretability across models, we will demonstrate how to perform valid downstream inference for \\(Y\\) on \\(\\boldsymbol{X}\\) when \\(Y\\) is only partially observed. We will:\n\nTrain each predictive model on the full 1,000-row training set.\nPredict on all 1,000 testing observations.\nRandomly split the 1,000 testing rows into two groups:\n\nLabeled subset (with true \\(Y\\) observed).\nUnlabeled subset (with only predicted \\(f\\)).\n\nIn the labeled subset, regress \\(Y\\) on \\(\\boldsymbol{X}\\) (“Classical”).\nIn the unlabeled subset, regress \\(f\\) on \\(\\boldsymbol{X}\\) (“Naive”).\nCombine the two subsets and use ipd::ipd() to correct bias and variance.\n\nThis mirrors realistic scenarios where the analytic dataset (here the testing set) has only some observations labeled, and we want to use model predictions for the rest while still making valid inference."
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#setup-and-data",
    "href": "content/Unit01_RashomonQuartet.html#setup-and-data",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "Setup and Data",
    "text": "Setup and Data\n\n# Install if needed:\n# install.packages(c(\"ipd\",\"DALEX\",\"partykit\",\"randomForest\",\"neuralnet\",\"GGally\",\"tidyverse\"))\n\nlibrary(ipd)           # Inference with Predicted Data\n\nWarning: package 'ipd' was built under R version 4.4.3\n\nlibrary(DALEX)         # Model Explanation\n\nWarning: package 'DALEX' was built under R version 4.4.3\n\n\nWelcome to DALEX (version: 2.5.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\n\nlibrary(partykit)      # Decision Trees\n\nWarning: package 'partykit' was built under R version 4.4.3\n\n\nLoading required package: grid\n\n\nLoading required package: libcoin\n\n\nWarning: package 'libcoin' was built under R version 4.4.3\n\n\nLoading required package: mvtnorm\n\n\nWarning: package 'mvtnorm' was built under R version 4.4.3\n\nlibrary(randomForest)  # Random Forests\n\nWarning: package 'randomForest' was built under R version 4.4.2\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nlibrary(neuralnet)     # Neural Networks\n\nWarning: package 'neuralnet' was built under R version 4.4.3\n\nlibrary(GGally)        # Pairwise Plots\n\nWarning: package 'GGally' was built under R version 4.4.3\n\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\nlibrary(tidyverse)     # Data Manipulation and Visualization\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'tibble' was built under R version 4.4.3\n\n\nWarning: package 'tidyr' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.2\n\n\nWarning: package 'stringr' was built under R version 4.4.3\n\n\nWarning: package 'forcats' was built under R version 4.4.3\n\n\nWarning: package 'lubridate' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ lubridate 1.9.4     ✔ tibble    3.3.0\n✔ purrr     1.1.0     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()  masks randomForest::combine()\n✖ dplyr::compute()  masks neuralnet::compute()\n✖ dplyr::explain()  masks DALEX::explain()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ ggplot2::margin() masks randomForest::margin()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nRead-In the Training and Testing Data\nWe have included two CSV files from the MI2DataLab/rashomon-quartet repository on GitHub. Note that these CSVs are semicolon-delimited.\n\n# Use read_delim to parse semicolons\ntrain &lt;- read_delim(\"data/rq_train.csv\", delim = \";\", col_types = cols(), show_col_types = FALSE)\ntest  &lt;- read_delim(\"data/rq_test.csv\",  delim = \";\", col_types = cols(), show_col_types = FALSE)\n\n# Show the first few rows of each\nglimpse(train)\n\nRows: 1,000\nColumns: 4\n$ y  &lt;dbl&gt; -0.47048454, 0.17177895, 0.07295012, -0.34301361, 0.47167402, 0.191…\n$ x1 &lt;dbl&gt; -0.51211395, 0.49038643, -0.89461065, 0.02725393, 0.44364997, -0.26…\n$ x2 &lt;dbl&gt; -0.34174653, 0.25511508, -0.28744899, -0.09277031, 0.52147200, 0.28…\n$ x3 &lt;dbl&gt; 0.255531783, -0.179610715, -0.310620340, -0.109603650, 0.920225141,…\n\nglimpse(test)\n\nRows: 1,000\nColumns: 4\n$ y  &lt;dbl&gt; -0.44547765, -0.67618374, -0.78822993, -0.55024530, -0.49205810, -0…\n$ x1 &lt;dbl&gt; -0.553685719, -0.965436826, -0.962515641, -0.871816734, -1.96565982…\n$ x2 &lt;dbl&gt; -0.74221752, -0.19959118, -1.25801758, -1.08469684, -1.43546177, -0…\n$ x3 &lt;dbl&gt; -1.15266476, -1.00944121, -1.34017902, -0.84322216, -1.29871031, -0…\n\n\n\nConfirm Dimensions and Column Names:\n\nEach dataset should have exactly 1,000 rows and four columns: y, x1, x2, x3.\nWe will use all 1,000 rows of train to fit each predictive model, and then randomly split the 1,000-row test for IPD."
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#predictive-model-training-on-the-training-set",
    "href": "content/Unit01_RashomonQuartet.html#predictive-model-training-on-the-training-set",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "Predictive Model Training on the Training Set",
    "text": "Predictive Model Training on the Training Set\nBelow, we fit four predictive models using the training set:\n\nDecision Tree via ctree() (max depth = 3, minsplit = 250).\nLinear Regression via lm().\nRandom Forest via randomForest() (100 trees).\nNeural Network via neuralnet() (two hidden layers: 8 units, 4 units; threshold = 0.05).\n\nWe then evaluate each model’s performance in the testing set using the DALEX package to extract the model’s \\(R^2\\) and \\(RMSE\\).\n\nset.seed(1568)\n\n# 1) Decision Tree (ctree)\nmodel_dt &lt;- ctree(\n  formula = y ~ x1 + x2 + x3,\n  data    = train,\n  control = ctree_control(maxdepth = 3, minsplit = 250)\n)\n\nexp_dt &lt;- DALEX::explain(\n  model   = model_dt,\n  data    = test |&gt; select(x1, x2, x3),\n  y       = test$y,\n  verbose = FALSE,\n  label   = \"Decision Tree\"\n)\n\nmp_dt &lt;- model_performance(exp_dt)\n\n# 2) Linear Regression\nmodel_lm &lt;- lm(y ~ x1 + x2 + x3, data = train)\n\nexp_lm &lt;- DALEX::explain(\n  model   = model_lm,\n  data    = test |&gt; select(x1, x2, x3),\n  y       = test$y,\n  verbose = FALSE,\n  label   = \"Linear Regression\"\n)\n\nmp_lm &lt;- model_performance(exp_lm)\n\n# 3) Random Forest\nmodel_rf &lt;- randomForest(\n  formula = y ~ x1 + x2 + x3,\n  data    = train,\n  ntree   = 100\n)\n\nexp_rf &lt;- DALEX::explain(\n  model   = model_rf,\n  data    = test |&gt; select(x1, x2, x3),\n  y       = test$y,\n  verbose = FALSE,\n  label   = \"Random Forest\"\n)\n\nmp_rf &lt;- model_performance(exp_rf)\n\n# 4) Neural Network\nmodel_nn &lt;- neuralnet(\n  formula      = y ~ x1 + x2 + x3,\n  data         = train,\n  hidden       = c(8, 4),\n  threshold    = 0.05,\n  linear.output = TRUE\n)\n\npredict_nn &lt;- function(object, newdata) {\n  as.numeric(predict(object, newdata))\n}\n\nexp_nn &lt;- DALEX::explain(\n  model            = model_nn,\n  data             = test |&gt; select(x1, x2, x3),\n  y                = test$y,\n  predict_function = predict_nn,\n  verbose          = FALSE,\n  label            = \"Neural Network\"\n)\n\nmp_nn &lt;- model_performance(exp_nn)\n\n# Save DALEX explainers for later reuse\nsave(exp_dt, exp_lm, exp_rf, exp_nn, file = \"models.RData\")\n\n# Compile performance metrics into one data frame\nmp_all &lt;- list(\n  \"Linear Regression\" = mp_lm,\n  \"Decision Tree\"     = mp_dt,\n  \"Random Forest\"     = mp_rf,\n  \"Neural Network\"    = mp_nn\n)\n\nR2_values   &lt;- sapply(mp_all, function(x) x$measures$r2)\nRMSE_values &lt;- sapply(mp_all, function(x) x$measures$rmse)\n\nperf_df &lt;- tibble(\n  Model = names(R2_values),\n  R2    = round(as.numeric(R2_values),   4),\n  RMSE  = round(as.numeric(RMSE_values), 4)\n)\n\nperf_df\n\n# A tibble: 4 × 3\n  Model                R2  RMSE\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 Linear Regression 0.729 0.354\n2 Decision Tree     0.729 0.354\n3 Random Forest     0.728 0.354\n4 Neural Network    0.730 0.352\n\n\n\nInterpretation: All four models (Linear Regression, Decision Tree, Random Forest, Neural Network) deliver essentially identical predictive performance on the testing set:\n\\[\nR^2 \\approx 0.729,\\quad \\mathrm{RMSE} \\approx 0.354.\n\\]"
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#inspecting-the-prediction-models",
    "href": "content/Unit01_RashomonQuartet.html#inspecting-the-prediction-models",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "Inspecting the Prediction Models",
    "text": "Inspecting the Prediction Models\nWe now take a quick look at each model’s internal structure or summary.\n\nNote:\n\nOften, in practice, we do not know the operating characteristics of the predictive model or have access to its training data.\nIPD methods are distinct in that they are agnostic to the source or form of these ‘black-box’ predictions\n\n\n\nDecision Tree Visualization\n\nplot(model_dt, main = \"Decision Tree (ctree) Structure\")\n\n\n\n\n\n\n\n\n\nThe tree has depth 3, with splits on \\(x_1, x_2, x_3\\) that approximate \\(\\sin\\left(\\tfrac{3X_{1} + X_{2}}{5}\\right)\\) piecewise.\n\n\n\nLinear Regression Summary\n\nsummary(model_lm)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.68045 -0.24213  0.01092  0.23409  1.26450 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.01268    0.01114  -1.138    0.255    \nx1           0.48481    0.03001  16.157  &lt; 2e-16 ***\nx2           0.14316    0.02966   4.826 1.61e-06 ***\nx3          -0.03113    0.02980  -1.045    0.296    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.352 on 996 degrees of freedom\nMultiple R-squared:  0.7268,    Adjusted R-squared:  0.726 \nF-statistic: 883.4 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe OLS coefficients show how the linear model approximates the underlying sine function. In particular, the slope on \\(x_1\\) will be roughly an average of this over the training distribution.\n\n\n\nRandom Forest Summary\n\nmodel_rf\n\n\nCall:\n randomForest(formula = y ~ x1 + x2 + x3, data = train, ntree = 100) \n               Type of random forest: regression\n                     Number of trees: 100\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 0.1193538\n                    % Var explained: 73.58\n\n\n\nWe see OOB MSE and variance explained on the training data. The forest builds many trees of depth &gt;3 and averages them to approximate the sine structure.\n\n\n\nNeural Network Topology\n\nplot(model_nn, rep = \"best\", main = \"Neural Network (8,4) Topology\")\n\n\n\n\n\n\n\n\n\nThis diagram shows two hidden layers (8 units \\(\\to\\) 4 units) with activation functions that capture nonlinearity. The network approximates \\(\\sin\\left(\\tfrac{3X_{1} + X_{2}}{5}\\right)\\) more flexibly than the decision tree or random forest."
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#variable-importance-dalex",
    "href": "content/Unit01_RashomonQuartet.html#variable-importance-dalex",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "Variable Importance (DALEX)",
    "text": "Variable Importance (DALEX)\nNext, we compute variable importance for each model via DALEX::model_parts(). We request type=\"difference\" so that importance is measured in terms of loss of performance (RMSE) when that variable is permuted.\n\nimp_dt &lt;- model_parts(exp_dt, N = NULL, B = 1, type = \"difference\")\nimp_lm &lt;- model_parts(exp_lm, N = NULL, B = 1, type = \"difference\")\nimp_rf &lt;- model_parts(exp_rf, N = NULL, B = 1, type = \"difference\")\nimp_nn &lt;- model_parts(exp_nn, N = NULL, B = 1, type = \"difference\")\n\nplot(imp_dt, imp_nn, imp_rf, imp_lm)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the ingredients package.\n  Please report the issue at\n  &lt;https://github.com/ModelOriented/ingredients/issues&gt;.\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nEach model ranks features \\((x_{1},x_{2},x_{3})\\) by their impact on predictive accuracy.\nBecause \\(\\mathbf{x}\\) are correlated, some models (e.g. tree, forest) may split differently than linear regression or neural network."
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#partial-dependence-plots-dalex",
    "href": "content/Unit01_RashomonQuartet.html#partial-dependence-plots-dalex",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "Partial-Dependence Plots (DALEX)",
    "text": "Partial-Dependence Plots (DALEX)\nWe now generate partial-dependence (PD) profiles for each feature under each model via DALEX::model_profile(). This shows how each model’s predicted \\(f\\) changes when we vary one feature at a time, averaging out the others.\n\npd_dt &lt;- model_profile(exp_dt, N = NULL)\npd_lm &lt;- model_profile(exp_lm, N = NULL)\npd_rf &lt;- model_profile(exp_rf, N = NULL)\npd_nn &lt;- model_profile(exp_nn, N = NULL)\n\nplot(pd_dt, pd_nn, pd_rf, pd_lm)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the ingredients package.\n  Please report the issue at\n  &lt;https://github.com/ModelOriented/ingredients/issues&gt;.\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nFor \\(x_{1}\\), the Linear Regression PD is a straight line.\nThe Decision Tree PD is piecewise-constant.\nThe Random Forest PD is smoother but still stepwise.\nThe Neural Network PD approximates the sine shape.\n\nBecause \\(x_{3}\\) does not appear in the generating formula, its PD should be nearly flat—though correlation can induce slight structure."
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#inference-with-predicted-data-ipd",
    "href": "content/Unit01_RashomonQuartet.html#inference-with-predicted-data-ipd",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "Inference with Predicted Data (IPD)",
    "text": "Inference with Predicted Data (IPD)\nWe now demonstrate how to perform valid inference in the testing set when only some test rows have true \\(Y\\). Specifically, we:\n\nRandomly split the 1,000 testing rows into:\n\nA labeled subset of size \\(n_l\\) (we choose 10%, but this can be adjusted).\nAn unlabeled subset of size \\(n_u = 1,000 - n_l\\).\n\nIn the labeled subset, regress \\(Y\\) on \\(X_1\\), \\(X_2\\), and \\(X_3\\) (Classical).\nIn the unlabeled subset, regress \\(f\\) on \\(X_1\\), \\(X_2\\), and \\(X_3\\) (Naive).\nUse both subsets in an IPD pipeline and run:\nipd_res &lt;- ipd(\n  formula        = y - f ~ x1 + x2 + x3,\n  data           = test_labeled,\n  unlabeled_data = test_unlabeled,\n  method         = \"pspa\",\n  model          = \"ols\"\n)\nto obtain bias-corrected estimates of \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\).\n\n\nNote: Instead of supplying one stacked dataset and the column name for the set labels, we can alternativel supply the labeled and unlabeled data separately.\n\n\nset.seed(12345)\n\n# Predict outcomes in the testing set\nipd_data &lt;- test |&gt;\n  mutate(\n    \n    preds_lm = c(predict(model_lm, newdata = test)),\n    preds_dt = c(predict(model_dt, newdata = test)),\n    preds_rf = c(predict(model_rf, newdata = test)),\n    preds_nn = c(predict(model_nn, newdata = test)))\n\n# Randomly split the testing set into labeled and unlabeled subsets\nfrac_labeled &lt;- 0.1\n\nn_test    &lt;- nrow(ipd_data)\nn_labeled &lt;- floor(frac_labeled * n_test)\n\nlabeled_idx &lt;- sample(seq_len(n_test), size = n_labeled)\n\ntest_labeled &lt;- ipd_data |&gt;\n  slice(labeled_idx) |&gt;\n  mutate(set_label = \"labeled\")\n  \ntest_unlabeled &lt;- ipd_data |&gt;\n  slice(-labeled_idx) |&gt;\n  mutate(set_label = \"unlabeled\")\n\n\n# Classical\nclass_fit &lt;- lm(y ~ x1 + x2 + x3, data = test_labeled)\nclass_df  &lt;- broom::tidy(class_fit) |&gt;\n  mutate(method = \"Classical\") \n  \n# Naive (Linear Model)\nnaive_fit_lm &lt;- lm(preds_lm ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_lm  &lt;- broom::tidy(naive_fit_lm) |&gt;\n  mutate(method = \"Naive (lm)\") \n\nWarning in summary.lm(x): essentially perfect fit: summary may be unreliable\n\n# Naive (Decision Tree)\nnaive_fit_dt &lt;- lm(preds_dt ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_dt  &lt;- broom::tidy(naive_fit_dt) |&gt;\n  mutate(method = \"Naive (dt)\") \n\n# Naive (Random Forest)\nnaive_fit_rf &lt;- lm(preds_rf ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_rf  &lt;- broom::tidy(naive_fit_rf) |&gt;\n  mutate(method = \"Naive (rf)\") \n\n# Naive (Neural Net)\nnaive_fit_nn &lt;- lm(preds_nn ~ x1 + x2 + x3, data = test_unlabeled)\nnaive_df_nn  &lt;- broom::tidy(naive_fit_nn) |&gt;\n  mutate(method = \"Naive (nn)\") \n\n# IPD (Linear Model)\nipd_fit_lm &lt;- ipd(y - preds_lm ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled, \n  method = \"pspa\", model = \"ols\")\nipd_df_lm  &lt;- tidy(ipd_fit_lm) |&gt;\n  mutate(method = \"IPD (lm)\") \n\n# IPD (Decision Tree)\nipd_fit_dt &lt;- ipd(y - preds_dt ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled,\n  method = \"pspa\", model = \"ols\")\nipd_df_dt  &lt;- tidy(ipd_fit_dt) |&gt;\n  mutate(method = \"IPD (dt)\") \n\n# IPD (Random Forest)\nipd_fit_rf &lt;- ipd(y - preds_rf ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled,\n  method = \"pspa\", model = \"ols\")\nipd_df_rf  &lt;- tidy(ipd_fit_rf) |&gt;\n  mutate(method = \"IPD (rf)\") \n\n# IPD (Neural Net)\nipd_fit_nn &lt;- ipd(y - preds_nn ~ x1 + x2 + x3, \n  data = test_labeled, unlabeled_data = test_unlabeled,\n  method = \"pspa\", model = \"ols\")\nipd_df_nn  &lt;- tidy(ipd_fit_nn) |&gt;\n  mutate(method = \"IPD (nn)\") \n\ncombined_df &lt;- bind_rows(class_df,\n  naive_df_lm, naive_df_dt, naive_df_rf, naive_df_nn,\n  ipd_df_lm,   ipd_df_dt,   ipd_df_rf,   ipd_df_nn) |&gt;\n  \n  mutate(\n    conf.low  = estimate - 1.96 * std.error,\n    conf.high = estimate + 1.96 * std.error\n  )\n\ncombined_df\n\n# A tibble: 36 × 8\n   term        estimate std.error statistic   p.value method  conf.low conf.high\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)  0.00872  3.68e- 2  2.37e- 1 8.13e-  1 Classi…  -0.0634   0.0808 \n 2 x1           0.643    1.09e- 1  5.91e+ 0 5.24e-  8 Classi…   0.430    0.856  \n 3 x2           0.0702   8.70e- 2  8.07e- 1 4.22e-  1 Classi…  -0.100    0.241  \n 4 x3          -0.0791   9.94e- 2 -7.96e- 1 4.28e-  1 Classi…  -0.274    0.116  \n 5 (Intercept) -0.0127   4.88e-18 -2.60e+15 0         Naive …  -0.0127  -0.0127 \n 6 x1           0.485    1.29e-17  3.77e+16 0         Naive …   0.485    0.485  \n 7 x2           0.143    1.31e-17  1.09e+16 0         Naive …   0.143    0.143  \n 8 x3          -0.0311   1.31e-17 -2.37e+15 0         Naive …  -0.0311  -0.0311 \n 9 (Intercept) -0.0183   6.76e- 3 -2.71e+ 0 6.89e-  3 Naive …  -0.0315  -0.00506\n10 x1           0.546    1.78e- 2  3.06e+ 1 1.57e-141 Naive …   0.511    0.581  \n# ℹ 26 more rows\n\n\n\nPlotting The Model Estimates\nWe now visualize, for each predictive model, the estimated coefficients and 95% confidence intervals from each inference method:\n\nNote: The vertical dashed line marks the estimated slope for the Classical method, which serves as our benchmark for comparison.\n\n\nref_lines &lt;- combined_df %&gt;% \n  filter(method == \"Classical\") %&gt;% \n  select(term, estimate)\n\nggplot(combined_df, aes(x = estimate, y = method, color = method)) +\n  geom_point(size = 2) +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  facet_wrap(~ term, ncol = 4, scales = \"free_x\") +\n  geom_vline(\n    data = ref_lines,\n    aes(xintercept = estimate),\n    linetype = \"dashed\",\n    color    = \"black\"\n  ) +\n  labs(\n    x = expression(hat(beta)),\n    y = \"\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nWarning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\n\nInterpretation of Results\n\nThe Naive estimates are biased due to the prediction error and has artificially narrow confidence intervals.\nThe Classical is our benchmark, as it is unbiased, but has wider CI’s\nThe IPD methods cover the Classical estimates and have wider CI’s to account for the prediction uncertainty."
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#data-distribution-train-vs-test-ggallyggpairs",
    "href": "content/Unit01_RashomonQuartet.html#data-distribution-train-vs-test-ggallyggpairs",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "Data Distribution: Train vs Test (GGally::ggpairs)",
    "text": "Data Distribution: Train vs Test (GGally::ggpairs)\nFinally, we visualize pairwise relationships among \\((x_{1},x_{2},x_{3},y)\\) for train vs test. This confirms that train and test were drawn from the same synthetic distribution.\n\nboth &lt;- bind_rows(\n  train |&gt; mutate(label = \"train\"),\n  test  |&gt; mutate(label = \"test\")\n)\n\nggpairs(\n  both,\n  columns = c(\"x1\", \"x2\", \"x3\", \"y\"),\n  mapping = aes(color = label, alpha = 0.3),\n  lower = list(\n    continuous = wrap(\"points\", size = 0.5, alpha = 0.3),\n    combo      = wrap(\"facethist\", bins = 20)\n  ),\n  diag = list(\n    continuous = wrap(\"densityDiag\", alpha = 0.4, bw = \"SJ\")\n  ),\n  upper = list(\n    continuous = wrap(\"cor\", size = 3, stars = FALSE)\n  )\n) +\n  theme_minimal() +\n  labs(\n    title    = \"Pairwise Distribution: Train vs Test (x1,x2,x3,y)\",\n    subtitle = \"Color = train (blue) vs test (pink)\"\n  )\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe diagonals show almost identical marginal distributions of \\(x_{1},x_{2},x_{3},y\\) in train vs test.\nThe off-diagonals confirm the covariance structure \\(\\Sigma\\) is consistent across both sets."
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#summary-and-takeaways",
    "href": "content/Unit01_RashomonQuartet.html#summary-and-takeaways",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "Summary and Takeaways",
    "text": "Summary and Takeaways\n\nPredictive Performance:\n\nLinear Regression, Decision Tree, Random Forest, and Neural Network each achieve nearly identical test set performance (\\(R^2 \\approx 0.729, \\mathrm{RMSE} \\approx 0.354\\)).\n\nModel Interpretability:\n\nVariable Importance and Partial-Dependence plots highlight how each model “tells a different story” about which features matter and how \\(y\\) responds to \\(x_{1},x_{2},x_{3}\\).\nThe Neural Network’s PD best tracks the true sine function; the tree’s PD is piecewise-constant; the forest’s PD is smoothed piecewise; the linear PD is a straight line.\n\nInference on Predicted Data (IPD):\n\nBy randomly splitting the test set into “labeled” and “unlabeled” halves, we simulate a real-world scenario where only some new observations have true \\(y\\).\nNaive regression of \\(f \\sim x_{1} + x_{2} + x_{3}\\) on the unlabeled set is biased due to prediction error.\nClassical regression of \\(y \\sim x_{1} + x_{2} + x_{3}\\) on the labeled set is unbiased but less efficient.\nIPD methods combine the labeled/unlabeled sets to correct bias and properly estimate variance.\n\nKey Lesson:\n\n“Performance is not enough.” Even models with identical \\(R^2\\) can lead to very different bias and variance in downstream inference on covariate effects. IPD provides a principled correction when using model predictions in place of true outcomes for some observations."
  },
  {
    "objectID": "content/Unit01_RashomonQuartet.html#references",
    "href": "content/Unit01_RashomonQuartet.html#references",
    "title": "Unit 01: The Rashomon Quartet",
    "section": "References",
    "text": "References\n\nBiecek, Przemysław, et al. “Performance is not enough: The story told by a Rashomon Quartet.” Journal of Computational and Graphical Statistics 33.3 (2024): 1118-1121.\nMI2DataLab. “The Rashomon Quartet.” GitHub repository (2023).\n\n\nThis is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Us",
    "section": "",
    "text": "If you have questions please contact:\n\nCarrie Wright (cwrigh60@jhu.edu)"
  },
  {
    "objectID": "content/Unit00_GettingStarted.html",
    "href": "content/Unit00_GettingStarted.html",
    "title": "Unit 00: Getting Started",
    "section": "",
    "text": "Welcome to this workshop on Inference with Predicted Data (IPD)! In this module, we will:\n\nIntroduce the ipd package and its main functions.\nDemonstrate how to generate synthetic data for different types of outcomes.\nFit and compare various IPD methods.\nInspect results using built-in tidy, glance, and augment methods.\n\nThroughout the workshop, exercises are provided with solutions for practice."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#overview",
    "href": "content/Unit00_GettingStarted.html#overview",
    "title": "Unit 00: Getting Started",
    "section": "",
    "text": "Welcome to this workshop on Inference with Predicted Data (IPD)! In this module, we will:\n\nIntroduce the ipd package and its main functions.\nDemonstrate how to generate synthetic data for different types of outcomes.\nFit and compare various IPD methods.\nInspect results using built-in tidy, glance, and augment methods.\n\nThroughout the workshop, exercises are provided with solutions for practice."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#background-and-motivation",
    "href": "content/Unit00_GettingStarted.html#background-and-motivation",
    "title": "Unit 00: Getting Started",
    "section": "Background and Motivation",
    "text": "Background and Motivation\nWhen an outcome, \\(Y\\), is costly or difficult to measure, it can be tempting to replace missing values with predictions, \\(f(\\boldsymbol{X})\\), from a machine learning model (e.g., a random forest or neural network) built on easier-to-measure features, \\(\\boldsymbol{X}\\). However, using \\(f\\) as if it were the true outcome in downstream analyses, e.g., in estimating a regression coefficient, \\(\\beta\\), for the association between \\(Y\\) and \\(\\boldsymbol{X}\\), leads to biased point estimates and underestimated uncertainty. Methods for Inference with Predicted Data (IPD) address this by leveraging a small subset of “labeled” data with true \\(Y\\) values to calibrate inference in a larger “unlabeled” dataset."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#the-ipd-framework",
    "href": "content/Unit00_GettingStarted.html#the-ipd-framework",
    "title": "Unit 00: Getting Started",
    "section": "The IPD Framework",
    "text": "The IPD Framework\nConsider data arising from three sets of observations:\n\nTraining Set: \\(\\{(X_j, Y_j)\\}_{j=1}^{n_t}\\), used to fit a predictive model, \\(f(\\cdot)\\).\nLabeled Set: \\(\\{(X_i, Y_i)\\}_{i=1}^{n_l}\\), smaller sample with true outcomes measured.\nUnlabeled Set: \\(\\{(X_i)\\}_{i=n_l +1}^{n_l + n_u}\\), only features available.\n\nAfter fitting \\(f\\) on the training set, we apply it to the labeled and unlabeled sets to obtain predictions \\(f_i = f(X_i)\\):\n\n\n\n\nOverview of setup for common ‘inference with predicted data’ problems\n\n\n\nEspecially for ‘good’ predictions, it is tempting to treat \\(f_i\\) as surrogate outcomes and use them to estimate quantities such as regression parameters, \\(\\beta\\). However, as we will see, this leads to invalid inference. By combining the predicted \\(f_i\\) with the observed \\(Y_i\\) in the labeled set, we can calibrate our estimates and standard errors to achieve valid inference."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#key-formulas",
    "href": "content/Unit00_GettingStarted.html#key-formulas",
    "title": "Unit 00: Getting Started",
    "section": "Key Formulas",
    "text": "Key Formulas\nConsider a simple linear regression model for the association between \\(Y\\) and \\(X\\). We discuss the following potential estimators, which we will later implement using simulated data.\n\nNaive Estimator\nUsing only the unlabeled predictions, the naive OLS estimator solves\n\\[\n\\hat\\gamma_{\\text{naive}} = \\arg\\min_\\gamma \\sum_{i\\in U} (f_i - X_i'\\gamma)^2.\n\\]\nWe are careful to write the coefficients for this model as \\(\\gamma\\), because they bear no necessary correspondence with \\(\\beta\\), except under the extremely restrictive scenario when \\(f\\) perfectly captures the true regression function.\n\n\nClassical Estimator\nInstead, a valid approach would be to use only the labeled data. This classical estimator solves\n\\[\n\\hat\\beta_{\\text{classical}} = \\arg\\min_\\beta \\sum_{i\\in L} (Y_i - X_i'\\beta)^2.\n\\] While this approach is valid, it has limited precision because \\(n_l\\) is small in practice and we do not utilize any potential information from the (often much larger) unlabeled data.\n\n\nIPD Estimators\nMany estimators tailored to inference with predicted data share a similar form, as given in Ji et al. (2025):\n\\[\n\\widehat\\beta_\\text{ipd} = \\arg\\min_\\beta \\frac{1}{n_l}\\sum_{i=1}^{n_l} \\ell(X_i, Y_i) - \\left[\\frac{1}{n_l}\\sum_{i=1}^{n_l} g(X_i, f_i) - \\frac{1}{n_l+n_u}\\sum_{i=n_l+1}^{n_l+n_u} g(X_i, f_i)\\right],\n\\]\nfor some loss function, \\(\\ell(\\cdot)\\), such as the squared error loss for linear regression, and some \\(g(\\cdot)\\), which they call the ‘imputed loss’. Here, the first term is exactly the classical estimator, which anchors these methods on a valid model, and the second term in the square brackets ‘augments’ the estimator with additional information from the predictions. This allows us to have an estimator that is provably unbiased and asymptotically at least as efficient as the classical estimator, which only uses a fraction of the data.\nThe Inference with Predicted Data (IPD) package implements several recent methods for IPD, such as Chen & Chen method of Gronsbell et al., the Prediction-Powered Inference (PPI) and PPI++ methods of Angelopoulos et al. (a) and Angelopoulos et al. (a), the Post-Prediction Inference (PostPI) method of Wang et al., and the Post-Prediction Adaptive Inference (PSPA) method of Miao et al. to conduct valid, efficient inference, even when a large proportion of outcomes are predicted.\nIn this first tutorial, we demonstrate how to:\n\nGenerate fully synthetic data with ipd::simdat().\nFit a simple linear prediction model (e.g., linear regression).\nApply ipd::ipd() to estimate the association, \\(\\beta\\), between \\(Y\\) and \\(X\\) using labeled and unlabeled data.\nCompare the naive, classical, and IPD estimates of \\(\\beta\\).\nVisualize these results."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#installation-and-setup",
    "href": "content/Unit00_GettingStarted.html#installation-and-setup",
    "title": "Unit 00: Getting Started",
    "section": "Installation and Setup",
    "text": "Installation and Setup\nFirst, insure you have the ipd package and some additional packages installed:\n\n# Install these packages if you have not already:\n# install.packages(c(\"ipd\", \"broom\", \"tidyverse\", \"patchwork\"))\n\nlibrary(ipd)\n\nWarning: package 'ipd' was built under R version 4.4.3\n\nlibrary(broom)\n\nWarning: package 'broom' was built under R version 4.4.3\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nWarning: package 'tibble' was built under R version 4.4.3\n\n\nWarning: package 'tidyr' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.2\n\n\nWarning: package 'stringr' was built under R version 4.4.3\n\n\nWarning: package 'forcats' was built under R version 4.4.3\n\n\nWarning: package 'lubridate' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\n\nThroughout the workshop, we will use reproducible seeds and tidyverse conventions."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#function-references",
    "href": "content/Unit00_GettingStarted.html#function-references",
    "title": "Unit 00: Getting Started",
    "section": "Function References",
    "text": "Function References\nBelow is a high-level summary of the core ipd functions.\n\nsimdat()\nGenerates synthetic datasets for various inferential models.\n\nsimdat(\n  n,        # Numeric vector of length 3: c(n_train, n_labeled, n_unlabeled)\n  effect,   # Numeric: true effect size for simulation\n  sigma_Y,  # Numeric: residual standard deviation\n  model,    # Character: one of \"mean\", \"quantile\", \"ols\", \"logistic\", \"poisson\"\n  ...       # Additional arguments\n)\n\nThis function returns a data.frame with columns:\n\nX1, X2, ...: covariates\nY: true outcome (for training, labeled, and unlabeled subsets)\nf: predictions from the model (for labeled and unlabeled subsets)\nset_label: character indicating “training”, “labeled”, or “unlabeled”.\n\n\n\nipd()\nFits IPD methods for downstream inference on predicted data.\n\nipd(\n  formula,      # A formula: e.g., Y - f ~ X1 + X2 + ...\n  method,       # Character: one of \"chen\", \"postpi_boot\", \"postpi_analytic\", \n                # \"ppi\", \"ppi_all\", \"ppi_plusplus\", \"pspa\"\n  model,        # Character: one of \"mean\", \"quantile\", \"ols\", \"logistic\", \n                # \"poisson\"\n  data,         # Data frame containing columns for formula and label\n  label,        # Character: name of the column with set labels (\"labeled\" and \n                # \"unlabeled\")\n  ...           # Additional arguments\n)\n\n\nSupported Methods\n\nchen: Chen and Chen estimator (Gronsbell et al., 2025)\npostpi_analytic: analytic post-prediction inference (Wang et al., 2020).\npostpi_boot: bootstrap post-prediction inference Wang et al., 2020).\nppi: prediction-powered inference (Angelopoulos et al., 2023)\nppi_plusplus: PPI++ (PPI with data-driven weighting; Angelopoulos et al., 2024)\nppi_a: PPI using all data (Gronsbell et al., 2025)\npspa: assumption-lean and data-adaptive post-prediction inference (Miao et al., 2024)\n\n\n\n\nTidy Methods\n\nprint() and summary(): display model summaries.\ntidy(): return a tibble of estimates and standard errors.\nglance(): return a one-row tibble of model-level metrics.\naugment(): return the original data with fitted values and residuals."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#simulating-data",
    "href": "content/Unit00_GettingStarted.html#simulating-data",
    "title": "Unit 00: Getting Started",
    "section": "Simulating Data",
    "text": "Simulating Data\nThe ipd::simdat() function makes it easy to generate:\n\nA training set (where you fit your prediction model),\nA labeled set (where you observe the true \\(Y\\)),\nAn unlabeled set (where \\(Y\\) is presumed missing but you compute predictions \\(f\\)).\n\nWe supply the sample sizes, n = c(n_train, n_label, n_unlabel), an effect size (effect), residual standard deviation (sigma_Y; i.e., how much random noise is in the data), and a model type (\"ols\", \"logistic\", etc.). In this tutorial, we focus on a continuous outcome generated from a linear regression model (\"ols\"). We can also optionally shift and scale the predictions (via the shift and scale arguments) to control how the predicted outcomes relate to their true underlying counterparts.\n\nExercise 1: Data Generation\nLet us generate a synthetic dataset for a linear model with:\n\n5,000 training observations\n500 labeled observations\n1,500 unlabeled observations\nEffect size = 1.5\nResidual SD = 3\nPredictions shifted by 1 and scaled by 2\n\n\nNote: Interactive exercises are provided below. Try writing your code in the first chunk; solutions are hidden in the second chunk.\n\n\nset.seed(123)\n\n# n_t = 5000, n_l = 500, n_u = 1500\nn &lt;- c(5000, 500, 1500)\n\n# Effect size = 1.5, noise sd = 3, model = \"ols\" (ordinary least squares)\n# We also shift the mean of the predictions by 1 and scale their values by 2\ndat &lt;- simdat(\n  n       = n,\n  effect  = 1.5,\n  sigma_Y = 3,\n  model   = \"ols\",\n  shift   = 1,\n  scale   = 2\n)\n\n\n# The resulting data.frame `dat` has columns:\n#  - X1, X2, X3, X4: Four simulated covariates (all numeric ~ N(0,1))\n#  - Y             : True outcome (available in unlabeled set for simulation)\n#  - f             : Predicted outcome (Generated internally in simdat)\n#  - set_label     : {\"training\", \"labeled\", \"unlabeled\"}\n\n# Quick look:\ndat |&gt; \n  group_by(set_label) |&gt; \n  summarize(n = n()) \n\n# A tibble: 3 × 2\n  set_label     n\n  &lt;chr&gt;     &lt;int&gt;\n1 labeled     500\n2 training   5000\n3 unlabeled  1500\n\n\nLet us inspect the first few rows of each subset:\n\n# Training set\ndat |&gt;\n  filter(set_label == \"training\") |&gt;\n  glimpse()\n\nRows: 5,000\nColumns: 7\n$ X1        &lt;dbl&gt; -0.56047565, -0.23017749, 1.55870831, 0.07050839, 0.12928774…\n$ X2        &lt;dbl&gt; -1.61803670, 0.37918115, 1.90225048, 0.60187427, 1.73234970,…\n$ X3        &lt;dbl&gt; -0.91006117, 0.28066267, -1.03567040, 0.27304874, 0.53779815…\n$ X4        &lt;dbl&gt; -1.119992047, -1.015819127, 1.258052722, -1.001231731, -0.40…\n$ Y         &lt;dbl&gt; 3.8625325, -1.6575634, 4.1872914, -3.3624963, 6.9978916, 1.5…\n$ f         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ set_label &lt;chr&gt; \"training\", \"training\", \"training\", \"training\", \"training\", …\n\n# Labeled set\ndat |&gt;\n  filter(set_label == \"labeled\") |&gt;\n  glimpse()\n\nRows: 500\nColumns: 7\n$ X1        &lt;dbl&gt; -0.4941739, 1.1275935, -1.1469495, 1.4810186, 0.9161912, 0.3…\n$ X2        &lt;dbl&gt; -0.15062996, 0.80094056, -1.18671785, 0.43063636, 0.21674709…\n$ X3        &lt;dbl&gt; 2.0279109, -1.4947497, -1.5729492, -0.3002123, -0.7643735, -…\n$ X4        &lt;dbl&gt; 0.53495620, 0.36182362, -1.89096604, -1.40631763, -0.4019282…\n$ Y         &lt;dbl&gt; 2.71822922, 1.72133689, 0.86081066, 3.77173123, -2.77191549,…\n$ f         &lt;dbl&gt; 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n$ set_label &lt;chr&gt; \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labe…\n\n# Unlabeled set\ndat |&gt;\n  filter(set_label == \"unlabeled\") |&gt;\n  glimpse()\n\nRows: 1,500\nColumns: 7\n$ X1        &lt;dbl&gt; -1.35723063, -1.29269781, -1.51720731, 0.85917603, -1.214617…\n$ X2        &lt;dbl&gt; 0.01014789, 1.56213812, 0.41284605, -1.18886219, 0.71454993,…\n$ X3        &lt;dbl&gt; -1.42521509, 1.73298966, 1.66085181, -0.85343610, 0.26905593…\n$ X4        &lt;dbl&gt; -1.1645365, -0.2522693, -1.3945975, 0.3959429, 0.5980741, 1.…\n$ Y         &lt;dbl&gt; -3.8296223, -3.0350282, -5.3736718, 2.6076634, -4.7813463, -…\n$ f         &lt;dbl&gt; -1.9014253, -0.1679210, -0.3030749, 0.1249168, -0.8998546, -…\n$ set_label &lt;chr&gt; \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabel…\n\n\n\nExplanation:\n\nRows where set_label == \"training\" form an internal training set. Here, Y is observed, but f is NA, as we learn the prediction rule in this set.\nRows where set_label == \"labeled\" also have both Y and f. In practice, f will be generated by your own prediction model; for simulation, simdat does so automatically.\nRows where set_label == \"unlabeled\" have Y for posterity (but in a real‐data scenario, you would not know Y); simdat still generates Y, but the IPD routines will not use these. The column f always contains ‘predicted’ values."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#fit-a-linear-prediction-model",
    "href": "content/Unit00_GettingStarted.html#fit-a-linear-prediction-model",
    "title": "Unit 00: Getting Started",
    "section": "Fit a Linear Prediction Model",
    "text": "Fit a Linear Prediction Model\nIn practice, we would take the training portion and fit an AI/ML model to predict \\(Y\\) from \\((X_1, X_2, X_3, X_4)\\). This is done automatically by the simdat function, but for demonstration, let us fit a linear prediction model on the training data:\n\n# 1) Subset training set\ndat_train &lt;- dat |&gt; \n  filter(set_label == \"training\")\n\n# 2) Fit a linear model: Y ~ X1 + X2 + X3 + X4\nlm_pred &lt;- lm(Y ~ X1 + X2 + X3 + X4, data = dat_train)\n\n# 3) Prepare a full-length vector of NA\ndat$f_pred &lt;- NA_real_\n\n# 4) Identify the rows to predict (all non–training rows)\nidx_analytic &lt;- dat$set_label != \"training\"\n\n# 5) Generate predictions just once on that subset (shifted and scaled to match)\npred_vals &lt;- (predict(lm_pred, newdata = dat[idx_analytic, ]) - 1) / 2 \n\n# 6) Insert them back into the full data frame\ndat$f_pred[idx_analytic] &lt;- pred_vals\n\n# 7) Verify: `f_pred` is equal to `f` for the labeled and unlabeled data\ndat |&gt; \n  select(set_label, Y, f, f_pred) |&gt; \n  filter(set_label != \"training\") |&gt;\n  glimpse()\n\nRows: 2,000\nColumns: 4\n$ set_label &lt;chr&gt; \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labe…\n$ Y         &lt;dbl&gt; 2.71822922, 1.72133689, 0.86081066, 3.77173123, -2.77191549,…\n$ f         &lt;dbl&gt; 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n$ f_pred    &lt;dbl&gt; 0.67556303, -0.13706321, -1.75579589, 0.84146158, 0.15512973…\n\n\n\nExplanation of arguments:\n\nlm(Y ~ X1 + X2 + X3 + X4, data = dat_train) fits an ordinary least squares (OLS) regression on the training subset.\npredict(lm_pred, newdata = .) generates a new f (stored as f_pred) for each row outside of the training set.\n\n\n\nNote: In real workflows, you might use random forests (ranger::ranger()), gradients (xgboost::xgboost()), or any other ML algorithm; the IPD methods only require that you supply a vector of predictions, f, in your data."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#create-labeled-and-unlabeled-datasets",
    "href": "content/Unit00_GettingStarted.html#create-labeled-and-unlabeled-datasets",
    "title": "Unit 00: Getting Started",
    "section": "Create ‘Labeled’ and ‘Unlabeled’ datasets",
    "text": "Create ‘Labeled’ and ‘Unlabeled’ datasets\nWe now split the data into two subsets:\n\nlabeled: those rows where we retain the true Y (to be used for final inference alongside their predictions).\nunlabeled: those rows where we hide the true Y (we pretend we do not observe them; ipd will still require the f + covariates).\n\n\ndat_ipd &lt;- dat |&gt;\n  filter(set_label != \"training\") |&gt;\n  # Keep only the columns needed for downstream IPD\n  select(set_label, Y, f, X1, X2, X3, X4) \n\n# Show counts:\ndat_ipd |&gt; \n  group_by(set_label) |&gt; \n  summarize(n = n())\n\n# A tibble: 2 × 2\n  set_label     n\n  &lt;chr&gt;     &lt;int&gt;\n1 labeled     500\n2 unlabeled  1500\n\n\n\nExplanation:\n\nAfter this step, dat_ipd has two groups:\n\nlabeled (500 rows where we observe both Y and f),\nunlabeled (1500 rows where we only ‘observe’ f)."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#comparison-of-the-true-vs-predicted-outcomes",
    "href": "content/Unit00_GettingStarted.html#comparison-of-the-true-vs-predicted-outcomes",
    "title": "Unit 00: Getting Started",
    "section": "Comparison of the True vs Predicted Outcomes",
    "text": "Comparison of the True vs Predicted Outcomes\nBefore modeling, it is helpful to see graphically how the predicted values, \\(f\\), compare to the true outcomes, \\(Y\\). We can visually assess the bias and variance of our predicted outcomes, \\(f\\), versus the true outcomes, \\(Y\\), in our analytic data by plotting:\n\nScatterplot of \\(Y\\) and \\(f\\) vs. \\(X_1\\)\n\nDensity plots of \\(Y\\) and \\(f\\)\n\n\n# Prepare data\ndat_visualize &lt;- dat_ipd |&gt; \n  select(X1, Y, f) |&gt;\n  pivot_longer(Y:f, names_to = \"Measure\", values_to = \"Value\") |&gt;\n  arrange(Measure)\n\n# Scatter + trend lines\nggplot(dat_visualize, aes(x = X1, y = Value, color = Measure)) +\n  theme_minimal() +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_manual(values = c(\"steelblue\", \"gray\")) +\n  labs(\n    x     = \"X1\",\n    y     = \"True Y or Predicted f\",\n    color = \"Measure\"\n  ) +\n  theme(legend.position = \"bottom\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Density plots\nggplot(dat_visualize, aes(x = Value, fill = Measure)) +\n  theme_minimal() +\n  geom_density(alpha = 0.4) +\n  scale_fill_manual(values = c(\"steelblue\", \"gray\")) +\n  labs(\n    x     = \"Value\",\n    y     = \"Density\",\n    fill  = \"Measure\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nInterpretation:\nIn the scatterplot, note that the predicted values \\(f\\) (in blue) lie more tightly along the fitted trend line than the true \\(Y\\) (in gray), indicating stronger correlation with \\(X_1\\).\nIn the density plot, you can see that the spread of \\(f\\) is narrower than that of \\(Y\\), illustrating that the predictive model has reduced variance (often due to “regression to the mean”)."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#some-baselines-naive-vs-classical-inference",
    "href": "content/Unit00_GettingStarted.html#some-baselines-naive-vs-classical-inference",
    "title": "Unit 00: Getting Started",
    "section": "Some Baselines: Naive vs Classical Inference",
    "text": "Some Baselines: Naive vs Classical Inference\nBefore applying IPD, let’s see what happens if we:\n\nRegress the unlabeled predicted f on X1 (the naive approach).\nRegress only the labeled true Y on X1 (the classical approach).\n\nWe will compare these to IPD‐corrected estimates.\n\nExercise 2: Naive vs. Classical Model Fitting\nUsing the labeled and unlabeled sets, fit two models:\n\nNaive OLS on the unlabeled set using lm() with f ~ X1.\nClassical OLS on the labeled set using lm() with Y ~ X1.\n\n\n# 1) Naive: treat f as if it were truth (only on unlabeled)\nnaive_model &lt;- lm(f ~ X1, data = filter(dat_ipd, set_label == \"unlabeled\"))\n\n# 2) Classical: regress true Y on X1, only on the labeled set\nclassical_model &lt;- lm(Y ~ X1, data = filter(dat_ipd, set_label == \"labeled\"))\n\nLet’s also extract the coefficient summaries using tidy and compare the results of the two approaches:\n\nnaive_df &lt;- tidy(naive_model) |&gt;\n  mutate(method = \"Naive\") |&gt;\n  filter(term == \"X1\") |&gt;\n  select(method, estimate, std.error) \n\nclassical_df &lt;- tidy(classical_model) |&gt;\n  mutate(method = \"Classical\") |&gt;\n  filter(term == \"X1\") |&gt;\n  select(method, estimate, std.error)\n\nbind_rows(naive_df, classical_df)\n\n# A tibble: 2 × 3\n  method    estimate std.error\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Naive        0.730    0.0145\n2 Classical    1.48     0.155 \n\n\n\nExpected behavior (interpretation):\n\nThe naive coefficient is attenuated, or biased toward zero, since the predictions are imperfect.\nThe classical coefficient is unbiased but has a larger standard error due to the smaller sample size."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#ipd-corrected-inference-with-ipdipd",
    "href": "content/Unit00_GettingStarted.html#ipd-corrected-inference-with-ipdipd",
    "title": "Unit 00: Getting Started",
    "section": "IPD: Corrected Inference with ipd::ipd()",
    "text": "IPD: Corrected Inference with ipd::ipd()\nThe single wrapper function ipd() implements multiple IPD methods (e.g., Chen & Chen, PostPI, PPI, PPI++, PSPA) for various inferential tasks (e.g., mean and quantile estimation, ols, logistic, and poisson regression).\n\nReminder: Basic usage of ipd():\n\nipd(\n  formula = Y - f ~ X1,     # The downstream inferential model\n  method  = \"pspa\"          # The IPD method to run \n  model   = \"ols\"           # The type of inferential model\n  data    = dat_ipd,        # A data.frame with columns:\n                            #   - set_label: {\"labeled\", \"unlabeled\"}\n                            #   - Y: true outcomes (for labeled data)\n                            #   - f: predicted outcomes\n                            #   - X covariates (here X1, X2, X3, X4)\n  label   = \"set_label\",    # Column name indicating \"labeled\"/\"unlabeled\"\n)\n\n\n\nExercise 3: IPD Model Fitting via the PSPA Estimator\nLet’s run one method, pspa, proposed by Miao et al., 2024. The PSPA estimator is an IPD method that combines information from:\n\nLabeled data (where the true outcome, \\(Y\\), and model predictions, \\(f\\), are available), and\n\nUnlabeled data (where only model predictions, \\(f\\), are available).\n\nRather than treating the predicted outcomes with importance as the true outcomes, the method estimates a data-driven weight, \\(\\hat{\\omega}\\), and applies it to the predicted outcome contributions:\n\\[\n\\hat{\\beta}_\\text{pspa} = \\hat{\\beta}_\\text{classical} - \\hat{\\omega}\\cdot (\\hat{\\gamma}_\\text{naive}^l - \\hat{\\gamma}_\\text{naive}^u),\n\\]\nwhere \\(\\hat{\\beta}_{\\rm classical}\\) is the estimate from the classical regression, \\(\\hat{\\gamma}_{\\rm naive}^l\\) is the estimate from the naive regression in the labeled data, \\(\\hat{\\gamma}_{\\rm naive}^u\\) is the estimate from the naive regression in the unlabeled data, and \\(\\hat{\\omega}\\) reflects the amount of additional information carried by the predictions. By adaptively weighting the unlabeled information, the PSPA estimator achieves greater precision than by using the labeled data alone, without sacrificing validity, even when the predictions are imperfect.\nLet’s call the method using the ipd() function and collect the estimate for the slope of X1 in a linear regression (model = \"ols\"):\n\nset.seed(123)\nipd_model &lt;- ipd(\n  formula = Y - f ~ X1,\n  data    = dat_ipd,\n  label   = \"set_label\",\n  method  = \"pspa\",\n  model   = \"ols\"\n)\n\nipd_model\n\n\nCall:\n Y - f ~ X1 \n\nCoefficients:\n(Intercept)          X1 \n  0.8801364   1.4324759 \n\n\nThe ipd_model is an S4 object with slots for things like the coefficient, se, ci, coefTable, fit, formula, data_l, data_u, method, model, and intercept. We can extract the coefficient table using ipd’s tidy helper and compare with the naive and classical methods:\n\n# Extract the coefficient estimates\nipd_df &lt;- tidy(ipd_model) |&gt;\n  mutate(method = \"IPD\") |&gt;\n  filter(term == \"X1\") |&gt;\n  select(method, estimate, std.error)\n\n# Combine with naive & classical:\ncompare_tab &lt;- bind_rows(naive_df, classical_df, ipd_df)\ncompare_tab\n\n# A tibble: 3 × 3\n  method    estimate std.error\n* &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 Naive        0.730    0.0145\n2 Classical    1.48     0.155 \n3 IPD          1.43     0.146 \n\n\n\n\nExercise 4: Visualizing Uncertainty\nLet’s plot the coefficient estimates and 95% CIs for each of the naive, classical, and IPD methods:\n\n# Forest plot of estimates and 95% confidence intervals\ncompare_tab |&gt;\n  mutate(\n    lower = estimate - 1.96 * std.error,\n    upper = estimate + 1.96 * std.error\n  ) |&gt;\n  ggplot(aes(x = estimate, y = method)) +\n    geom_point(size = 2) +\n    geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2) +\n    geom_vline(xintercept = 1.5, linetype = \"dashed\", color = \"red\") +\n    labs(\n      title = \"Comparison of slope estimates \\u00B1 1.96·SE\",\n      x = expression(hat(beta)[X1]),\n      y = \"\"\n    ) +\n    theme_minimal()\n\nWarning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThe dashed red line at 1.5 is the true data-generating effect for \\(X_1\\).\nCompare how far each method’s interval is from 1.5, and whether 1.5 lies inside each interval.\n‘Naive’ often severely underestimates (biased); ‘Classical’ is unbiased but wide; IPD methods cluster around 1.5 with better coverage than ‘naive,’ often similar to classical but sometimes narrower.\n\n\n\n\nExercise 5: Inspecting Results\nUse tidy(), glance(), and augment() on ipd_model. Compare the coefficient estimate and standard error for X1 with the naive fit.\n\ntidy(ipd_model)\n\n                   term  estimate std.error  conf.low conf.high\n(Intercept) (Intercept) 0.8801364 0.1470190 0.5919845  1.168288\nX1                   X1 1.4324759 0.1462767 1.1457788  1.719173\n\nglance(ipd_model)\n\n  method model include_intercept nobs_labeled nobs_unlabeled       call\n1   pspa   ols              TRUE          500           1500 Y - f ~ X1\n\naugment(ipd_model) |&gt; glimpse()\n\nRows: 1,500\nColumns: 9\n$ set_label &lt;chr&gt; \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabeled\", \"unlabel…\n$ Y         &lt;dbl&gt; -3.8296223, -3.0350282, -5.3736718, 2.6076634, -4.7813463, -…\n$ f         &lt;dbl&gt; -1.9014253, -0.1679210, -0.3030749, 0.1249168, -0.8998546, -…\n$ X1        &lt;dbl&gt; -1.35723063, -1.29269781, -1.51720731, 0.85917603, -1.214617…\n$ X2        &lt;dbl&gt; 0.01014789, 1.56213812, 0.41284605, -1.18886219, 0.71454993,…\n$ X3        &lt;dbl&gt; -1.42521509, 1.73298966, 1.66085181, -0.85343610, 0.26905593…\n$ X4        &lt;dbl&gt; -1.1645365, -0.2522693, -1.3945975, 0.3959429, 0.5980741, 1.…\n$ .fitted   &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;\n$ .resid    &lt;dbl[,1]&gt; &lt;matrix[26 x 1]&gt;\n\n# Compare with naive\nbroom::tidy(naive_model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -0.118    0.0146     -8.07 1.40e-15\n2 X1             0.730    0.0145     50.3  0"
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#summary-and-key-takeaways",
    "href": "content/Unit00_GettingStarted.html#summary-and-key-takeaways",
    "title": "Unit 00: Getting Started",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nNaive regression on predicted outcomes is biased (point estimates are pulled toward zero and SEs are artificially small).\nClassical regression on the labeled data alone is unbiased but inefficient when the labeled set is small.\nIPD methods (Chen & Chen, PPI, PPI++, PostPI, PSPA) strike a balance: they use predictions to effectively enlarge sample size but adjust for prediction error to avoid bias.\nEven with ‘simple’ linear prediction models, IPD corrections can drastically improve inference on downstream regression coefficients."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#further-exploration",
    "href": "content/Unit00_GettingStarted.html#further-exploration",
    "title": "Unit 00: Getting Started",
    "section": "Further Exploration",
    "text": "Further Exploration\n\nTry other methods such as PPI++ (\"ppi_plusplus\"). How do the results compare?\nRepeat the analysis for a logistic model by setting model = \"logistic\" in both simdat() and ipd().\n\nHappy coding! Feel free to modify and extend these exercises for your own data."
  },
  {
    "objectID": "content/Unit00_GettingStarted.html#references",
    "href": "content/Unit00_GettingStarted.html#references",
    "title": "Unit 00: Getting Started",
    "section": "References",
    "text": "References\n\nSalerno, Stephen, et al. “ipd: An R Package for Conducting Inference on Predicted Data.” Bioinformatics (2025): btaf055.\n\n\nThis is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org"
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html",
    "href": "content/Unit02_MeasuringAdiposity.html",
    "title": "Unit 02: Measuring Adiposity",
    "section": "",
    "text": "Body mass index (BMI) is the most commonly used anthropometric proxy for adiposity in epidemiologic studies and clinical settings. It is simple to calculate, weight in kilograms divided by height in meters squared, and has well-established cut-points for classifying patients as overweight or obese. However, BMI does not distinguish between fat mass and lean mass, nor does it capture fat distribution (visceral versus subcutaneous). As a result, BMI can both under- and over-estimate true adiposity in key subgroups. For example muscular individuals (e.g., athletes) may be misclassified as “obese,” while older adults with sarcopenia (low muscle mass) may fall below BMI thresholds despite having high percent body fat.\n\n\n\n\nSource: https://www.medrxiv.org/content/10.1101/2025.04.01.25325037v1.full.pdf\n\n\n\nWaist circumference (WC) is another simple measure of central adiposity that may better reflect visceral fat, a key driver of metabolic risk. Standard WC cut-points (e.g., ≥ 102 cm in men, ≥ 88 cm in women) identify individuals at increased cardiometabolic risk, even when their BMI is in the normal or overweight range. Yet WC also does not directly quantify total body fat and is influenced by body build, posture, and measurement error.\nDual-energy X-ray absorptiometry (DXA) provides a more accurate “gold-standard” measure of body composition by directly quantifying total and regional fat and lean mass. DXA-derived percent fat thresholds (e.g., &gt; 30% in men, &gt; 42% in women) have been validated against metabolic outcomes. Comparing BMI-based obesity (BMI ≥ 30 kg/m²) and WC-based obesity (WC ≥ 102 cm men, ≥ 88 cm women) with DXA-defined obesity reveals where and how often these common anthropometric proxies misclassify true adiposity.\nIt is worth noting that BMI is often defended as a valid measure for population-level inference, under the assumption that individual-level misclassification errors are harmless when estimating group-level trends. This justification appears frequently in epidemiology, where BMI is treated as a convenient stand-in for adiposity in regression models. But this reasoning masks a deeper issue central to the IPD framework: BMI is itself a prediction model, a crude one, but a model nonetheless. It encodes a deterministic function (weight divided by height squared) to approximate an unobserved latent quantity (body fat), and like any prediction model, it introduces systematic biases that may not vanish with aggregation. IPD adopts a prediction-agnostic perspective: whether the surrogate is a simple index like BMI or a high-dimensional neural network, the challenge is the same: how to draw valid statistical inference when the outcome is a model-based proxy. By treating BMI as a prediction rather than a ground truth, we clarify the role of uncertainty, calibration, and correction, and demonstrate how IPD methods can help recover valid estimates even when only such surrogates are available.\nIn this module, we will use data from the National Health and Nutrition Examination Survey (NHANES) to:\n\nLoad BMI, WC, and DXA-based measures of adiposity and their associated features\nDefine obesity by three standards:\n\nBMI (≥ 30 kg/m²)\nWaist circumference (men ≥ 102 cm; women ≥ 88 cm)\nDXA % body fat (&gt; 30% men; &gt; 42% women)\n\nVisualize misclassification rates overall and by age, sex, and race/ethnicity\nCalibrate both BMI and WC-based obesity measures to DXA using the ipd package\nDiscuss implications for research and practice\n\nBy the end, we may have a better understanding of the strengths and limitations of BMI and WC as proxies for adiposity, know how to assess their sensitivity and specificity versus DXA, and be equipped to correct for measurement error using IPD when only BMI/WC are available.\n\n\n\nThe National Health and Nutrition Examination Survey (NHANES) is a nationally representative program conducted by the U.S. Centers for Disease Control and Prevention that combines in-home interviews with standardized physical examinations and laboratory tests to assess the health and nutritional status of Americans. Initiated in the early 1960s and carried out continuously since 1999 in two-year cycles, NHANES informs public health policy, tracks trends in chronic conditions and risk factors, and support research on diet, disease, and health disparities. NHANES provides:\n\nDXA-measured percent body fat (DXDTOPF) for a subset of participants\nBMI (BMXBMI), derived from measured height and weight\nWaist circumference (BMXWAIST), a potential measure of central fat\n\nAs DXA scans are costly and time-consuming, many studies only record BMI and WC. When our scientific question involves true adiposity (e.g., its association with certain risk factors), but we only have BMI and WC in most participants, we can use ipd to correct our downstream inference on % body fat to account for the fact that these proxies are being used in place of the true DXA measurement.\nNote: DXA scans were collected through the 2017 - 2018 NHANES cycle but were suspended during the COVID-19 pandemic. As a result, subsequent waves, including the August 2021 - August 2023 data, do not include DXA measurements. In this tutorial, we will therefore:\n\nUse the 2017-2018 wave as our labeled data (which includes DXDTOPF).\n\nUse IPD to correct our inference when estimating associations between % body fat and other covariates in the absence of direct DXA measurements in the unlabeled August 2021 - August 2023 wave.\n\nThis approach lets us leverage historic DXA data to predict adiposity in more recent participants, enabling unbiased inference across pre- and post-pandemic cohorts."
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#background-and-motivation",
    "href": "content/Unit02_MeasuringAdiposity.html#background-and-motivation",
    "title": "Unit 02: Measuring Adiposity",
    "section": "",
    "text": "Body mass index (BMI) is the most commonly used anthropometric proxy for adiposity in epidemiologic studies and clinical settings. It is simple to calculate, weight in kilograms divided by height in meters squared, and has well-established cut-points for classifying patients as overweight or obese. However, BMI does not distinguish between fat mass and lean mass, nor does it capture fat distribution (visceral versus subcutaneous). As a result, BMI can both under- and over-estimate true adiposity in key subgroups. For example muscular individuals (e.g., athletes) may be misclassified as “obese,” while older adults with sarcopenia (low muscle mass) may fall below BMI thresholds despite having high percent body fat.\n\n\n\n\nSource: https://www.medrxiv.org/content/10.1101/2025.04.01.25325037v1.full.pdf\n\n\n\nWaist circumference (WC) is another simple measure of central adiposity that may better reflect visceral fat, a key driver of metabolic risk. Standard WC cut-points (e.g., ≥ 102 cm in men, ≥ 88 cm in women) identify individuals at increased cardiometabolic risk, even when their BMI is in the normal or overweight range. Yet WC also does not directly quantify total body fat and is influenced by body build, posture, and measurement error.\nDual-energy X-ray absorptiometry (DXA) provides a more accurate “gold-standard” measure of body composition by directly quantifying total and regional fat and lean mass. DXA-derived percent fat thresholds (e.g., &gt; 30% in men, &gt; 42% in women) have been validated against metabolic outcomes. Comparing BMI-based obesity (BMI ≥ 30 kg/m²) and WC-based obesity (WC ≥ 102 cm men, ≥ 88 cm women) with DXA-defined obesity reveals where and how often these common anthropometric proxies misclassify true adiposity.\nIt is worth noting that BMI is often defended as a valid measure for population-level inference, under the assumption that individual-level misclassification errors are harmless when estimating group-level trends. This justification appears frequently in epidemiology, where BMI is treated as a convenient stand-in for adiposity in regression models. But this reasoning masks a deeper issue central to the IPD framework: BMI is itself a prediction model, a crude one, but a model nonetheless. It encodes a deterministic function (weight divided by height squared) to approximate an unobserved latent quantity (body fat), and like any prediction model, it introduces systematic biases that may not vanish with aggregation. IPD adopts a prediction-agnostic perspective: whether the surrogate is a simple index like BMI or a high-dimensional neural network, the challenge is the same: how to draw valid statistical inference when the outcome is a model-based proxy. By treating BMI as a prediction rather than a ground truth, we clarify the role of uncertainty, calibration, and correction, and demonstrate how IPD methods can help recover valid estimates even when only such surrogates are available.\nIn this module, we will use data from the National Health and Nutrition Examination Survey (NHANES) to:\n\nLoad BMI, WC, and DXA-based measures of adiposity and their associated features\nDefine obesity by three standards:\n\nBMI (≥ 30 kg/m²)\nWaist circumference (men ≥ 102 cm; women ≥ 88 cm)\nDXA % body fat (&gt; 30% men; &gt; 42% women)\n\nVisualize misclassification rates overall and by age, sex, and race/ethnicity\nCalibrate both BMI and WC-based obesity measures to DXA using the ipd package\nDiscuss implications for research and practice\n\nBy the end, we may have a better understanding of the strengths and limitations of BMI and WC as proxies for adiposity, know how to assess their sensitivity and specificity versus DXA, and be equipped to correct for measurement error using IPD when only BMI/WC are available.\n\n\n\nThe National Health and Nutrition Examination Survey (NHANES) is a nationally representative program conducted by the U.S. Centers for Disease Control and Prevention that combines in-home interviews with standardized physical examinations and laboratory tests to assess the health and nutritional status of Americans. Initiated in the early 1960s and carried out continuously since 1999 in two-year cycles, NHANES informs public health policy, tracks trends in chronic conditions and risk factors, and support research on diet, disease, and health disparities. NHANES provides:\n\nDXA-measured percent body fat (DXDTOPF) for a subset of participants\nBMI (BMXBMI), derived from measured height and weight\nWaist circumference (BMXWAIST), a potential measure of central fat\n\nAs DXA scans are costly and time-consuming, many studies only record BMI and WC. When our scientific question involves true adiposity (e.g., its association with certain risk factors), but we only have BMI and WC in most participants, we can use ipd to correct our downstream inference on % body fat to account for the fact that these proxies are being used in place of the true DXA measurement.\nNote: DXA scans were collected through the 2017 - 2018 NHANES cycle but were suspended during the COVID-19 pandemic. As a result, subsequent waves, including the August 2021 - August 2023 data, do not include DXA measurements. In this tutorial, we will therefore:\n\nUse the 2017-2018 wave as our labeled data (which includes DXDTOPF).\n\nUse IPD to correct our inference when estimating associations between % body fat and other covariates in the absence of direct DXA measurements in the unlabeled August 2021 - August 2023 wave.\n\nThis approach lets us leverage historic DXA data to predict adiposity in more recent participants, enabling unbiased inference across pre- and post-pandemic cohorts."
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#data-preparation-and-exploration",
    "href": "content/Unit02_MeasuringAdiposity.html#data-preparation-and-exploration",
    "title": "Unit 02: Measuring Adiposity",
    "section": "Data Preparation and Exploration",
    "text": "Data Preparation and Exploration\nWe will now load a pre-compiled NHANES dataset, explore key variables (BMI, waist circumference, DXA % body fat) by cohort, and define obesity categories for later misclassification analyses and IPD.\n\nLoading the Pre-Compiled NHANES Data\nFor convenience, we prepared and saved a data file, data/NHANES.rda, which contains a tibble, NHANES, with the following columns:\n\nSEQN - Respondent ID\n\nCohort - Factor: “2017-2018” or “2021-2023”\n\nAge - Factor: “Under 20”, “20-39”, “40-59”, or “60+”\nSex - Factor: “Male” or “Female”\n\nRace - Factor: “Non-Hispanic White”, “Hispanic”, “Non-Hispanic Black”, “Non-Hispanic Asian”, or “Other Race - Including Multi-Racial”\nSmoking - Factor: “Never Smoker”, “Former Smoker”, or “Current Smoker”\nEducation - Factor: “Less than high school”, “High school graduate/GED or equivalent”, “Some college or AA degree”, “College graduate or above”, or “Refused/Unknown”\nBMXBMI - Body Mass Index (kg/m²)\n\nBMXWAIST - Waist Circumference (cm)\n\nDXDTOPF - DXA Percent Body Fat (only measured in 2017-2018; NA in 2021-2023)\nWTMEC4YR - Four-Year Adjusted Survey Weights\n\nNote: The the code used to produce this dataset is available in this repository at inst/NHANES_DATA.R.\n\n\nInstall and Load the Necessary Packages\n\n# Install these packages if you have not already:\n# install.packages(c(\"ipd\", \"broom\", \"scales\", \"tidyverse\"))\n\nlibrary(ipd)        # Inference with predicted data\n\nWarning: package 'ipd' was built under R version 4.4.3\n\nlibrary(broom)      # Convert model objects (lm, glm, ipd) into tidy data.frames\n\nWarning: package 'broom' was built under R version 4.4.3\n\nlibrary(scales)     # Formatting scales and labels for ggplot2\n\nWarning: package 'scales' was built under R version 4.4.3\n\nlibrary(tidyverse)  # Meta‐package for data manipulation and visualization\n\nWarning: package 'tidyverse' was built under R version 4.4.2\n\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nWarning: package 'tibble' was built under R version 4.4.3\n\n\nWarning: package 'tidyr' was built under R version 4.4.2\n\n\nWarning: package 'readr' was built under R version 4.4.2\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.2\n\n\nWarning: package 'stringr' was built under R version 4.4.3\n\n\nWarning: package 'forcats' was built under R version 4.4.3\n\n\nWarning: package 'lubridate' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nLoad the Data\nWe first load the prepared dataset and take a look at its features:\n\n# Load the dataset\nload(\"data/NHANES.RData\")\n\n# Inspect its structure\nglimpse(NHANES)\n\nRows: 11,782\nColumns: 11\n$ SEQN      &lt;dbl&gt; 93706, 93707, 93711, 93712, 93714, 93717, 93719, 93725, 9372…\n$ Cohort    &lt;fct&gt; 2017-2018, 2017-2018, 2017-2018, 2017-2018, 2017-2018, 2017-…\n$ Age       &lt;fct&gt; Under 20, Under 20, 40-59, Under 20, 40-59, 20-39, Under 20,…\n$ Sex       &lt;fct&gt; Male, Male, Male, Male, Female, Male, Female, Female, Male, …\n$ Race      &lt;fct&gt; Non-Hispanic Asian, Other Race - Including Multi-Racial, Non…\n$ Smoking   &lt;fct&gt; NA, NA, NA, NA, Former Smoker, NA, NA, NA, NA, NA, NA, NA, N…\n$ Education &lt;fct&gt; Refused/Unknown, Refused/Unknown, College graduate or above,…\n$ BMXBMI    &lt;dbl&gt; 21.5, 18.1, 21.3, 19.7, 39.9, 24.5, 26.0, 16.1, 27.6, 28.6, …\n$ BMXWAIST  &lt;dbl&gt; 79.3, 64.1, 86.6, 72.0, 118.4, 86.2, 86.0, 66.8, 101.5, 96.3…\n$ DXDTOPF   &lt;dbl&gt; 22.7, 19.0, 22.8, 16.7, 42.1, 20.4, 33.4, 26.9, 29.4, 22.8, …\n$ WTMEC4YR  &lt;dbl&gt; 4361.720, 3532.305, 6195.460, 15168.327, 7739.791, 30058.968…\n\n\n\n\nOverview by Cohort\nWe can now get a sense of the different cohorts and our variables of interest:\n\nSample Sizes\n\nNHANES |&gt;\n  count(Cohort) |&gt;\n  mutate(pct = n / sum(n) * 100)\n\n     Cohort    n      pct\n1 2017-2018 3617 30.69937\n2 2021-2023 8165 69.30063\n\n\n\n\nDXA Missingness\n\nNHANES |&gt;\n  group_by(Cohort) |&gt;\n  summarize(\n    total       = n(),\n    pct_missing = mean(is.na(DXDTOPF)) * 100)\n\n# A tibble: 2 × 3\n  Cohort    total pct_missing\n  &lt;fct&gt;     &lt;int&gt;       &lt;dbl&gt;\n1 2017-2018  3617           0\n2 2021-2023  8165         100\n\n\n\nExpected:\n\n2017-2018: 100% of DXA (DXDTOPF) present (using those with mobile examination center (MEC) tests and MEC weights).\n2021-2023: 100% missing for DXDTOPF (no DXA data post-pandemic).\n\n\n\n\n\nDescriptive Statistics\n\nBMI, Waist Circumference, and DXA % Fat\n\nNHANES |&gt;\n  group_by(Cohort) |&gt;\n  summarize(\n    BMI_mean = mean(BMXBMI),\n    BMI_sd   = sd(BMXBMI),\n    WC_mean  = mean(BMXWAIST),\n    WC_sd    = sd(BMXWAIST),\n    DXA_mean = mean(DXDTOPF),\n    DXA_sd   = sd(DXDTOPF)\n  ) |&gt;\n  pivot_longer(-Cohort)|&gt;\n  separate(name, c(\"Metric\", \"Statistic\")) |&gt;\n  pivot_wider(names_from = Statistic, values_from = value)\n\n# A tibble: 6 × 4\n  Cohort    Metric  mean    sd\n  &lt;fct&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 2017-2018 BMI     26.5  7.24\n2 2017-2018 WC      89.3 18.8 \n3 2017-2018 DXA     32.3  8.70\n4 2021-2023 BMI     27.1  7.96\n5 2021-2023 WC      92.1 22.0 \n6 2021-2023 DXA     NA   NA   \n\n\n\n\nDistributions of Continuous Measures of Adiposity\n\nNHANES |&gt;\n  pivot_longer(c(BMXBMI, BMXWAIST, DXDTOPF)) |&gt;\n  ggplot(aes(x = value, fill = Cohort)) +\n    facet_wrap(~ name, scales = \"free\") +\n    geom_histogram(alpha = 0.6, position = \"identity\", bins = 30) +\n    scale_fill_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n    labs(\n      title = \"Measures of Adiposity by Cohort\", \n      x = \"BMI (kg/m2), WC (cm), or % Fat\", \n      y = \"Count\") +\n    theme_minimal()\n\nWarning: Removed 8165 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\nDXA vs Anthropometry in 2017-2018\nOnly the 2017-2018 cohort has DXDTOPF, so we examine how BMI and WC relate to true % body fat among the study participants in this wave:\n\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  pivot_longer(c(BMXBMI, BMXWAIST)) |&gt;\n  ggplot(aes(x = value, y = DXDTOPF)) +\n    facet_wrap(~ name, scales = \"free_x\") +\n    geom_point(alpha = 0.4) +\n    geom_smooth(method = \"lm\", color = \"#1B365D\") +\n    labs(\n      title = \"DXA % Body Fat vs BMI and WC (2017-2018)\",\n      x = \"BMI (kg/m2) or WC (cm)\", y = \"DXDTOPF (%)\") +\n    theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet’s also compare these measures by Sex. We can also add reference lines to see where the measure-specific obesity cut-offs would be:\n\ncutoffs &lt;- tibble(\n  name = c(\"BMXBMI\", \"BMXBMI\", \"BMXWAIST\", \"BMXWAIST\"),\n  Sex  = c(\"Male\", \"Female\", \"Male\", \"Female\"),\n  xint = c(30, 30, 102, 88),\n  yint = c(30, 42,  30, 42)\n)\n\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  pivot_longer(c(BMXBMI, BMXWAIST)) |&gt;\n  ggplot(aes(x = value, y = DXDTOPF, group = Sex, fill = Sex, color = Sex)) +\n    facet_wrap(~ name, scales = \"free_x\") +\n    geom_point(alpha = 0.4) +\n    geom_smooth(method = \"lm\") +\n    geom_vline(data = cutoffs, \n      aes(xintercept = xint, color = Sex), linetype = \"dashed\") +\n    geom_hline(data = cutoffs, \n      aes(yintercept = yint, color = Sex), linetype = \"dashed\") +\n    scale_fill_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n    scale_color_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n    labs(\n      title = \"DXA % Body Fat vs BMI and WC (2017-2018)\",\n      x = \"BMI (kg/m2) or WC (cm)\", y = \"DXDTOPF (%)\") +\n    theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nThere is a strong positive linear relationship between both BMI and WC with DXA body fat percentage.\nThis relationship is apparent across both sexes, but slopes differ, indicating different body composition patterns between males and females.\nThe presence of individuals in mismatched quadrants (e.g., high BMI but low body fat) suggests limitations in using BMI/WC alone as diagnostic tools, particularly across sexes."
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#defining-obesity-categories",
    "href": "content/Unit02_MeasuringAdiposity.html#defining-obesity-categories",
    "title": "Unit 02: Measuring Adiposity",
    "section": "Defining Obesity Categories",
    "text": "Defining Obesity Categories\nLet us create three binary indicators:\n\nobese_BMI: BMI ≥ 30 kg/m²\nobese_WC: WC ≥ 102 cm (men) or ≥ 88 cm (women)\nobese_DXA: DXA % Fat &gt; 30% (men) or &gt; 42% (women)\n\n\nNHANES &lt;- NHANES |&gt;\n  mutate(\n    obese_BMI = BMXBMI &gt;= 30,\n    obese_WC  = case_when(\n      Sex == \"Male\"   & BMXWAIST &gt;= 102 ~ TRUE,\n      Sex == \"Female\" & BMXWAIST &gt;=  88 ~ TRUE,\n      .default                          = FALSE),\n    obese_DXA = case_when(\n      is.na(DXDTOPF)                 ~ NA,\n      Sex == \"Male\"   & DXDTOPF &gt; 30 ~ TRUE,\n      Sex == \"Female\" & DXDTOPF &gt; 42 ~ TRUE,\n      .default                       = FALSE)\n  )\n\n\nMisclassification in 2017-2018\nCompare BMI-defined vs DXA-defined obesity:\n\n# Overall\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  count(obese_DXA, obese_BMI) |&gt;\n  mutate(percent = sprintf(\"%.1f%%\", 100 * n / sum(n)))\n\n  obese_DXA obese_BMI    n percent\n1     FALSE     FALSE 2180   60.3%\n2     FALSE      TRUE  307    8.5%\n3      TRUE     FALSE  415   11.5%\n4      TRUE      TRUE  715   19.8%\n\n# By Sex\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  count(Sex, obese_DXA, obese_BMI) |&gt;\n  group_by(Sex) |&gt;\n  mutate(percent = sprintf(\"%.1f%%\", 100 * n / sum(n))) |&gt;\n  ungroup()\n\n# A tibble: 8 × 5\n  Sex    obese_DXA obese_BMI     n percent\n  &lt;fct&gt;  &lt;lgl&gt;     &lt;lgl&gt;     &lt;int&gt; &lt;chr&gt;  \n1 Male   FALSE     FALSE      1003 56.1%  \n2 Male   FALSE     TRUE        138 7.7%   \n3 Male   TRUE      FALSE       302 16.9%  \n4 Male   TRUE      TRUE        345 19.3%  \n5 Female FALSE     FALSE      1177 64.4%  \n6 Female FALSE     TRUE        169 9.2%   \n7 Female TRUE      FALSE       113 6.2%   \n8 Female TRUE      TRUE        370 20.2%  \n\n\nAnd WC-defined vs DXA-defined:\n\n# Overall\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  count(obese_DXA, obese_WC) |&gt;\n  mutate(percent = sprintf(\"%.1f%%\", 100 * n / sum(n)))\n\n  obese_DXA obese_WC    n percent\n1     FALSE    FALSE 1959   54.2%\n2     FALSE     TRUE  528   14.6%\n3      TRUE    FALSE  319    8.8%\n4      TRUE     TRUE  811   22.4%\n\n# By Sex\nNHANES |&gt;\n  filter(Cohort == \"2017-2018\") |&gt;\n  count(Sex, obese_DXA, obese_WC) |&gt;\n  group_by(Sex) |&gt;\n  mutate(percent = sprintf(\"%.1f%%\", 100 * n / sum(n))) |&gt;\n  ungroup()\n\n# A tibble: 8 × 5\n  Sex    obese_DXA obese_WC     n percent\n  &lt;fct&gt;  &lt;lgl&gt;     &lt;lgl&gt;    &lt;int&gt; &lt;chr&gt;  \n1 Male   FALSE     FALSE     1020 57.0%  \n2 Male   FALSE     TRUE       121 6.8%   \n3 Male   TRUE      FALSE      286 16.0%  \n4 Male   TRUE      TRUE       361 20.2%  \n5 Female FALSE     FALSE      939 51.3%  \n6 Female FALSE     TRUE       407 22.3%  \n7 Female TRUE      FALSE       33 1.8%   \n8 Female TRUE      TRUE       450 24.6%  \n\n\n\nInterpretation:\nBMI vs. DXA:\n\nThe misclassification rate is 8.5% + 11.5% = 20.0%, with BMI underestimating DXA-defined obesity more than it overestimates it.\nBy sex, BMI misses many obese males per DXA, indicating underestimation of adiposity, but it overcalls obesity more often than it misses it in females.\nBMI is less sensitive for men, while women are more likely to be labeled obese by BMI even when not by DXA, indicating reduced specificity.\n\nWC vs. DXA:\n\nThe misclassification rate is 14.6% + 8.8% = 23.4%, with WC slightly overestimating obesity more than it underestimates, opposite to BMI.\nWC underestimates DXA-defined obesity in males, similar to BMI, but overestimates obesity in females, often classifying non-obese women as obese.\nFor women, WC has higher sensitivity but lower specificity than BMI, as it detects more DXA-obese individuals but also overcalls many who are not.\n\n\n\n\nAdditional Stratified Comparisons: Grouped Bar Charts by Measure\nLet us reshape the data to long format so that BMI, WC, and DXA-defined obesity are all in one “Measure” column:\n\n# Assume NHANES is already loaded and has Cohort, Age, Sex, Race, \n# and the three obesity variables\nnhanes_long &lt;- NHANES |&gt;\n  select(Cohort, Age, Sex, Race, obese_BMI, obese_WC, obese_DXA) |&gt;\n  pivot_longer(\n    cols = starts_with(\"obese_\"),\n    names_to = \"Measure\",\n    values_to = \"Obese\"\n  ) |&gt;\n  mutate(\n    Measure = recode(Measure,\n      obese_BMI = \"BMI\",\n      obese_WC  = \"Waist Circumference\",\n      obese_DXA = \"DXA\"\n    )\n  ) |&gt;\n  filter(!is.na(Obese))\n\n\nBy Age Group\n\nprop_age &lt;- nhanes_long |&gt;\n  group_by(Cohort, Age, Measure, Obese) |&gt;\n  summarize(n = n(), .groups = \"drop\") |&gt;\n  group_by(Cohort, Age, Measure) |&gt;\n  mutate(prop = n / sum(n))\n\nggplot(prop_age, aes(x = Age, y = prop, fill = Obese)) +\n  geom_col(position = \"stack\") +\n  facet_grid(Cohort ~ Measure) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  scale_color_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  labs(\n    x     = \"Age Group\",\n    y     = \"Percent\",\n    fill  = \"Obesity Status\",\n    title = \"Obesity Classification by Age, Cohort, and Measure\"\n  ) +\n  theme_minimal()\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\nWarning in min(d[d &gt; tolerance]): no non-missing arguments to min; returning\nInf\n\n\n\n\n\n\n\n\n\n\n\nBy Sex\n\nprop_sex &lt;- nhanes_long |&gt;\n  group_by(Cohort, Sex, Measure, Obese) |&gt;\n  summarize(n = n(), .groups = \"drop\") |&gt;\n  group_by(Cohort, Sex, Measure) |&gt;\n  mutate(prop = n / sum(n))\n\nggplot(prop_sex, aes(x = Sex, y = prop, fill = Obese)) +\n  geom_col(position = \"stack\") +\n  facet_grid(Cohort ~ Measure) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  scale_color_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  labs(\n    x     = \"Sex\",\n    y     = \"Percent\",\n    fill  = \"Obesity Status\",\n    title = \"Obesity Classification by Sex, Cohort, and Measure\"\n  ) +\n  theme_minimal()\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\nWarning in min(d[d &gt; tolerance]): no non-missing arguments to min; returning\nInf\n\n\n\n\n\n\n\n\n\n\n\nBy Race/Ethnicity\n\nprop_race &lt;- nhanes_long |&gt;\n  group_by(Cohort, Race, Measure, Obese) |&gt;\n  summarize(n = n(), .groups = \"drop\") |&gt;\n  group_by(Cohort, Race, Measure) |&gt;\n  mutate(prop = n / sum(n))\n\nggplot(prop_race, aes(x = Race, y = prop, fill = Obese)) +\n  geom_col(position = \"stack\") +\n  facet_grid(Cohort ~ Measure) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  scale_color_manual(values = c(\"#00C1D5\", \"#AA4AC4\")) +\n  labs(\n    x     = \"Race / Ethnicity\",\n    y     = \"Percent\",\n    fill  = \"Obesity Status\",\n    title = \"Obesity Classification by Race/Ethnicity, Cohort, and Measure\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning in min(x): no non-missing arguments to min; returning Inf\n\n\nWarning in max(x): no non-missing arguments to max; returning -Inf\n\n\nWarning in min(d[d &gt; tolerance]): no non-missing arguments to min; returning\nInf\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nBMI misclassification varies systematically across subgroups:\n\nAge: Young adults (18-34) often have lower lean mass, so normal-BMI individuals can still have high body fat (“normal-weight obesity”).\nSex: Women generally have higher percent-fat at the same BMI; sex-specific DXA thresholds help adjust for this.\nRace/Ethnicity: Differences in body composition and fat distribution mean that a single BMI cut-point may not correspond to the same adiposity level across groups.\n\nImplications: Reliance on BMI alone can bias epidemiologic associations with true adiposity-driven outcomes (e.g., diabetes, cardiovascular disease). When DXA or other body-composition measures are impractical, consider calibration equations or subgroup-specific BMI thresholds.\nNext: we will split the combined NHANES data into labeled (2017-2018 with DXDTOPF) and unlabeled (2021-2023 without DXA), and then proceed with IPD using BMI or WC as our proxy."
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#demographic-associations-naive-vs-classical-vs-ipd",
    "href": "content/Unit02_MeasuringAdiposity.html#demographic-associations-naive-vs-classical-vs-ipd",
    "title": "Unit 02: Measuring Adiposity",
    "section": "Demographic Associations: Naive vs Classical vs IPD",
    "text": "Demographic Associations: Naive vs Classical vs IPD\n\nNaive vs Classical vs IPD Inference\nWe are interested in studying the association between true percent body fat (DXA % fat) and risk factors such as age, sex, and race. We model the binary obesity outcome (DXA-defined “true” vs BMI/WC “proxy”) as a function of Age, Sex, and Race using logistic regression:\n\nNaive: proxy-only model on unlabeled data (2021-2023)\n\nClassical: true-only model on labeled data (2017-2018)\n\nIPD: combine both while correcting for proxy error\n\n\nSetup Labeled vs Unlabeled Data\n\n# Split NHANES into labeled (DXA available) vs unlabeled\nlabeled   &lt;- NHANES |&gt; filter(Cohort == \"2017-2018\")\nunlabeled &lt;- NHANES |&gt; filter(Cohort == \"2021-2023\")\n\n# Stack for IPD\ncombined &lt;- bind_rows(\n  labeled   |&gt; mutate(set_label = \"labeled\"),\n  unlabeled |&gt; mutate(set_label = \"unlabeled\")\n)\n\n\n\nNaive versus Classical Regressions\nWe fit:\n\nNaive-BMI: lm(BMI ~ Age + Sex + Race,   data = unlabeled)\nNaive-WC: lm(WC  ~ Age + Sex + Race,   data = unlabeled)\nClassical: lm(DXDTOPF ~ Age + Sex + Race, data = labeled)\n\nFor the naive method, we treat BMI- and WC-based obesity (our ’predictions) as the true outcomes, fit on the unlabeled participants:\n\n# Naive on unlabeled using BMI\nnaive_bmi_fit &lt;- glm(obese_BMI ~ Age + Sex + Race, \n  family = binomial, data = unlabeled)\nnaive_bmi_df  &lt;- broom::tidy(naive_bmi_fit) |&gt;\n  mutate(method = \"Naive (BMI)\")\n\n# Naive on unlabeled using WC\nnaive_wc_fit &lt;- glm(obese_WC ~ Age + Sex + Race,\n  family = binomial, data = unlabeled)\nnaive_wc_df  &lt;- broom::tidy(naive_wc_fit) |&gt;\n  mutate(method = \"Naive (WC)\")\n\n\nInterpretation:\n\nBecause we ‘predicted’ obesity using BMI or WC, the “naive” coefficient are biased and the confidence intervals are artifically too narrow.\n\n\nFor the classical approach, we fit the true model on the labeled subset (with actual DXA % fat). That is:\n\n# Classical on labeled using DXA\nclass_fit &lt;- glm(obese_DXA ~ Age + Sex + Race,\n  family = binomial, data = labeled)\nclass_df  &lt;- broom::tidy(class_fit) |&gt;\n  mutate(method = \"Classical (DXA)\")\n\n# Combine\ncoef_df &lt;- bind_rows(naive_bmi_df, naive_wc_df, class_df) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(\n    term = recode(term,\n      \"Age20-39\" = \"Age 20-39 (vs Under 20)\",\n      \"Age40-59\" = \"Age 40-59 (vs Under 20)\",\n      \"Age60+\"   = \"Age 60+ (vs Under 20)\",\n      \"SexFemale\" = \"Female (vs Male)\",\n      \"RaceNon-Hispanic Black\" = \"Non-Hispanic Black (vs Non-Hispanic White)\",\n      \"RaceNon-Hispanic Asian\" = \"Non-Hispanic Asian (vs Non-Hispanic White)\",\n      \"RaceHispanic\" = \"Hispanic (vs Non-Hispanic White)\",\n      \"RaceOther Race - Including Multi-Racial\" = \"Other Race - Including Multi-Racial (vs Non-Hispanic White)\"\n    ),\n    method = factor(method, levels = c(\"Classical (DXA)\", \"Naive (BMI)\", \"Naive (WC)\"))\n  )\n    \n# Forest plot\nggplot(coef_df, aes(x = estimate, y = term, color = method)) +\n  geom_point(position = position_dodge(width = 0.6)) +\n  geom_errorbarh(aes(xmin = estimate - 1.96 * std.error,\n                     xmax = estimate + 1.96 * std.error),\n                 height = 0.2,\n                 position = position_dodge(width = 0.6)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(values  = c(\"#1B365D\", \"#00C1D5\", \"#AA4AC4\")) +\n  scale_color_manual(values = c(\"#1B365D\", \"#00C1D5\", \"#AA4AC4\")) +\n  labs(\n    x = \"Log-Odds Estimate (95% CI)\",\n    y = \"Covariate\",\n    title = \"Comparison of Obesity Model Coefficients\",\n    color = \"Model Type\"\n  ) +\n  theme_minimal()\n\nWarning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\n\nHow to interpret these results?\n\nNotice the differences in significance and magnitude, even direction/sign change depending on the measure of obesity used!\nAge Groups (vs Under 20): All age groups have positive coefficients, indicating higher odds of DXA-defined obesity with age, but naive BMI and WC show even larger positive estimates, suggesting that BMI and WC may over-classify obesity in older adults compared to DXA.\nSex (Female vs Male): DXA shows a negative association, while BMI and WC have uch larger effect sizes, particularly for WC, implying that WC-based obesity strongly overclassifies women as obese relative to DXA.\nRace/Ethnicity (vs Non-Hispanic White): BMI/WC models show larger effects in Non-Hispanic Black participants, again hinting that BMI and WC may overestimate obesity in this group, while the opposite is true for Non-Hispanic Asian participants.\n\n\n\n\nIPD Corrections\nWe now apply IPD to leverage all participants (labeled + unlabeled) while adjusting for prediction error. We run two IPD analyses, one using BMI as our ‘f’ and one using WC.\n\nA note on two calling styles:\nWe can either provide a single, combined dataset to the data argument and the name of the column that gives the set labels in label:\nipd_bmi_1 &lt;- ipd( formula = obese_DXA - obese_BMI ~ Age + Sex + Race, data = combined, label = “set_label”, model = “logistic”, method = “pspa” )\nor we can provide the labeled set to data and the unlabeled set to unlabeled_data separately:\nipd_bmi_2 &lt;- ipd( formula = obese_DXA - obese_BMI ~ Age + Sex + Race, data = labeled, unlabeled_data = unlabeled, model = “logistic”, method = “pspa” )\n\nNow we can try to run the IPD model:\n\n# Note: This code chunk results in an error, but we have an informative warning\n# We can try running it and see!\n\n# ipd_bmi_fit &lt;- ipd(\n#     formula = obese_DXA - obese_BMI ~ Age + Sex + Race,\n#     method  = \"pspa\",\n#     model   = \"logistic\",\n#     data    = combined,\n#     label   = \"set_label\"\n# )\n# \n# ipd_wc_fit &lt;- ipd(\n#     formula = obese_DXA - obese_WC ~ Age + Sex + Race,\n#     method  = \"pspa\",\n#     model   = \"logistic\",\n#     data    = labeled,\n#     unlabeled_data = unlabeled\n# )\n\n\nWe get an informative warning:\nWarning in .warn_differing_levels(inp$data_l, inp$data_u, factor_vars): Differing factor levels between labeled and unlabeled data:\nWarning: - Age: labeled = [Under 20, 20-39, 40-59] unlabeled = [Under 20, 20-39, 40-59, 60+]\n\nWe can see that the labeled subset does not include any \"60+\" observations, but IPD expects the same factor levels in both sets in order to fit the model using all the data. To fix this, let us refactor both the labeled and unlabeled data so they have consistent age categories:\n\n# Recode age\nNHANES2 &lt;- NHANES |&gt;\n  mutate(Age_recode = fct_collapse(Age, `40+` = c(\"40-59\", \"60+\")))\n      \n# Split NHANES into labeled (DXA available) vs unlabeled\nlabeled2   &lt;- NHANES2 |&gt; \n  filter(Cohort == \"2017-2018\") |&gt; \n  select(obese_DXA, obese_BMI, obese_WC, Age_recode, Sex, Race)\nunlabeled2 &lt;- NHANES2 |&gt; filter(Cohort == \"2021-2023\") |&gt; \n  select(obese_DXA, obese_BMI, obese_WC, Age_recode, Sex, Race)\n\n# Stack for IPD\ncombined2 &lt;- bind_rows(\n  labeled2   |&gt; mutate(set_label = \"labeled\"),\n  unlabeled2 |&gt; mutate(set_label = \"unlabeled\")\n)\n\nNow let us rerun ipd::ipd() and compare our results to the Naive and Classical models:\n\n# Note: This code chunk now runs without error!\nipd_bmi_fit &lt;- ipd(\n    formula = obese_DXA - obese_BMI ~ Age_recode + Sex + Race,\n    method  = \"pspa\",\n    model   = \"logistic\",\n    data    = combined2,\n    label   = \"set_label\"\n)\n\nipd_wc_fit &lt;- ipd(\n    formula = obese_DXA - obese_WC ~ Age_recode + Sex + Race,\n    method  = \"pspa\",\n    model   = \"logistic\",\n    data    = labeled2,\n    unlabeled_data = unlabeled2\n)\n\n# Collect results using the tidy() method\nipd_bmi_df &lt;- tidy(ipd_bmi_fit) |&gt;\n  mutate(method = \"IPD (BMI)\")\n\nipd_wc_df  &lt;- tidy(ipd_wc_fit) |&gt;\n  mutate(method = \"IPD (WC)\")\n\n\n# Rerun previous models\n# Naive on unlabeled using BMI\nnaive_bmi_fit &lt;- glm(obese_BMI ~ Age_recode + Sex + Race, \n  family = binomial, data = unlabeled2)\nnaive_bmi_df  &lt;- broom::tidy(naive_bmi_fit) |&gt;\n  mutate(method = \"Naive (BMI)\")\n\n# Naive on unlabeled using WC\nnaive_wc_fit &lt;- glm(obese_WC ~ Age_recode + Sex + Race,\n  family = binomial, data = unlabeled2)\nnaive_wc_df  &lt;- broom::tidy(naive_wc_fit) |&gt;\n  mutate(method = \"Naive (WC)\")\n\n# Classical on labeled using DXA\nclass_fit &lt;- glm(obese_DXA ~ Age_recode + Sex + Race,\n  family = binomial, data = labeled2)\nclass_df  &lt;- broom::tidy(class_fit) |&gt;\n  mutate(method = \"Classical (DXA)\")\n\n# Combine\ncoef_df &lt;- bind_rows(\n  ipd_bmi_df, ipd_wc_df, naive_bmi_df, naive_wc_df, class_df) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(\n    term = recode(term,\n      \"Age_recode20-39\" = \"Age 20-39 (vs Under 20)\",\n      \"Age_recode40+\" = \"Age 40+ (vs Under 20)\",\n      \"SexFemale\" = \"Female (vs Male)\",\n      \"RaceNon-Hispanic Black\" = \"Non-Hispanic Black (vs Non-Hispanic White)\",\n      \"RaceNon-Hispanic Asian\" = \"Non-Hispanic Asian (vs Non-Hispanic White)\",\n      \"RaceHispanic\" = \"Hispanic (vs Non-Hispanic White)\",\n      \"RaceOther Race - Including Multi-Racial\" = \"Other Race - Including Multi-Racial (vs Non-Hispanic White)\"\n    ),\n    method = factor(method, \n      levels = c(\"IPD (BMI)\", \"IPD (WC)\", \n                 \"Classical (DXA)\", \"Naive (BMI)\", \"Naive (WC)\"))\n  )\n    \n# Forest plot\nggplot(coef_df, aes(x = estimate, y = term, \n    color = method, fill = method, shape = method)) +\n  geom_point(position = position_dodge(width = 0.6)) +\n  geom_errorbarh(aes(xmin = estimate - 1.96 * std.error,\n                     xmax = estimate + 1.96 * std.error),\n                 height = 0.2,\n                 position = position_dodge(width = 0.6)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(\n    values = c(\n      \"Classical (DXA)\" = \"#1B365D\", \n      \"Naive (BMI)\"     = \"#00C1D5\", \n      \"Naive (WC)\"      = \"#AA4AC4\",\n      \"IPD (BMI)\"       = \"#00C1D5\",\n      \"IPD (WC)\"        = \"#AA4AC4\"\n    )\n  ) +\n  scale_color_manual(\n    values = c(\n      \"Classical (DXA)\" = \"#1B365D\", \n      \"Naive (BMI)\"     = \"#00C1D5\", \n      \"Naive (WC)\"      = \"#AA4AC4\",\n      \"IPD (BMI)\"       = \"#00C1D5\",\n      \"IPD (WC)\"        = \"#AA4AC4\"\n    )\n  ) +\n  scale_shape_manual(\n    values = c(\n      \"Classical (DXA)\" = 16, \n      \"Naive (BMI)\"     = 16, \n      \"Naive (WC)\"      = 16,\n      \"IPD (BMI)\"       = 17,\n      \"IPD (WC)\"        = 17\n    )\n  ) +\n  labs(\n    x = \"Log-Odds Estimate (95% CI)\",\n    y = \"Covariate\",\n    title = \"Comparison of Obesity Model Coefficients\",\n    color = \"Model Type\", fill = \"Model Type\", shape = \"Model Type\"\n  ) +\n  theme_minimal()\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\n\nKey Point: The IPD methods combine both cohorts, using the labeled DXA values and the proxy in the unlabeled cohort to correct bias and recover valid estimates for the effect of Age, Sex, and Race/Ethnicity on true adiposity."
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#summary-and-key-takeaways",
    "href": "content/Unit02_MeasuringAdiposity.html#summary-and-key-takeaways",
    "title": "Unit 02: Measuring Adiposity",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nBMI and WC are imperfect proxies for DXA-based % body fat.\nNaive regression on BMI or WC leads to biased estimates for associations between obesity and risk factors such as age, sex, and race.\nClassical regression on true DXA-based % fat is unbiased but costly.\nIPD allows you to leverage a large cohort with only BMI + covariates, correct for prediction error, and recover unbiased estimates with more precision than the Classical approach.\nUnderstanding how different proxies like BMI and WC affect population-level inference is important for epidemiologic and clinical studies."
  },
  {
    "objectID": "content/Unit02_MeasuringAdiposity.html#references",
    "href": "content/Unit02_MeasuringAdiposity.html#references",
    "title": "Unit 02: Measuring Adiposity",
    "section": "References",
    "text": "References\n\nVisokay, Adam, et al. “How to measure obesity in public health research? Problems with using BMI for population inference.” medRxiv (2025): 2025-04.\n\n\nThis is the end of the module. We hope this was informative! For question/concerns/suggestions, please reach out to ssalerno@fredhutch.org"
  },
  {
    "objectID": "editing.html",
    "href": "editing.html",
    "title": "Editing",
    "section": "",
    "text": "Now that your website is alive and hosted, how do you start customizing it to have your content?\nIf you already know how to file pull requests, feel free to start making edits as you see fit using some of the structure points we’ve noted below.\nIf you are not familiar with pull requests, read this guide to get started."
  },
  {
    "objectID": "editing.html#title",
    "href": "editing.html#title",
    "title": "Editing",
    "section": "Title",
    "text": "Title\nTo change the title of the website, modify the name line of the _site.yml file."
  },
  {
    "objectID": "editing.html#modifying-pages",
    "href": "editing.html#modifying-pages",
    "title": "Editing",
    "section": "Modifying pages",
    "text": "Modifying pages\nPages are specified in the navigation bar by the lines that say -text: and href: .\nThe -text: specifies what the navigation bar will say for that tab.\nThe href: specifies which rendered Rmd file to use for that tab, it needs to be the html version of this file.\nThe tabs are specified to be aligned to the left (as specified by the left on line 5).\nThe tabs will appear in the order listed.\nYou can also add icons to these tabs using font awesome as is shown for the contact page on line 18.\n\n\n\n\n\n\n\n\n\nOther icon options include Bootstap glyphicons or ion icons. Note that not all icons will work because they are not all set up with the packages that make rendering the website possible, so this may require some trial and error. Here is an example of how you would use all of these icon options to add more:\n\n\n\n\n\n\n\n\n\nThis would result in a navigation bar with these icons:"
  },
  {
    "objectID": "hosting.html",
    "href": "hosting.html",
    "title": "Hosting",
    "section": "",
    "text": "To host your website on GitHub, you will need to go to settings and click on the pages tab.\nAgain to go to settings click on the far upper right corner:\n\n\n\n\n\n\n\n\n\nClick on Pages:\n\n\n\n\n\n\n\n\n\nSelect the main branch and the root directory and press save. Be sure to also check the “Enforce HTTPS” box. Afterwards your repository should look like this:\n\n\n\n\n\n\n\n\n\nNote that in general, your website will be published to a URL like this:\nhttps://username.github.io/repository_name/\nIf you have multiple websites published underneath your username or organization, this should still publish fine. This website will be a subdirectory that is named whatever you have named this repository.\nIf you’ve published this website underneath a GitHub organization (not your own personal GitHub profile), then in the above example URL the organization name will be where we’ve put username.\nFor more about GitHub pages (including how to personalize your URL) see the GitHub pages documentation here: https://pages.github.com/\nSometimes, GitHub page publishing will take a bit of time. You can click on the Actions tab in your repository and see if there is a pages and deployment action currently running (indicated by a yellow circle next to the action name). If this is the case, you will need to wait until this becomes a green check mark before your GitHub page will be published."
  },
  {
    "objectID": "hosting.html#hosting-setup",
    "href": "hosting.html#hosting-setup",
    "title": "Hosting",
    "section": "",
    "text": "To host your website on GitHub, you will need to go to settings and click on the pages tab.\nAgain to go to settings click on the far upper right corner:\n\n\n\n\n\n\n\n\n\nClick on Pages:\n\n\n\n\n\n\n\n\n\nSelect the main branch and the root directory and press save. Be sure to also check the “Enforce HTTPS” box. Afterwards your repository should look like this:\n\n\n\n\n\n\n\n\n\nNote that in general, your website will be published to a URL like this:\nhttps://username.github.io/repository_name/\nIf you have multiple websites published underneath your username or organization, this should still publish fine. This website will be a subdirectory that is named whatever you have named this repository.\nIf you’ve published this website underneath a GitHub organization (not your own personal GitHub profile), then in the above example URL the organization name will be where we’ve put username.\nFor more about GitHub pages (including how to personalize your URL) see the GitHub pages documentation here: https://pages.github.com/\nSometimes, GitHub page publishing will take a bit of time. You can click on the Actions tab in your repository and see if there is a pages and deployment action currently running (indicated by a yellow circle next to the action name). If this is the case, you will need to wait until this becomes a green check mark before your GitHub page will be published."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "Getting started\nCreate your repository by clicking on the Use this Template button at OTTR_Template_Website repository\n\nYou’ll need to make your repository public.\n\n\nSet your GH_PAT\nTo enable the GitHub actions, your repository needs to be setup in a specific way.\nFor OTTR GitHub actions to run, they need to have credentials through a personal access token.\n\nSet up your own personal access token following these instructions - but create a classic token. Keep this personal access token handy for the next step. When you get to the permissions page, check the box that says repo and select all that is underneath that header. No other permissions are necessary.\n\n\n\nClick here for more detailed instructions. The instructions for this step may change with updates to GitHub.\n\nFirst, go to your username settings, by clicking on your user icon (upper right corner) and scrolling down to settings.\n\n\n\n\n\n\n\n\n\nNext, scroll all the way down on the far right menu to “Developer Settings”.\n\n\n\n\n\n\n\n\n\nThen select “Personal Access Tokens” and “Tokens (classic)”\n\n\n\n\n\n\n\n\n\nThen click “Generate new token” and confirm that you want classic.\n\n\n\n\n\n\n\n\n\nFinally, add a name select all the repo scopes and scroll down to the green button to generate the token. Copy this somewhere safe to then paste into your repository settings.\n\n\n\n\n\n\n\n\n\n\n\nIn your new OTTR_Template_Website derived repository, go to Settings &gt; Secrets and variables &gt; Actions. Click New Repository Secret.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the window opened, name this new secret GH_PAT and paste the personal access token in the box below. (Note that the name GH_PAT is specific to how OTTR works and other secret names cannot be used and for OTTR to still work).\nClick the green button to add the secret.\n\n\n\n\n\n\n\n\n\n\n\nAllow GitHub Actions\nGo to the settings menu for your repository that you created from the template. This should be located at the top of GitHub on the right side.\nScroll down to the “Actions” button and click it, then click “General”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScroll down to the workflow permissions section and select “Read and write permissions”, then click “Allow GitHub actions to create and approve pull requests.\nFinally, click “save”.\n\n\n\n\n\n\n\n\n\n\n\nProtect branches\nAlthough this isn’t entirely required, its strongly recommended that you use these settings to protect your main branches.\nClick on settings in the far upper right corner:\n\n\n\n\n\n\n\n\n\nClick on branches:\n\n\n\n\n\n\n\n\n\nClick the add rule button.\n\n\n\n\n\n\n\n\n\nType “main” as the branch name pattern:\n\n\n\n\n\n\n\n\n\nClick on the following boxes to require pull requests before merging:\n\n\n\n\n\n\n\n\n\nNote that if you have admin privileges on this repository, you will likely still be able to override these branch protections so use caution when git pushing!"
  }
]